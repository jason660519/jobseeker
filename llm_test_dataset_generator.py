#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMæ¸¬è©¦æ¡ˆä¾‹æ•¸æ“šé›†ç”Ÿæˆå™¨
ç”Ÿæˆå¤§é‡å¤šæ¨£åŒ–çš„æ¸¬è©¦æ¡ˆä¾‹ï¼Œç”¨æ–¼å…¨é¢è©•ä¼°LLMæ¨¡å‹æ€§èƒ½

Author: JobSpy Team
Date: 2025-01-27
"""

import json
import random
import itertools
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from collections import defaultdict
import string
import re


class DatasetType(Enum):
    """æ•¸æ“šé›†é¡å‹"""
    TRAINING = "training"  # è¨“ç·´é›†
    VALIDATION = "validation"  # é©—è­‰é›†
    TESTING = "testing"  # æ¸¬è©¦é›†
    BENCHMARK = "benchmark"  # åŸºæº–æ¸¬è©¦é›†
    STRESS = "stress"  # å£“åŠ›æ¸¬è©¦é›†
    ADVERSARIAL = "adversarial"  # å°æŠ—æ¸¬è©¦é›†


class GenerationStrategy(Enum):
    """ç”Ÿæˆç­–ç•¥"""
    TEMPLATE_BASED = "template_based"  # åŸºæ–¼æ¨¡æ¿
    COMBINATORIAL = "combinatorial"  # çµ„åˆå¼
    RULE_BASED = "rule_based"  # åŸºæ–¼è¦å‰‡
    SYNTHETIC = "synthetic"  # åˆæˆå¼
    AUGMENTATION = "augmentation"  # æ•¸æ“šå¢å¼·
    ADVERSARIAL = "adversarial"  # å°æŠ—å¼


@dataclass
class DatasetConfig:
    """æ•¸æ“šé›†é…ç½®"""
    dataset_type: DatasetType
    target_size: int
    generation_strategies: List[GenerationStrategy]
    complexity_distribution: Dict[str, float] = field(default_factory=lambda: {
        "simple": 0.3, "medium": 0.4, "complex": 0.2, "extreme": 0.1
    })
    language_distribution: Dict[str, float] = field(default_factory=lambda: {
        "chinese": 0.6, "english": 0.3, "mixed": 0.1
    })
    category_distribution: Dict[str, float] = field(default_factory=lambda: {
        "job_search": 0.4, "skill_query": 0.2, "location_query": 0.15,
        "salary_query": 0.1, "career_transition": 0.05, "remote_work": 0.05,
        "non_job_related": 0.05
    })
    quality_threshold: float = 0.8
    diversity_threshold: float = 0.7
    enable_validation: bool = True
    enable_augmentation: bool = True


@dataclass
class GeneratedDataset:
    """ç”Ÿæˆçš„æ•¸æ“šé›†"""
    dataset_id: str
    dataset_type: DatasetType
    test_cases: List[Dict[str, Any]]
    metadata: Dict[str, Any] = field(default_factory=dict)
    statistics: Dict[str, Any] = field(default_factory=dict)
    generation_time: Optional[datetime] = None
    quality_score: float = 0.0
    diversity_score: float = 0.0


class LLMTestDatasetGenerator:
    """LLMæ¸¬è©¦æ•¸æ“šé›†ç”Ÿæˆå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨"""
        self.templates = self._load_templates()
        self.entities = self._load_entities()
        self.patterns = self._load_patterns()
        self.augmentation_rules = self._load_augmentation_rules()
        self.generated_queries = set()  # é¿å…é‡è¤‡
        
    def _load_templates(self) -> Dict[str, List[str]]:
        """è¼‰å…¥æŸ¥è©¢æ¨¡æ¿"""
        return {
            "job_search": [
                "æˆ‘æƒ³æ‰¾{job_title}çš„å·¥ä½œ",
                "è«‹å¹«æˆ‘æœå°‹{location}çš„{job_title}è·ä½",
                "æœ‰æ²’æœ‰{company}çš„{job_title}ç©ºç¼º",
                "å°‹æ‰¾{experience_level}{job_title}å·¥ä½œæ©Ÿæœƒ",
                "æˆ‘è¦æ‡‰å¾µ{industry}è¡Œæ¥­çš„{job_title}",
                "Find {job_title} jobs in {location}",
                "Looking for {job_title} position at {company}",
                "Search for {experience_level} {job_title} roles",
                "I want to work as a {job_title} in {industry}",
                "Show me {job_title} openings with {salary_range} salary"
            ],
            "skill_query": [
                "æˆ‘éœ€è¦å­¸ç¿’ä»€éº¼æŠ€èƒ½æ‰èƒ½æˆç‚º{job_title}",
                "{job_title}éœ€è¦å…·å‚™å“ªäº›æŠ€èƒ½",
                "å¦‚ä½•æå‡{skill_name}æŠ€èƒ½",
                "What skills are required for {job_title}",
                "How to improve {skill_name} skills",
                "Best way to learn {technology} for {job_title}",
                "æˆ‘æƒ³è½‰è·åˆ°{job_title}ï¼Œéœ€è¦ä»€éº¼æŠ€èƒ½",
                "{industry}è¡Œæ¥­æœ€é‡è¦çš„æŠ€èƒ½æ˜¯ä»€éº¼"
            ],
            "location_query": [
                "{location}æœ‰å“ªäº›{job_title}å·¥ä½œ",
                "æˆ‘æƒ³åœ¨{location}å·¥ä½œ",
                "æ¬åˆ°{location}å¾Œå¦‚ä½•æ‰¾å·¥ä½œ",
                "Jobs in {location} for {job_title}",
                "Best cities for {job_title} career",
                "Remote {job_title} jobs available",
                "æ¯”è¼ƒ{location1}å’Œ{location2}çš„å·¥ä½œæ©Ÿæœƒ"
            ],
            "salary_query": [
                "{job_title}çš„è–ªæ°´å¤§æ¦‚å¤šå°‘",
                "æˆ‘æƒ³è¦è–ªæ°´{salary_range}çš„å·¥ä½œ",
                "å¦‚ä½•è«‡åˆ¤æ›´é«˜çš„è–ªæ°´",
                "What is the average salary for {job_title}",
                "High paying {job_title} jobs",
                "Salary negotiation tips for {job_title}",
                "{location}çš„{job_title}è–ªè³‡æ°´å¹³"
            ],
            "career_transition": [
                "æˆ‘æƒ³å¾{current_job}è½‰è·åˆ°{target_job}",
                "å¦‚ä½•è½‰æ›è·æ¥­è·‘é“",
                "Career change from {industry1} to {industry2}",
                "è½‰è·éœ€è¦æ³¨æ„ä»€éº¼",
                "æˆ‘{age}æ­²äº†é‚„èƒ½è½‰è·å—",
                "How to switch from {current_job} to {target_job}"
            ],
            "remote_work": [
                "æˆ‘æƒ³æ‰¾é ç«¯å·¥ä½œ",
                "æœ‰å“ªäº›{job_title}å¯ä»¥åœ¨å®¶å·¥ä½œ",
                "Remote {job_title} opportunities",
                "Work from home {job_title} jobs",
                "å¦‚ä½•ç”³è«‹åœ‹å¤–çš„é ç«¯å·¥ä½œ",
                "Best remote work platforms for {job_title}"
            ],
            "non_job_related": [
                "ä»Šå¤©å¤©æ°£å¦‚ä½•",
                "æ¨è–¦ä¸€äº›å¥½åƒçš„é¤å»³",
                "What is the capital of France",
                "å¦‚ä½•ç…®å’–å•¡",
                "æœ€æ–°çš„é›»å½±æ¨è–¦",
                "How to lose weight",
                "è‚¡ç¥¨æŠ•è³‡å»ºè­°"
            ],
            "boundary_cases": [
                "",  # ç©ºæŸ¥è©¢
                "   ",  # åªæœ‰ç©ºæ ¼
                "a",  # å–®å­—ç¬¦
                "å·¥ä½œ" * 100,  # è¶…é•·é‡è¤‡
                "!@#$%^&*()",  # ç‰¹æ®Šå­—ç¬¦
                "SELECT * FROM jobs WHERE salary > 100000",  # SQLæ³¨å…¥
                "<script>alert('xss')</script>",  # XSSæ”»æ“Š
                "å·¥ä½œå·¥ä½œå·¥ä½œå·¥ä½œå·¥ä½œ",  # é‡è¤‡è©å½™
                "æˆ‘æˆ‘æˆ‘æˆ‘æˆ‘æƒ³æƒ³æƒ³æ‰¾æ‰¾å·¥å·¥ä½œä½œ",  # å­—ç¬¦é‡è¤‡
            ],
            "multilingual": [
                "æˆ‘æƒ³æ‰¾software engineerçš„å·¥ä½œ",  # ä¸­è‹±æ··åˆ
                "Looking for è»Ÿé«”å·¥ç¨‹å¸« position",  # è‹±ä¸­æ··åˆ
                "åœ¨å°åŒ—æ‰¾developerå·¥ä½œ",  # åœ°å+è‹±æ–‡è·ä½
                "I want to work in å°ç£",  # è‹±æ–‡+ä¸­æ–‡åœ°å
                "å°‹æ‰¾remote workæ©Ÿæœƒ",  # ä¸­æ–‡+è‹±æ–‡æ¦‚å¿µ
            ]
        }
    
    def _load_entities(self) -> Dict[str, List[str]]:
        """è¼‰å…¥å¯¦é«”è©å…¸"""
        return {
            "job_title": [
                "è»Ÿé«”å·¥ç¨‹å¸«", "è³‡æ–™ç§‘å­¸å®¶", "ç”¢å“ç¶“ç†", "è¨­è¨ˆå¸«", "è¡ŒéŠ·å°ˆå“¡",
                "æ¥­å‹™ä»£è¡¨", "æœƒè¨ˆå¸«", "äººè³‡å°ˆå“¡", "å°ˆæ¡ˆç¶“ç†", "ç³»çµ±ç®¡ç†å“¡",
                "software engineer", "data scientist", "product manager", 
                "designer", "marketing specialist", "sales representative",
                "accountant", "hr specialist", "project manager", "system administrator",
                "å‰ç«¯å·¥ç¨‹å¸«", "å¾Œç«¯å·¥ç¨‹å¸«", "å…¨ç«¯å·¥ç¨‹å¸«", "DevOpså·¥ç¨‹å¸«", "QAå·¥ç¨‹å¸«",
                "UI/UXè¨­è¨ˆå¸«", "æ•¸ä½è¡ŒéŠ·å°ˆå“¡", "å®¢æœå°ˆå“¡", "è²¡å‹™åˆ†æå¸«", "æ³•å‹™å°ˆå“¡"
            ],
            "location": [
                "å°åŒ—", "æ–°åŒ—", "æ¡ƒåœ’", "å°ä¸­", "å°å—", "é«˜é›„", "æ–°ç«¹", "åŸºéš†",
                "å°ç£", "ä¸­åœ‹", "ç¾åœ‹", "æ—¥æœ¬", "æ–°åŠ å¡", "é¦™æ¸¯", "æ¾³æ´²", "åŠ æ‹¿å¤§",
                "Taipei", "New Taipei", "Taoyuan", "Taichung", "Tainan", "Kaohsiung",
                "Taiwan", "China", "USA", "Japan", "Singapore", "Hong Kong",
                "ä¿¡ç¾©å€", "å¤§å®‰å€", "ä¸­å±±å€", "æ¾å±±å€", "å…§æ¹–å€", "å—æ¸¯å€",
                "ç«¹åŒ—", "ç«¹æ±", "è‹—æ —", "å½°åŒ–", "é›²æ—", "å˜‰ç¾©", "å±æ±", "å®œè˜­"
            ],
            "company": [
                "å°ç©é›»", "é´»æµ·", "è¯ç™¼ç§‘", "è¯ç¢©", "å®ç¢", "å»£é”", "ä»å¯¶", "å’Œç¢©",
                "Google", "Microsoft", "Apple", "Amazon", "Facebook", "Netflix",
                "Tesla", "Uber", "Airbnb", "Spotify", "Adobe", "Oracle",
                "é˜¿é‡Œå·´å·´", "é¨°è¨Š", "ç™¾åº¦", "å­—ç¯€è·³å‹•", "ç¾åœ˜", "æ»´æ»´", "å°ç±³", "è¯ç‚º",
                "LINE", "Yahoo", "IBM", "Intel", "NVIDIA", "AMD", "Qualcomm"
            ],
            "industry": [
                "ç§‘æŠ€æ¥­", "é‡‘èæ¥­", "è£½é€ æ¥­", "æœå‹™æ¥­", "é›¶å”®æ¥­", "é†«ç™‚æ¥­", "æ•™è‚²æ¥­",
                "technology", "finance", "manufacturing", "service", "retail", 
                "healthcare", "education", "entertainment", "automotive", "aerospace",
                "åŠå°é«”", "é›»å­æ¥­", "è»Ÿé«”æ¥­", "éŠæˆ²æ¥­", "é›»å•†", "ç‰©æµæ¥­", "æˆ¿åœ°ç”¢",
                "ç”ŸæŠ€æ¥­", "èƒ½æºæ¥­", "è¾²æ¥­", "è§€å…‰æ¥­", "åª’é«”æ¥­", "å»£å‘Šæ¥­", "é¡§å•æ¥­"
            ],
            "experience_level": [
                "æ–°é®®äºº", "1-3å¹´ç¶“é©—", "3-5å¹´ç¶“é©—", "5-10å¹´ç¶“é©—", "10å¹´ä»¥ä¸Šç¶“é©—",
                "junior", "mid-level", "senior", "lead", "principal", "director",
                "å¯¦ç¿’ç”Ÿ", "åˆç´š", "ä¸­ç´š", "é«˜ç´š", "è³‡æ·±", "é¦–å¸­", "ç¸½ç›£", "VP"
            ],
            "skill_name": [
                "Python", "Java", "JavaScript", "React", "Vue.js", "Angular",
                "Node.js", "Django", "Flask", "Spring", "Docker", "Kubernetes",
                "AWS", "Azure", "GCP", "MySQL", "PostgreSQL", "MongoDB",
                "æ©Ÿå™¨å­¸ç¿’", "æ·±åº¦å­¸ç¿’", "è³‡æ–™åˆ†æ", "å°ˆæ¡ˆç®¡ç†", "æºé€šæŠ€å·§", "é ˜å°èƒ½åŠ›",
                "è‹±æ–‡", "æ—¥æ–‡", "éŸ“æ–‡", "å¾·æ–‡", "æ³•æ–‡", "è¥¿ç­ç‰™æ–‡"
            ],
            "technology": [
                "React", "Vue", "Angular", "Python", "Java", "C++", "Go", "Rust",
                "Docker", "Kubernetes", "Terraform", "Jenkins", "GitLab CI",
                "TensorFlow", "PyTorch", "Scikit-learn", "Pandas", "NumPy",
                "å€å¡Šéˆ", "äººå·¥æ™ºæ…§", "ç‰©è¯ç¶²", "é›²ç«¯é‹ç®—", "å¤§æ•¸æ“š", "DevOps"
            ],
            "salary_range": [
                "30-50è¬", "50-80è¬", "80-120è¬", "120-200è¬", "200è¬ä»¥ä¸Š",
                "$30k-50k", "$50k-80k", "$80k-120k", "$120k-200k", "$200k+",
                "æœˆè–ª3-5è¬", "æœˆè–ª5-8è¬", "æœˆè–ª8-12è¬", "æœˆè–ª12-20è¬", "æœˆè–ª20è¬ä»¥ä¸Š"
            ],
            "age": ["25", "30", "35", "40", "45", "50", "55"]
        }
    
    def _load_patterns(self) -> Dict[str, List[str]]:
        """è¼‰å…¥æŸ¥è©¢æ¨¡å¼"""
        return {
            "question_patterns": [
                "ä»€éº¼æ˜¯{}", "å¦‚ä½•{}", "ç‚ºä»€éº¼{}", "å“ªè£¡å¯ä»¥{}", "èª°èƒ½{}",
                "What is {}", "How to {}", "Why {}", "Where can {}", "Who can {}"
            ],
            "request_patterns": [
                "æˆ‘æƒ³è¦{}", "æˆ‘éœ€è¦{}", "è«‹å¹«æˆ‘{}", "å¯ä»¥çµ¦æˆ‘{}", "æˆ‘è¦æ‰¾{}",
                "I want {}", "I need {}", "Please help me {}", "Can you give me {}", "I'm looking for {}"
            ],
            "comparison_patterns": [
                "{}å’Œ{}å“ªå€‹æ¯”è¼ƒå¥½", "æ¯”è¼ƒ{}èˆ‡{}", "{}vs{}",
                "Which is better {} or {}", "Compare {} and {}", "{} vs {}"
            ],
            "conditional_patterns": [
                "å¦‚æœ{}çš„è©±", "å‡è¨­{}", "ç•¶{}æ™‚", "åœ¨{}æƒ…æ³ä¸‹",
                "If {}", "Suppose {}", "When {}", "In case of {}"
            ]
        }
    
    def _load_augmentation_rules(self) -> Dict[str, List[Tuple[str, str]]]:
        """è¼‰å…¥æ•¸æ“šå¢å¼·è¦å‰‡"""
        return {
            "synonym_replacement": [
                ("å·¥ä½œ", "è·ä½"), ("å·¥ä½œ", "è·ç¼º"), ("å·¥ä½œ", "å°±æ¥­æ©Ÿæœƒ"),
                ("æ‰¾", "æœå°‹"), ("æ‰¾", "å°‹æ‰¾"), ("æ‰¾", "æŸ¥æ‰¾"),
                ("è–ªæ°´", "è–ªè³‡"), ("è–ªæ°´", "å¾…é‡"), ("è–ªæ°´", "æ”¶å…¥"),
                ("å…¬å¸", "ä¼æ¥­"), ("å…¬å¸", "çµ„ç¹”"), ("å…¬å¸", "æ©Ÿæ§‹"),
                ("job", "position"), ("job", "role"), ("job", "career"),
                ("find", "search"), ("find", "look for"), ("find", "seek")
            ],
            "insertion": [
                ("è«‹", "éº»ç…©"), ("å¯ä»¥", "èƒ½å¤ "), ("æƒ³è¦", "å¸Œæœ›"),
                ("please", "kindly"), ("can", "could"), ("want", "would like")
            ],
            "deletion": [
                "è«‹", "éº»ç…©", "å¯ä»¥", "èƒ½å¤ ", "çš„è©±", "ä¸€ä¸‹",
                "please", "kindly", "just", "really", "actually"
            ],
            "reordering": [
                # è©åºèª¿æ•´è¦å‰‡
                (r"æˆ‘æƒ³æ‰¾(.+)çš„å·¥ä½œ", r"æ‰¾(.+)å·¥ä½œ"),
                (r"I want to find (.+) job", r"Looking for (.+) job")
            ]
        }
    
    def generate_dataset(self, config: DatasetConfig) -> GeneratedDataset:
        """ç”Ÿæˆæ•¸æ“šé›†"""
        print(f"ğŸ¯ é–‹å§‹ç”Ÿæˆ {config.dataset_type.value} æ•¸æ“šé›†...")
        print(f"   ç›®æ¨™å¤§å°: {config.target_size}")
        print(f"   ç”Ÿæˆç­–ç•¥: {[s.value for s in config.generation_strategies]}")
        
        start_time = datetime.now()
        
        # åˆå§‹åŒ–æ•¸æ“šé›†
        dataset = GeneratedDataset(
            dataset_id=f"{config.dataset_type.value}_{int(start_time.timestamp())}",
            dataset_type=config.dataset_type,
            test_cases=[],
            generation_time=start_time
        )
        
        # è¨ˆç®—å„é¡åˆ¥ç›®æ¨™æ•¸é‡
        category_targets = self._calculate_category_targets(config)
        
        # æŒ‰ç­–ç•¥ç”Ÿæˆæ¸¬è©¦æ¡ˆä¾‹
        for strategy in config.generation_strategies:
            print(f"   ğŸ”„ ä½¿ç”¨ç­–ç•¥: {strategy.value}")
            
            if strategy == GenerationStrategy.TEMPLATE_BASED:
                cases = self._generate_template_based(category_targets, config)
            elif strategy == GenerationStrategy.COMBINATORIAL:
                cases = self._generate_combinatorial(category_targets, config)
            elif strategy == GenerationStrategy.RULE_BASED:
                cases = self._generate_rule_based(category_targets, config)
            elif strategy == GenerationStrategy.SYNTHETIC:
                cases = self._generate_synthetic(category_targets, config)
            elif strategy == GenerationStrategy.AUGMENTATION:
                cases = self._generate_augmentation(dataset.test_cases, config)
            elif strategy == GenerationStrategy.ADVERSARIAL:
                cases = self._generate_adversarial(category_targets, config)
            else:
                cases = []
            
            dataset.test_cases.extend(cases)
            print(f"     âœ… ç”Ÿæˆ {len(cases)} å€‹æ¡ˆä¾‹")
        
        # å»é‡å’Œéæ¿¾
        dataset.test_cases = self._deduplicate_and_filter(dataset.test_cases, config)
        
        # èª¿æ•´åˆ°ç›®æ¨™å¤§å°
        dataset.test_cases = self._adjust_to_target_size(dataset.test_cases, config.target_size)
        
        # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
        dataset.statistics = self._calculate_statistics(dataset.test_cases)
        
        # è¨ˆç®—è³ªé‡å’Œå¤šæ¨£æ€§åˆ†æ•¸
        dataset.quality_score = self._calculate_quality_score(dataset.test_cases)
        dataset.diversity_score = self._calculate_diversity_score(dataset.test_cases)
        
        # æ·»åŠ å…ƒæ•¸æ“š
        dataset.metadata = {
            "config": {
                "dataset_type": config.dataset_type.value,
                "target_size": config.target_size,
                "strategies": [s.value for s in config.generation_strategies],
                "complexity_distribution": config.complexity_distribution,
                "language_distribution": config.language_distribution,
                "category_distribution": config.category_distribution
            },
            "generation_info": {
                "generation_time": start_time.isoformat(),
                "duration_seconds": (datetime.now() - start_time).total_seconds(),
                "actual_size": len(dataset.test_cases)
            }
        }
        
        print(f"   âœ… æ•¸æ“šé›†ç”Ÿæˆå®Œæˆ: {len(dataset.test_cases)} å€‹æ¡ˆä¾‹")
        print(f"   ğŸ“Š è³ªé‡åˆ†æ•¸: {dataset.quality_score:.3f}")
        print(f"   ğŸ¨ å¤šæ¨£æ€§åˆ†æ•¸: {dataset.diversity_score:.3f}")
        
        return dataset
    
    def _calculate_category_targets(self, config: DatasetConfig) -> Dict[str, int]:
        """è¨ˆç®—å„é¡åˆ¥ç›®æ¨™æ•¸é‡"""
        targets = {}
        for category, ratio in config.category_distribution.items():
            targets[category] = int(config.target_size * ratio)
        return targets
    
    def _generate_template_based(self, category_targets: Dict[str, int], 
                               config: DatasetConfig) -> List[Dict[str, Any]]:
        """åŸºæ–¼æ¨¡æ¿ç”Ÿæˆ"""
        cases = []
        
        for category, target_count in category_targets.items():
            if category not in self.templates:
                continue
                
            templates = self.templates[category]
            generated_count = 0
            
            while generated_count < target_count:
                template = random.choice(templates)
                
                # å¡«å……æ¨¡æ¿
                filled_query = self._fill_template(template)
                
                if filled_query and filled_query not in self.generated_queries:
                    case = self._create_test_case(
                        query=filled_query,
                        category=category,
                        complexity=self._sample_complexity(config.complexity_distribution),
                        language=self._detect_language(filled_query),
                        generation_method="template_based",
                        template=template
                    )
                    
                    cases.append(case)
                    self.generated_queries.add(filled_query)
                    generated_count += 1
        
        return cases
    
    def _generate_combinatorial(self, category_targets: Dict[str, int], 
                              config: DatasetConfig) -> List[Dict[str, Any]]:
        """çµ„åˆå¼ç”Ÿæˆ"""
        cases = []
        
        # ç”Ÿæˆå¯¦é«”çµ„åˆ
        entity_combinations = self._generate_entity_combinations()
        
        for category, target_count in category_targets.items():
            if category not in self.templates:
                continue
                
            generated_count = 0
            templates = self.templates[category]
            
            for template in templates:
                if generated_count >= target_count:
                    break
                    
                # ç‚ºæ¯å€‹æ¨¡æ¿ç”Ÿæˆå¤šå€‹çµ„åˆ
                combinations_per_template = min(10, target_count - generated_count)
                
                for _ in range(combinations_per_template):
                    combination = random.choice(entity_combinations)
                    filled_query = self._fill_template_with_combination(template, combination)
                    
                    if filled_query and filled_query not in self.generated_queries:
                        case = self._create_test_case(
                            query=filled_query,
                            category=category,
                            complexity=self._sample_complexity(config.complexity_distribution),
                            language=self._detect_language(filled_query),
                            generation_method="combinatorial",
                            template=template,
                            entities=combination
                        )
                        
                        cases.append(case)
                        self.generated_queries.add(filled_query)
                        generated_count += 1
        
        return cases
    
    def _generate_rule_based(self, category_targets: Dict[str, int], 
                           config: DatasetConfig) -> List[Dict[str, Any]]:
        """åŸºæ–¼è¦å‰‡ç”Ÿæˆ"""
        cases = []
        
        # å®šç¾©ç”Ÿæˆè¦å‰‡
        rules = {
            "job_search": [
                lambda: f"æˆ‘æƒ³æ‰¾{random.choice(self.entities['job_title'])}å·¥ä½œåœ¨{random.choice(self.entities['location'])}",
                lambda: f"å°‹æ‰¾{random.choice(self.entities['experience_level'])}çš„{random.choice(self.entities['job_title'])}è·ä½",
                lambda: f"{random.choice(self.entities['company'])}æœ‰{random.choice(self.entities['job_title'])}çš„ç©ºç¼ºå—"
            ],
            "skill_query": [
                lambda: f"å­¸ç¿’{random.choice(self.entities['technology'])}éœ€è¦å¤šä¹…æ™‚é–“",
                lambda: f"{random.choice(self.entities['job_title'])}éœ€è¦æœƒ{random.choice(self.entities['skill_name'])}å—",
                lambda: f"å¦‚ä½•å¿«é€ŸæŒæ¡{random.choice(self.entities['technology'])}æŠ€è¡“"
            ],
            "salary_query": [
                lambda: f"{random.choice(self.entities['location'])}çš„{random.choice(self.entities['job_title'])}è–ªæ°´å¤šå°‘",
                lambda: f"æˆ‘æƒ³è¦{random.choice(self.entities['salary_range'])}çš„å·¥ä½œ",
                lambda: f"{random.choice(self.entities['experience_level'])}çš„è–ªè³‡ç¯„åœæ˜¯å¤šå°‘"
            ]
        }
        
        for category, target_count in category_targets.items():
            if category not in rules:
                continue
                
            category_rules = rules[category]
            generated_count = 0
            
            while generated_count < target_count:
                rule = random.choice(category_rules)
                query = rule()
                
                if query and query not in self.generated_queries:
                    case = self._create_test_case(
                        query=query,
                        category=category,
                        complexity=self._sample_complexity(config.complexity_distribution),
                        language=self._detect_language(query),
                        generation_method="rule_based"
                    )
                    
                    cases.append(case)
                    self.generated_queries.add(query)
                    generated_count += 1
        
        return cases
    
    def _generate_synthetic(self, category_targets: Dict[str, int], 
                          config: DatasetConfig) -> List[Dict[str, Any]]:
        """åˆæˆå¼ç”Ÿæˆ"""
        cases = []
        
        # ç”ŸæˆåˆæˆæŸ¥è©¢
        for category, target_count in category_targets.items():
            generated_count = 0
            
            while generated_count < target_count:
                # éš¨æ©Ÿçµ„åˆè©å½™ç”Ÿæˆæ–°æŸ¥è©¢
                query = self._generate_synthetic_query(category)
                
                if query and query not in self.generated_queries:
                    case = self._create_test_case(
                        query=query,
                        category=category,
                        complexity=self._sample_complexity(config.complexity_distribution),
                        language=self._detect_language(query),
                        generation_method="synthetic"
                    )
                    
                    cases.append(case)
                    self.generated_queries.add(query)
                    generated_count += 1
        
        return cases
    
    def _generate_augmentation(self, existing_cases: List[Dict[str, Any]], 
                             config: DatasetConfig) -> List[Dict[str, Any]]:
        """æ•¸æ“šå¢å¼·ç”Ÿæˆ"""
        if not existing_cases:
            return []
        
        cases = []
        augmentation_count = min(len(existing_cases), config.target_size // 4)  # å¢å¼·25%
        
        for _ in range(augmentation_count):
            base_case = random.choice(existing_cases)
            base_query = base_case["query"]
            
            # æ‡‰ç”¨å¢å¼·æŠ€è¡“
            augmented_queries = []
            
            # åŒç¾©è©æ›¿æ›
            augmented_queries.append(self._apply_synonym_replacement(base_query))
            
            # æ’å…¥è©å½™
            augmented_queries.append(self._apply_insertion(base_query))
            
            # åˆªé™¤è©å½™
            augmented_queries.append(self._apply_deletion(base_query))
            
            # è©åºèª¿æ•´
            augmented_queries.append(self._apply_reordering(base_query))
            
            for aug_query in augmented_queries:
                if aug_query and aug_query != base_query and aug_query not in self.generated_queries:
                    case = self._create_test_case(
                        query=aug_query,
                        category=base_case["category"],
                        complexity=base_case["complexity"],
                        language=self._detect_language(aug_query),
                        generation_method="augmentation",
                        base_case_id=base_case["id"]
                    )
                    
                    cases.append(case)
                    self.generated_queries.add(aug_query)
        
        return cases
    
    def _generate_adversarial(self, category_targets: Dict[str, int], 
                            config: DatasetConfig) -> List[Dict[str, Any]]:
        """å°æŠ—å¼ç”Ÿæˆ"""
        cases = []
        
        # ç”Ÿæˆå°æŠ—æ€§æ¸¬è©¦æ¡ˆä¾‹
        adversarial_patterns = [
            # é‚Šç•Œæ¡ˆä¾‹
            "",  # ç©ºæŸ¥è©¢
            "   ",  # åªæœ‰ç©ºæ ¼
            "a" * 1000,  # è¶…é•·æŸ¥è©¢
            "å·¥ä½œ" * 50,  # é‡è¤‡è©å½™
            
            # ç‰¹æ®Šå­—ç¬¦
            "!@#$%^&*()",
            "<script>alert('test')</script>",
            "SELECT * FROM jobs",
            "../../../etc/passwd",
            
            # æ··æ·†æŸ¥è©¢
            "æˆ‘æˆ‘æˆ‘æƒ³æƒ³æ‰¾æ‰¾å·¥å·¥ä½œä½œ",
            "å·¥ä½œæ‰¾æˆ‘æƒ³",
            "JOB job Job jOb",
            
            # å¤šèªè¨€æ··åˆ
            "æˆ‘wantæ‰¾jobåœ¨å°åŒ—",
            "Looking forå·¥ä½œinå°ç£",
            "ä»•äº‹ã‚’æ¢ã—ã¦ã„ã‚‹",
            "ì°¾ê³  ìˆëŠ” ì§ì—…",
        ]
        
        for pattern in adversarial_patterns:
            case = self._create_test_case(
                query=pattern,
                category="boundary_cases",
                complexity="extreme",
                language="mixed" if self._is_multilingual(pattern) else self._detect_language(pattern),
                generation_method="adversarial",
                is_adversarial=True
            )
            
            cases.append(case)
            self.generated_queries.add(pattern)
        
        return cases
    
    def _fill_template(self, template: str) -> str:
        """å¡«å……æ¨¡æ¿"""
        filled = template
        
        # æŸ¥æ‰¾æ‰€æœ‰ä½”ä½ç¬¦
        placeholders = re.findall(r'\{([^}]+)\}', template)
        
        for placeholder in placeholders:
            if placeholder in self.entities:
                replacement = random.choice(self.entities[placeholder])
                filled = filled.replace(f"{{{placeholder}}}", replacement)
        
        return filled
    
    def _fill_template_with_combination(self, template: str, combination: Dict[str, str]) -> str:
        """ä½¿ç”¨çµ„åˆå¡«å……æ¨¡æ¿"""
        filled = template
        
        for placeholder, value in combination.items():
            filled = filled.replace(f"{{{placeholder}}}", value)
        
        return filled
    
    def _generate_entity_combinations(self) -> List[Dict[str, str]]:
        """ç”Ÿæˆå¯¦é«”çµ„åˆ"""
        combinations = []
        
        # ç”Ÿæˆå¸¸è¦‹çµ„åˆ
        for _ in range(100):
            combination = {}
            
            # éš¨æ©Ÿé¸æ“‡å¯¦é«”
            for entity_type, values in self.entities.items():
                if random.random() < 0.3:  # 30%æ©Ÿç‡åŒ…å«æ­¤å¯¦é«”
                    combination[entity_type] = random.choice(values)
            
            if combination:
                combinations.append(combination)
        
        return combinations
    
    def _generate_synthetic_query(self, category: str) -> str:
        """ç”ŸæˆåˆæˆæŸ¥è©¢"""
        # æ ¹æ“šé¡åˆ¥ç”ŸæˆåˆæˆæŸ¥è©¢
        if category == "job_search":
            parts = [
                random.choice(["æˆ‘æƒ³", "æˆ‘è¦", "å°‹æ‰¾", "æ‰¾", "æœå°‹"]),
                random.choice(self.entities["job_title"]),
                random.choice(["å·¥ä½œ", "è·ä½", "è·ç¼º", "æ©Ÿæœƒ"]),
                random.choice(["åœ¨", "æ–¼", "ä½æ–¼"]) if random.random() < 0.5 else "",
                random.choice(self.entities["location"]) if random.random() < 0.5 else ""
            ]
        elif category == "skill_query":
            parts = [
                random.choice(["å¦‚ä½•", "æ€éº¼", "æ€æ¨£"]),
                random.choice(["å­¸ç¿’", "æŒæ¡", "æå‡"]),
                random.choice(self.entities["skill_name"]),
                random.choice(["æŠ€èƒ½", "èƒ½åŠ›", "æŠ€è¡“"])
            ]
        else:
            # é»˜èªç”Ÿæˆ
            parts = [
                random.choice(["æˆ‘æƒ³", "è«‹å•", "å¦‚ä½•"]),
                random.choice(self.entities["job_title"]),
                random.choice(["ç›¸é—œ", "æœ‰é—œ", "é—œæ–¼"]),
                random.choice(["å•é¡Œ", "è³‡è¨Š", "è³‡æ–™"])
            ]
        
        return " ".join([part for part in parts if part])
    
    def _apply_synonym_replacement(self, query: str) -> str:
        """æ‡‰ç”¨åŒç¾©è©æ›¿æ›"""
        result = query
        
        for original, replacement in self.augmentation_rules["synonym_replacement"]:
            if original in result:
                result = result.replace(original, replacement)
                break  # åªæ›¿æ›ä¸€æ¬¡
        
        return result
    
    def _apply_insertion(self, query: str) -> str:
        """æ‡‰ç”¨æ’å…¥"""
        insertions = self.augmentation_rules["insertion"]
        
        if insertions and random.random() < 0.5:
            insertion, _ = random.choice(insertions)
            words = query.split()
            if words:
                pos = random.randint(0, len(words))
                words.insert(pos, insertion)
                return " ".join(words)
        
        return query
    
    def _apply_deletion(self, query: str) -> str:
        """æ‡‰ç”¨åˆªé™¤"""
        deletions = self.augmentation_rules["deletion"]
        
        result = query
        for deletion in deletions:
            if deletion in result:
                result = result.replace(deletion, "", 1)  # åªåˆªé™¤ä¸€æ¬¡
                break
        
        return result.strip()
    
    def _apply_reordering(self, query: str) -> str:
        """æ‡‰ç”¨è©åºèª¿æ•´"""
        words = query.split()
        
        if len(words) > 2 and random.random() < 0.3:
            # éš¨æ©Ÿäº¤æ›å…©å€‹è©çš„ä½ç½®
            i, j = random.sample(range(len(words)), 2)
            words[i], words[j] = words[j], words[i]
        
        return " ".join(words)
    
    def _create_test_case(self, query: str, category: str, complexity: str, 
                         language: str, generation_method: str, **kwargs) -> Dict[str, Any]:
        """å‰µå»ºæ¸¬è©¦æ¡ˆä¾‹"""
        case_id = f"{category}_{len(self.generated_queries)}_{int(datetime.now().timestamp())}"
        
        case = {
            "id": case_id,
            "query": query,
            "category": category,
            "complexity": complexity,
            "language": language,
            "generation_method": generation_method,
            "expected_intent": self._infer_expected_intent(query, category),
            "metadata": {
                "query_length": len(query),
                "word_count": len(query.split()),
                "has_special_chars": bool(re.search(r'[^\w\s\u4e00-\u9fff]', query)),
                "is_question": query.strip().endswith('?') or any(q in query for q in ['ä»€éº¼', 'å¦‚ä½•', 'ç‚ºä»€éº¼', 'what', 'how', 'why']),
                "generation_time": datetime.now().isoformat()
            }
        }
        
        # æ·»åŠ é¡å¤–å…ƒæ•¸æ“š
        case["metadata"].update(kwargs)
        
        return case
    
    def _sample_complexity(self, distribution: Dict[str, float]) -> str:
        """æ ¹æ“šåˆ†ä½ˆæ¡æ¨£è¤‡é›œåº¦"""
        rand = random.random()
        cumulative = 0
        
        for complexity, prob in distribution.items():
            cumulative += prob
            if rand <= cumulative:
                return complexity
        
        return "medium"  # é»˜èª
    
    def _detect_language(self, query: str) -> str:
        """æª¢æ¸¬æŸ¥è©¢èªè¨€"""
        if not query:
            return "unknown"
        
        # ç°¡å–®çš„èªè¨€æª¢æ¸¬
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', query))
        english_chars = len(re.findall(r'[a-zA-Z]', query))
        total_chars = len(query.replace(' ', ''))
        
        if total_chars == 0:
            return "unknown"
        
        chinese_ratio = chinese_chars / total_chars
        english_ratio = english_chars / total_chars
        
        if chinese_ratio > 0.5:
            return "chinese"
        elif english_ratio > 0.5:
            return "english"
        elif chinese_ratio > 0.1 and english_ratio > 0.1:
            return "mixed"
        else:
            return "other"
    
    def _is_multilingual(self, query: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºå¤šèªè¨€æŸ¥è©¢"""
        return self._detect_language(query) == "mixed"
    
    def _infer_expected_intent(self, query: str, category: str) -> str:
        """æ¨æ–·æœŸæœ›æ„åœ–"""
        # åŸºæ–¼é¡åˆ¥å’ŒæŸ¥è©¢å…§å®¹æ¨æ–·æ„åœ–
        intent_mapping = {
            "job_search": "job_search",
            "skill_query": "skill_inquiry",
            "location_query": "location_search",
            "salary_query": "salary_inquiry",
            "career_transition": "career_advice",
            "remote_work": "remote_job_search",
            "non_job_related": "non_job_related",
            "boundary_cases": "unclear",
            "multilingual": "job_search"  # å¤§å¤šæ•¸å¤šèªè¨€æŸ¥è©¢æ˜¯æ±‚è·ç›¸é—œ
        }
        
        return intent_mapping.get(category, "unknown")
    
    def _deduplicate_and_filter(self, test_cases: List[Dict[str, Any]], 
                               config: DatasetConfig) -> List[Dict[str, Any]]:
        """å»é‡å’Œéæ¿¾"""
        print("   ğŸ”„ å»é‡å’Œéæ¿¾æ¸¬è©¦æ¡ˆä¾‹...")
        
        # å»é‡
        seen_queries = set()
        unique_cases = []
        
        for case in test_cases:
            query = case["query"]
            if query not in seen_queries:
                seen_queries.add(query)
                unique_cases.append(case)
        
        print(f"     å»é‡å¾Œ: {len(unique_cases)} å€‹æ¡ˆä¾‹")
        
        # è³ªé‡éæ¿¾
        if config.enable_validation:
            filtered_cases = []
            
            for case in unique_cases:
                quality_score = self._calculate_case_quality(case)
                if quality_score >= config.quality_threshold:
                    filtered_cases.append(case)
            
            print(f"     è³ªé‡éæ¿¾å¾Œ: {len(filtered_cases)} å€‹æ¡ˆä¾‹")
            return filtered_cases
        
        return unique_cases
    
    def _calculate_case_quality(self, case: Dict[str, Any]) -> float:
        """è¨ˆç®—æ¡ˆä¾‹è³ªé‡åˆ†æ•¸"""
        query = case["query"]
        
        if not query or not query.strip():
            return 0.0
        
        score = 1.0
        
        # é•·åº¦æª¢æŸ¥
        if len(query) < 2:
            score -= 0.3
        elif len(query) > 500:
            score -= 0.2
        
        # é‡è¤‡å­—ç¬¦æª¢æŸ¥
        if len(set(query)) / len(query) < 0.3:
            score -= 0.3
        
        # ç‰¹æ®Šå­—ç¬¦æ¯”ä¾‹
        special_char_ratio = len(re.findall(r'[^\w\s\u4e00-\u9fff]', query)) / len(query)
        if special_char_ratio > 0.5:
            score -= 0.4
        
        # èªè¨€ä¸€è‡´æ€§
        if case["language"] == "unknown":
            score -= 0.2
        
        return max(0.0, score)
    
    def _adjust_to_target_size(self, test_cases: List[Dict[str, Any]], 
                              target_size: int) -> List[Dict[str, Any]]:
        """èª¿æ•´åˆ°ç›®æ¨™å¤§å°"""
        if len(test_cases) > target_size:
            # éš¨æ©Ÿæ¡æ¨£
            return random.sample(test_cases, target_size)
        elif len(test_cases) < target_size:
            # å¦‚æœä¸è¶³ï¼Œé‡è¤‡ä¸€äº›æ¡ˆä¾‹ï¼ˆæ·»åŠ è®Šé«”ï¼‰
            additional_needed = target_size - len(test_cases)
            additional_cases = []
            
            for _ in range(additional_needed):
                base_case = random.choice(test_cases)
                # å‰µå»ºè®Šé«”
                variant = base_case.copy()
                variant["id"] = f"{base_case['id']}_variant_{len(additional_cases)}"
                variant["query"] = self._create_query_variant(base_case["query"])
                additional_cases.append(variant)
            
            return test_cases + additional_cases
        
        return test_cases
    
    def _create_query_variant(self, original_query: str) -> str:
        """å‰µå»ºæŸ¥è©¢è®Šé«”"""
        # ç°¡å–®çš„è®Šé«”ç”Ÿæˆ
        variants = [
            self._apply_synonym_replacement(original_query),
            self._apply_insertion(original_query),
            self._apply_deletion(original_query)
        ]
        
        # é¸æ“‡èˆ‡åŸæŸ¥è©¢ä¸åŒçš„è®Šé«”
        for variant in variants:
            if variant != original_query:
                return variant
        
        return original_query
    
    def _calculate_statistics(self, test_cases: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è¨ˆç®—çµ±è¨ˆä¿¡æ¯"""
        if not test_cases:
            return {}
        
        # é¡åˆ¥åˆ†ä½ˆ
        category_counts = defaultdict(int)
        complexity_counts = defaultdict(int)
        language_counts = defaultdict(int)
        generation_method_counts = defaultdict(int)
        
        query_lengths = []
        word_counts = []
        
        for case in test_cases:
            category_counts[case["category"]] += 1
            complexity_counts[case["complexity"]] += 1
            language_counts[case["language"]] += 1
            generation_method_counts[case["generation_method"]] += 1
            
            query_lengths.append(len(case["query"]))
            word_counts.append(len(case["query"].split()))
        
        return {
            "total_cases": len(test_cases),
            "category_distribution": dict(category_counts),
            "complexity_distribution": dict(complexity_counts),
            "language_distribution": dict(language_counts),
            "generation_method_distribution": dict(generation_method_counts),
            "query_length_stats": {
                "min": min(query_lengths) if query_lengths else 0,
                "max": max(query_lengths) if query_lengths else 0,
                "avg": sum(query_lengths) / len(query_lengths) if query_lengths else 0
            },
            "word_count_stats": {
                "min": min(word_counts) if word_counts else 0,
                "max": max(word_counts) if word_counts else 0,
                "avg": sum(word_counts) / len(word_counts) if word_counts else 0
            }
        }
    
    def _calculate_quality_score(self, test_cases: List[Dict[str, Any]]) -> float:
        """è¨ˆç®—æ•´é«”è³ªé‡åˆ†æ•¸"""
        if not test_cases:
            return 0.0
        
        total_score = 0.0
        
        for case in test_cases:
            case_score = self._calculate_case_quality(case)
            total_score += case_score
        
        return total_score / len(test_cases)
    
    def _calculate_diversity_score(self, test_cases: List[Dict[str, Any]]) -> float:
        """è¨ˆç®—å¤šæ¨£æ€§åˆ†æ•¸"""
        if not test_cases:
            return 0.0
        
        # è¨ˆç®—å„ç¶­åº¦çš„å¤šæ¨£æ€§
        categories = set(case["category"] for case in test_cases)
        complexities = set(case["complexity"] for case in test_cases)
        languages = set(case["language"] for case in test_cases)
        
        # æŸ¥è©¢ç›¸ä¼¼æ€§
        unique_queries = set(case["query"] for case in test_cases)
        query_diversity = len(unique_queries) / len(test_cases)
        
        # ç¶œåˆå¤šæ¨£æ€§åˆ†æ•¸
        diversity_score = (
            len(categories) / 10 * 0.3 +  # é¡åˆ¥å¤šæ¨£æ€§
            len(complexities) / 4 * 0.2 +  # è¤‡é›œåº¦å¤šæ¨£æ€§
            len(languages) / 5 * 0.2 +  # èªè¨€å¤šæ¨£æ€§
            query_diversity * 0.3  # æŸ¥è©¢å¤šæ¨£æ€§
        )
        
        return min(1.0, diversity_score)
    
    def export_dataset(self, dataset: GeneratedDataset, output_file: str = None) -> str:
        """å°å‡ºæ•¸æ“šé›†"""
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"dataset_{dataset.dataset_type.value}_{timestamp}.json"
        
        print(f"ğŸ’¾ å°å‡ºæ•¸æ“šé›†åˆ° {output_file}...")
        
        export_data = {
            "dataset_info": {
                "dataset_id": dataset.dataset_id,
                "dataset_type": dataset.dataset_type.value,
                "generation_time": dataset.generation_time.isoformat() if dataset.generation_time else None,
                "quality_score": dataset.quality_score,
                "diversity_score": dataset.diversity_score
            },
            "metadata": dataset.metadata,
            "statistics": dataset.statistics,
            "test_cases": dataset.test_cases
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        print(f"   âœ… æ•¸æ“šé›†å·²å°å‡º: {len(dataset.test_cases)} å€‹æ¡ˆä¾‹")
        return output_file
    
    def print_dataset_summary(self, dataset: GeneratedDataset) -> None:
        """æ‰“å°æ•¸æ“šé›†æ‘˜è¦"""
        print("\n" + "=" * 60)
        print(f"ğŸ“Š æ•¸æ“šé›†æ‘˜è¦: {dataset.dataset_type.value}")
        print("=" * 60)
        
        print(f"\nğŸ“ˆ åŸºæœ¬ä¿¡æ¯:")
        print(f"   æ•¸æ“šé›†ID: {dataset.dataset_id}")
        print(f"   æ¡ˆä¾‹æ•¸é‡: {len(dataset.test_cases)}")
        print(f"   è³ªé‡åˆ†æ•¸: {dataset.quality_score:.3f}")
        print(f"   å¤šæ¨£æ€§åˆ†æ•¸: {dataset.diversity_score:.3f}")
        
        if dataset.statistics:
            stats = dataset.statistics
            
            print(f"\nğŸ“Š åˆ†ä½ˆçµ±è¨ˆ:")
            
            # é¡åˆ¥åˆ†ä½ˆ
            if "category_distribution" in stats:
                print(f"   é¡åˆ¥åˆ†ä½ˆ:")
                for category, count in stats["category_distribution"].items():
                    percentage = count / stats["total_cases"] * 100
                    print(f"     {category}: {count} ({percentage:.1f}%)")
            
            # è¤‡é›œåº¦åˆ†ä½ˆ
            if "complexity_distribution" in stats:
                print(f"   è¤‡é›œåº¦åˆ†ä½ˆ:")
                for complexity, count in stats["complexity_distribution"].items():
                    percentage = count / stats["total_cases"] * 100
                    print(f"     {complexity}: {count} ({percentage:.1f}%)")
            
            # èªè¨€åˆ†ä½ˆ
            if "language_distribution" in stats:
                print(f"   èªè¨€åˆ†ä½ˆ:")
                for language, count in stats["language_distribution"].items():
                    percentage = count / stats["total_cases"] * 100
                    print(f"     {language}: {count} ({percentage:.1f}%)")
            
            # æŸ¥è©¢é•·åº¦çµ±è¨ˆ
            if "query_length_stats" in stats:
                length_stats = stats["query_length_stats"]
                print(f"   æŸ¥è©¢é•·åº¦çµ±è¨ˆ:")
                print(f"     æœ€çŸ­: {length_stats['min']} å­—ç¬¦")
                print(f"     æœ€é•·: {length_stats['max']} å­—ç¬¦")
                print(f"     å¹³å‡: {length_stats['avg']:.1f} å­—ç¬¦")
        
        print("\n" + "=" * 60)


def main():
    """ä¸»å‡½æ•¸ - æ•¸æ“šé›†ç”Ÿæˆå™¨å…¥å£é»"""
    print("ğŸ¯ LLMæ¸¬è©¦æ•¸æ“šé›†ç”Ÿæˆå™¨")
    print("=" * 60)
    
    try:
        # é¸æ“‡æ•¸æ“šé›†é¡å‹
        print("\nğŸ“Š é¸æ“‡æ•¸æ“šé›†é¡å‹:")
        dataset_types = list(DatasetType)
        for i, dtype in enumerate(dataset_types, 1):
            print(f"{i}. {dtype.value}")
        
        type_choice = input("è«‹é¸æ“‡æ•¸æ“šé›†é¡å‹ (1-6): ").strip()
        try:
            dataset_type = dataset_types[int(type_choice) - 1]
        except:
            print("ç„¡æ•ˆé¸æ“‡ï¼Œä½¿ç”¨é»˜èªé¡å‹: testing")
            dataset_type = DatasetType.TESTING
        
        # è¨­ç½®ç›®æ¨™å¤§å°
        size_input = input("è«‹è¼¸å…¥ç›®æ¨™æ•¸æ“šé›†å¤§å° (é»˜èª1000): ").strip()
        target_size = int(size_input) if size_input.isdigit() else 1000
        
        # é¸æ“‡ç”Ÿæˆç­–ç•¥
        print("\nğŸ”§ é¸æ“‡ç”Ÿæˆç­–ç•¥:")
        strategies = list(GenerationStrategy)
        for i, strategy in enumerate(strategies, 1):
            print(f"{i}. {strategy.value}")
        
        strategy_input = input("è«‹é¸æ“‡ç­–ç•¥ (ç”¨é€—è™Ÿåˆ†éš”ç·¨è™Ÿï¼Œæˆ–æŒ‰Enteré¸æ“‡æ‰€æœ‰): ").strip()
        
        if strategy_input:
            try:
                indices = [int(x.strip()) - 1 for x in strategy_input.split(",")]
                selected_strategies = [strategies[i] for i in indices if 0 <= i < len(strategies)]
            except:
                print("ç„¡æ•ˆé¸æ“‡ï¼Œä½¿ç”¨æ‰€æœ‰ç­–ç•¥")
                selected_strategies = strategies
        else:
            selected_strategies = strategies
        
        # å‰µå»ºé…ç½®
        config = DatasetConfig(
            dataset_type=dataset_type,
            target_size=target_size,
            generation_strategies=selected_strategies
        )
        
        # å‰µå»ºç”Ÿæˆå™¨
        generator = LLMTestDatasetGenerator()
        
        # ç”Ÿæˆæ•¸æ“šé›†
        print(f"\nğŸš€ é–‹å§‹ç”Ÿæˆæ•¸æ“šé›†...")
        dataset = generator.generate_dataset(config)
        
        # é¡¯ç¤ºæ‘˜è¦
        generator.print_dataset_summary(dataset)
        
        # å°å‡ºæ•¸æ“šé›†
        export_choice = input("\næ˜¯å¦å°å‡ºæ•¸æ“šé›†ï¼Ÿ (y/n): ").strip().lower()
        if export_choice == 'y':
            output_file = generator.export_dataset(dataset)
            print(f"   ğŸ“„ æ•¸æ“šé›†æ–‡ä»¶: {output_file}")
        
        print("\nğŸ‰ æ•¸æ“šé›†ç”Ÿæˆå®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâŒ ç”Ÿæˆéç¨‹è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ ç”Ÿæˆéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()