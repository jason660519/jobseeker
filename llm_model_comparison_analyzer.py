#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMæ¨¡å‹æ¯”è¼ƒåˆ†æå™¨
æ·±å…¥åˆ†æå’Œæ¯”è¼ƒä¸åŒLLMæ¨¡å‹åœ¨å„ç¨®æ¸¬è©¦å ´æ™¯ä¸‹çš„è¡¨ç¾å·®ç•°

Author: JobSpy Team
Date: 2025-01-27
"""

import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from collections import defaultdict, Counter
import statistics
from scipy import stats
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')

# è¨­ç½®ä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False


class ComparisonDimension(Enum):
    """æ¯”è¼ƒç¶­åº¦"""
    ACCURACY = "accuracy"  # æº–ç¢ºæ€§
    SPEED = "speed"  # é€Ÿåº¦
    CONSISTENCY = "consistency"  # ä¸€è‡´æ€§
    ROBUSTNESS = "robustness"  # é­¯æ£’æ€§
    COST_EFFICIENCY = "cost_efficiency"  # æˆæœ¬æ•ˆç‡
    LANGUAGE_SUPPORT = "language_support"  # èªè¨€æ”¯æŒ
    COMPLEXITY_HANDLING = "complexity_handling"  # è¤‡é›œåº¦è™•ç†
    ERROR_RATE = "error_rate"  # éŒ¯èª¤ç‡


class AnalysisType(Enum):
    """åˆ†æé¡å‹"""
    STATISTICAL = "statistical"  # çµ±è¨ˆåˆ†æ
    COMPARATIVE = "comparative"  # æ¯”è¼ƒåˆ†æ
    TREND = "trend"  # è¶¨å‹¢åˆ†æ
    CORRELATION = "correlation"  # ç›¸é—œæ€§åˆ†æ
    PERFORMANCE = "performance"  # æ€§èƒ½åˆ†æ
    QUALITY = "quality"  # è³ªé‡åˆ†æ


@dataclass
class ModelPerformance:
    """æ¨¡å‹æ€§èƒ½æ•¸æ“š"""
    model_name: str
    provider: str
    total_tests: int
    successful_tests: int
    failed_tests: int
    timeout_tests: int
    accuracy_score: float
    avg_response_time: float
    median_response_time: float
    consistency_score: float
    robustness_score: float
    cost_per_request: float
    error_rate: float
    confidence_scores: List[float] = field(default_factory=list)
    response_times: List[float] = field(default_factory=list)
    category_performance: Dict[str, float] = field(default_factory=dict)
    complexity_performance: Dict[str, float] = field(default_factory=dict)
    language_performance: Dict[str, float] = field(default_factory=dict)


@dataclass
class ComparisonResult:
    """æ¯”è¼ƒçµæœ"""
    dimension: ComparisonDimension
    models: List[str]
    scores: List[float]
    rankings: List[int]
    statistical_significance: Dict[str, float]
    effect_size: float
    confidence_interval: Tuple[float, float]
    p_value: float
    interpretation: str


@dataclass
class AnalysisInsight:
    """åˆ†ææ´å¯Ÿ"""
    insight_type: str
    title: str
    description: str
    evidence: Dict[str, Any]
    confidence: float
    recommendations: List[str]
    impact_level: str  # high, medium, low


class LLMModelComparisonAnalyzer:
    """LLMæ¨¡å‹æ¯”è¼ƒåˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.model_performances = {}
        self.comparison_results = {}
        self.analysis_insights = []
        self.raw_data = []
        
    def load_test_results(self, results_files: List[str]) -> None:
        """è¼‰å…¥æ¸¬è©¦çµæœ"""
        print(f"ğŸ“‚ è¼‰å…¥æ¸¬è©¦çµæœæ–‡ä»¶...")
        
        for file_path in results_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                self.raw_data.append(data)
                print(f"   âœ… è¼‰å…¥ {file_path}")
                
            except Exception as e:
                print(f"   âŒ è¼‰å…¥å¤±æ•— {file_path}: {str(e)}")
        
        print(f"   ğŸ“Š ç¸½å…±è¼‰å…¥ {len(self.raw_data)} å€‹çµæœæ–‡ä»¶")
    
    def extract_model_performances(self) -> None:
        """æå–æ¨¡å‹æ€§èƒ½æ•¸æ“š"""
        print("ğŸ” æå–æ¨¡å‹æ€§èƒ½æ•¸æ“š...")
        
        for data in self.raw_data:
            if "batches" not in data:
                continue
            
            # æŒ‰æ¨¡å‹åˆ†çµ„åŸ·è¡Œçµæœ
            model_executions = defaultdict(list)
            
            for batch in data["batches"]:
                for execution in batch.get("executions", []):
                    provider = execution.get("provider", "unknown")
                    model_executions[provider].append(execution)
            
            # è¨ˆç®—æ¯å€‹æ¨¡å‹çš„æ€§èƒ½æŒ‡æ¨™
            for provider, executions in model_executions.items():
                performance = self._calculate_model_performance(provider, executions)
                
                if provider in self.model_performances:
                    # åˆä½µæ€§èƒ½æ•¸æ“š
                    self._merge_performance_data(self.model_performances[provider], performance)
                else:
                    self.model_performances[provider] = performance
        
        print(f"   âœ… æå–äº† {len(self.model_performances)} å€‹æ¨¡å‹çš„æ€§èƒ½æ•¸æ“š")
    
    def _calculate_model_performance(self, provider: str, executions: List[Dict[str, Any]]) -> ModelPerformance:
        """è¨ˆç®—æ¨¡å‹æ€§èƒ½"""
        total_tests = len(executions)
        successful_tests = len([e for e in executions if e.get("status") == "completed"])
        failed_tests = len([e for e in executions if e.get("status") == "failed"])
        timeout_tests = len([e for e in executions if e.get("status") == "timeout"])
        
        # éŸ¿æ‡‰æ™‚é–“çµ±è¨ˆ
        response_times = [e.get("duration", 0) for e in executions if e.get("duration") is not None]
        avg_response_time = statistics.mean(response_times) if response_times else 0
        median_response_time = statistics.median(response_times) if response_times else 0
        
        # ç½®ä¿¡åº¦åˆ†æ•¸
        confidence_scores = []
        for execution in executions:
            result = execution.get("result", {})
            if isinstance(result, dict) and "confidence" in result:
                confidence_scores.append(result["confidence"])
        
        # æº–ç¢ºæ€§åˆ†æ
        accuracy_score = self._calculate_accuracy(executions)
        
        # ä¸€è‡´æ€§åˆ†æ
        consistency_score = self._calculate_consistency(executions)
        
        # é­¯æ£’æ€§åˆ†æ
        robustness_score = self._calculate_robustness(executions)
        
        # é¡åˆ¥æ€§èƒ½
        category_performance = self._calculate_category_performance(executions)
        
        # è¤‡é›œåº¦æ€§èƒ½
        complexity_performance = self._calculate_complexity_performance(executions)
        
        # èªè¨€æ€§èƒ½
        language_performance = self._calculate_language_performance(executions)
        
        return ModelPerformance(
            model_name=provider,
            provider=provider,
            total_tests=total_tests,
            successful_tests=successful_tests,
            failed_tests=failed_tests,
            timeout_tests=timeout_tests,
            accuracy_score=accuracy_score,
            avg_response_time=avg_response_time,
            median_response_time=median_response_time,
            consistency_score=consistency_score,
            robustness_score=robustness_score,
            cost_per_request=self._estimate_cost(provider),
            error_rate=failed_tests / total_tests if total_tests > 0 else 0,
            confidence_scores=confidence_scores,
            response_times=response_times,
            category_performance=category_performance,
            complexity_performance=complexity_performance,
            language_performance=language_performance
        )
    
    def _calculate_accuracy(self, executions: List[Dict[str, Any]]) -> float:
        """è¨ˆç®—æº–ç¢ºæ€§"""
        correct_predictions = 0
        total_predictions = 0
        
        for execution in executions:
            if execution.get("status") != "completed":
                continue
            
            result = execution.get("result", {})
            metadata = execution.get("metadata", {})
            
            predicted_intent = result.get("intent", "unknown")
            expected_intent = metadata.get("expected_intent", "unknown")
            
            if expected_intent != "unknown":
                total_predictions += 1
                if predicted_intent == expected_intent:
                    correct_predictions += 1
        
        return correct_predictions / total_predictions if total_predictions > 0 else 0
    
    def _calculate_consistency(self, executions: List[Dict[str, Any]]) -> float:
        """è¨ˆç®—ä¸€è‡´æ€§"""
        # è¨ˆç®—ç›¸åŒæŸ¥è©¢çš„é æ¸¬ä¸€è‡´æ€§
        query_predictions = defaultdict(list)
        
        for execution in executions:
            if execution.get("status") != "completed":
                continue
            
            test_case_id = execution.get("test_case_id", "")
            result = execution.get("result", {})
            intent = result.get("intent", "unknown")
            
            query_predictions[test_case_id].append(intent)
        
        consistency_scores = []
        for predictions in query_predictions.values():
            if len(predictions) > 1:
                # è¨ˆç®—æœ€å¸¸è¦‹é æ¸¬çš„æ¯”ä¾‹
                most_common = Counter(predictions).most_common(1)[0][1]
                consistency = most_common / len(predictions)
                consistency_scores.append(consistency)
        
        return statistics.mean(consistency_scores) if consistency_scores else 1.0
    
    def _calculate_robustness(self, executions: List[Dict[str, Any]]) -> float:
        """è¨ˆç®—é­¯æ£’æ€§"""
        # åŸºæ–¼éŒ¯èª¤ç‡å’Œè¶…æ™‚ç‡è¨ˆç®—é­¯æ£’æ€§
        total_tests = len(executions)
        failed_tests = len([e for e in executions if e.get("status") == "failed"])
        timeout_tests = len([e for e in executions if e.get("status") == "timeout"])
        
        failure_rate = (failed_tests + timeout_tests) / total_tests if total_tests > 0 else 0
        robustness = 1.0 - failure_rate
        
        # è€ƒæ…®éŸ¿æ‡‰æ™‚é–“çš„ç©©å®šæ€§
        response_times = [e.get("duration", 0) for e in executions if e.get("duration") is not None]
        if len(response_times) > 1:
            cv = statistics.stdev(response_times) / statistics.mean(response_times)
            time_stability = max(0, 1 - cv)  # è®Šç•°ä¿‚æ•¸è¶Šå°ï¼Œç©©å®šæ€§è¶Šé«˜
            robustness = (robustness + time_stability) / 2
        
        return robustness
    
    def _calculate_category_performance(self, executions: List[Dict[str, Any]]) -> Dict[str, float]:
        """è¨ˆç®—é¡åˆ¥æ€§èƒ½"""
        category_stats = defaultdict(lambda: {"correct": 0, "total": 0})
        
        for execution in executions:
            if execution.get("status") != "completed":
                continue
            
            metadata = execution.get("metadata", {})
            category = metadata.get("test_case_category", "unknown")
            
            result = execution.get("result", {})
            predicted_intent = result.get("intent", "unknown")
            expected_intent = metadata.get("expected_intent", "unknown")
            
            category_stats[category]["total"] += 1
            if predicted_intent == expected_intent and expected_intent != "unknown":
                category_stats[category]["correct"] += 1
        
        category_performance = {}
        for category, stats in category_stats.items():
            if stats["total"] > 0:
                category_performance[category] = stats["correct"] / stats["total"]
        
        return category_performance
    
    def _calculate_complexity_performance(self, executions: List[Dict[str, Any]]) -> Dict[str, float]:
        """è¨ˆç®—è¤‡é›œåº¦æ€§èƒ½"""
        complexity_stats = defaultdict(lambda: {"correct": 0, "total": 0})
        
        for execution in executions:
            if execution.get("status") != "completed":
                continue
            
            metadata = execution.get("metadata", {})
            complexity = metadata.get("test_case_complexity", "unknown")
            
            result = execution.get("result", {})
            predicted_intent = result.get("intent", "unknown")
            expected_intent = metadata.get("expected_intent", "unknown")
            
            complexity_stats[complexity]["total"] += 1
            if predicted_intent == expected_intent and expected_intent != "unknown":
                complexity_stats[complexity]["correct"] += 1
        
        complexity_performance = {}
        for complexity, stats in complexity_stats.items():
            if stats["total"] > 0:
                complexity_performance[complexity] = stats["correct"] / stats["total"]
        
        return complexity_performance
    
    def _calculate_language_performance(self, executions: List[Dict[str, Any]]) -> Dict[str, float]:
        """è¨ˆç®—èªè¨€æ€§èƒ½"""
        language_stats = defaultdict(lambda: {"success": 0, "total": 0})
        
        for execution in executions:
            metadata = execution.get("metadata", {})
            # é€™è£¡éœ€è¦å¾æ¸¬è©¦æ¡ˆä¾‹ä¸­ç²å–èªè¨€ä¿¡æ¯
            # æš«æ™‚ä½¿ç”¨ç°¡å–®çš„èªè¨€æª¢æ¸¬
            language = "unknown"  # å¯¦éš›æ‡‰è©²å¾æ¸¬è©¦æ¡ˆä¾‹æ•¸æ“šä¸­ç²å–
            
            language_stats[language]["total"] += 1
            if execution.get("status") == "completed":
                language_stats[language]["success"] += 1
        
        language_performance = {}
        for language, stats in language_stats.items():
            if stats["total"] > 0:
                language_performance[language] = stats["success"] / stats["total"]
        
        return language_performance
    
    def _estimate_cost(self, provider: str) -> float:
        """ä¼°ç®—æˆæœ¬"""
        # ç°¡åŒ–çš„æˆæœ¬ä¼°ç®—
        cost_estimates = {
            "openai": 0.002,
            "anthropic": 0.003,
            "google": 0.001,
            "azure_openai": 0.002,
            "huggingface": 0.0001,
            "cohere": 0.0015,
            "ollama": 0.0
        }
        
        return cost_estimates.get(provider.lower(), 0.001)
    
    def _merge_performance_data(self, existing: ModelPerformance, new: ModelPerformance) -> None:
        """åˆä½µæ€§èƒ½æ•¸æ“š"""
        # åˆä½µåŸºæœ¬çµ±è¨ˆ
        total_tests = existing.total_tests + new.total_tests
        existing.successful_tests += new.successful_tests
        existing.failed_tests += new.failed_tests
        existing.timeout_tests += new.timeout_tests
        
        # é‡æ–°è¨ˆç®—å¹³å‡å€¼
        existing.accuracy_score = (existing.accuracy_score * existing.total_tests + 
                                 new.accuracy_score * new.total_tests) / total_tests
        
        existing.avg_response_time = (existing.avg_response_time * existing.total_tests + 
                                    new.avg_response_time * new.total_tests) / total_tests
        
        existing.total_tests = total_tests
        
        # åˆä½µåˆ—è¡¨æ•¸æ“š
        existing.confidence_scores.extend(new.confidence_scores)
        existing.response_times.extend(new.response_times)
        
        # é‡æ–°è¨ˆç®—ä¸­ä½æ•¸
        if existing.response_times:
            existing.median_response_time = statistics.median(existing.response_times)
    
    def perform_comparative_analysis(self) -> None:
        """åŸ·è¡Œæ¯”è¼ƒåˆ†æ"""
        print("ğŸ“Š åŸ·è¡Œæ¯”è¼ƒåˆ†æ...")
        
        if len(self.model_performances) < 2:
            print("   âš ï¸ éœ€è¦è‡³å°‘2å€‹æ¨¡å‹é€²è¡Œæ¯”è¼ƒ")
            return
        
        models = list(self.model_performances.keys())
        
        # å„ç¶­åº¦æ¯”è¼ƒ
        for dimension in ComparisonDimension:
            result = self._compare_dimension(dimension, models)
            self.comparison_results[dimension] = result
            print(f"   âœ… å®Œæˆ {dimension.value} æ¯”è¼ƒ")
        
        print(f"   ğŸ“ˆ å®Œæˆ {len(self.comparison_results)} å€‹ç¶­åº¦çš„æ¯”è¼ƒåˆ†æ")
    
    def _compare_dimension(self, dimension: ComparisonDimension, models: List[str]) -> ComparisonResult:
        """æ¯”è¼ƒç‰¹å®šç¶­åº¦"""
        scores = []
        
        for model in models:
            performance = self.model_performances[model]
            
            if dimension == ComparisonDimension.ACCURACY:
                score = performance.accuracy_score
            elif dimension == ComparisonDimension.SPEED:
                # é€Ÿåº¦åˆ†æ•¸ï¼šéŸ¿æ‡‰æ™‚é–“è¶ŠçŸ­åˆ†æ•¸è¶Šé«˜
                score = 1.0 / (1.0 + performance.avg_response_time)
            elif dimension == ComparisonDimension.CONSISTENCY:
                score = performance.consistency_score
            elif dimension == ComparisonDimension.ROBUSTNESS:
                score = performance.robustness_score
            elif dimension == ComparisonDimension.COST_EFFICIENCY:
                # æˆæœ¬æ•ˆç‡ï¼šæº–ç¢ºæ€§/æˆæœ¬
                score = performance.accuracy_score / (performance.cost_per_request + 0.0001)
            elif dimension == ComparisonDimension.ERROR_RATE:
                score = 1.0 - performance.error_rate  # éŒ¯èª¤ç‡è¶Šä½åˆ†æ•¸è¶Šé«˜
            else:
                score = 0.5  # é»˜èªåˆ†æ•¸
            
            scores.append(score)
        
        # è¨ˆç®—æ’å
        rankings = self._calculate_rankings(scores)
        
        # çµ±è¨ˆé¡¯è‘—æ€§æª¢é©—
        statistical_significance = self._test_statistical_significance(scores, models)
        
        # æ•ˆæ‡‰å¤§å°
        effect_size = self._calculate_effect_size(scores)
        
        # ç½®ä¿¡å€é–“
        confidence_interval = self._calculate_confidence_interval(scores)
        
        # på€¼
        p_value = self._calculate_p_value(scores)
        
        # è§£é‡‹
        interpretation = self._interpret_comparison(dimension, scores, models, p_value)
        
        return ComparisonResult(
            dimension=dimension,
            models=models,
            scores=scores,
            rankings=rankings,
            statistical_significance=statistical_significance,
            effect_size=effect_size,
            confidence_interval=confidence_interval,
            p_value=p_value,
            interpretation=interpretation
        )
    
    def _calculate_rankings(self, scores: List[float]) -> List[int]:
        """è¨ˆç®—æ’å"""
        # åˆ†æ•¸è¶Šé«˜æ’åè¶Šé å‰
        sorted_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)
        rankings = [0] * len(scores)
        
        for rank, index in enumerate(sorted_indices):
            rankings[index] = rank + 1
        
        return rankings
    
    def _test_statistical_significance(self, scores: List[float], models: List[str]) -> Dict[str, float]:
        """çµ±è¨ˆé¡¯è‘—æ€§æª¢é©—"""
        significance = {}
        
        if len(scores) < 2:
            return significance
        
        # é€²è¡Œæˆå°tæª¢é©—
        for i in range(len(models)):
            for j in range(i + 1, len(models)):
                model1, model2 = models[i], models[j]
                
                # ç²å–åŸå§‹æ•¸æ“šé€²è¡Œtæª¢é©—
                perf1 = self.model_performances[model1]
                perf2 = self.model_performances[model2]
                
                # ä½¿ç”¨éŸ¿æ‡‰æ™‚é–“ä½œç‚ºæ¨£æœ¬æ•¸æ“š
                data1 = perf1.response_times if perf1.response_times else [scores[i]]
                data2 = perf2.response_times if perf2.response_times else [scores[j]]
                
                if len(data1) > 1 and len(data2) > 1:
                    try:
                        t_stat, p_val = stats.ttest_ind(data1, data2)
                        significance[f"{model1}_vs_{model2}"] = p_val
                    except:
                        significance[f"{model1}_vs_{model2}"] = 1.0
                else:
                    significance[f"{model1}_vs_{model2}"] = 1.0
        
        return significance
    
    def _calculate_effect_size(self, scores: List[float]) -> float:
        """è¨ˆç®—æ•ˆæ‡‰å¤§å°"""
        if len(scores) < 2:
            return 0.0
        
        # ä½¿ç”¨Cohen's dè¨ˆç®—æ•ˆæ‡‰å¤§å°
        max_score = max(scores)
        min_score = min(scores)
        std_dev = statistics.stdev(scores) if len(scores) > 1 else 1.0
        
        return (max_score - min_score) / std_dev if std_dev > 0 else 0.0
    
    def _calculate_confidence_interval(self, scores: List[float]) -> Tuple[float, float]:
        """è¨ˆç®—ç½®ä¿¡å€é–“"""
        if len(scores) < 2:
            return (0.0, 1.0)
        
        mean_score = statistics.mean(scores)
        std_error = statistics.stdev(scores) / (len(scores) ** 0.5)
        
        # 95%ç½®ä¿¡å€é–“
        margin = 1.96 * std_error
        return (mean_score - margin, mean_score + margin)
    
    def _calculate_p_value(self, scores: List[float]) -> float:
        """è¨ˆç®—på€¼"""
        if len(scores) < 2:
            return 1.0
        
        # ä½¿ç”¨å–®æ¨£æœ¬tæª¢é©—æª¢é©—æ˜¯å¦é¡¯è‘—ä¸åŒæ–¼0.5ï¼ˆéš¨æ©Ÿæ°´å¹³ï¼‰
        try:
            t_stat, p_val = stats.ttest_1samp(scores, 0.5)
            return p_val
        except:
            return 1.0
    
    def _interpret_comparison(self, dimension: ComparisonDimension, scores: List[float], 
                            models: List[str], p_value: float) -> str:
        """è§£é‡‹æ¯”è¼ƒçµæœ"""
        best_model_idx = scores.index(max(scores))
        best_model = models[best_model_idx]
        best_score = scores[best_model_idx]
        
        worst_model_idx = scores.index(min(scores))
        worst_model = models[worst_model_idx]
        worst_score = scores[worst_model_idx]
        
        improvement = ((best_score - worst_score) / worst_score * 100) if worst_score > 0 else 0
        
        significance = "é¡¯è‘—" if p_value < 0.05 else "ä¸é¡¯è‘—"
        
        interpretation = f"åœ¨{dimension.value}ç¶­åº¦ä¸Šï¼Œ{best_model}è¡¨ç¾æœ€ä½³ï¼ˆåˆ†æ•¸ï¼š{best_score:.3f}ï¼‰ï¼Œ" \
                        f"æ¯”{worst_model}é«˜å‡º{improvement:.1f}%ã€‚å·®ç•°çµ±è¨ˆä¸Š{significance}ï¼ˆp={p_value:.3f}ï¼‰ã€‚"
        
        return interpretation
    
    def generate_insights(self) -> None:
        """ç”Ÿæˆåˆ†ææ´å¯Ÿ"""
        print("ğŸ’¡ ç”Ÿæˆåˆ†ææ´å¯Ÿ...")
        
        self.analysis_insights = []
        
        # æ•´é«”æ€§èƒ½æ´å¯Ÿ
        self._generate_overall_performance_insights()
        
        # ç¶­åº¦ç‰¹å®šæ´å¯Ÿ
        self._generate_dimension_specific_insights()
        
        # æˆæœ¬æ•ˆç›Šæ´å¯Ÿ
        self._generate_cost_benefit_insights()
        
        # ä½¿ç”¨å ´æ™¯å»ºè­°
        self._generate_use_case_recommendations()
        
        print(f"   âœ… ç”Ÿæˆäº† {len(self.analysis_insights)} å€‹æ´å¯Ÿ")
    
    def _generate_overall_performance_insights(self) -> None:
        """ç”Ÿæˆæ•´é«”æ€§èƒ½æ´å¯Ÿ"""
        if not self.model_performances:
            return
        
        # æ‰¾å‡ºæ•´é«”æœ€ä½³æ¨¡å‹
        overall_scores = {}
        for model, perf in self.model_performances.items():
            # ç¶œåˆåˆ†æ•¸ï¼šæº–ç¢ºæ€§ã€é€Ÿåº¦ã€é­¯æ£’æ€§çš„åŠ æ¬Šå¹³å‡
            overall_score = (perf.accuracy_score * 0.4 + 
                           (1.0 / (1.0 + perf.avg_response_time)) * 0.3 + 
                           perf.robustness_score * 0.3)
            overall_scores[model] = overall_score
        
        best_model = max(overall_scores.keys(), key=lambda k: overall_scores[k])
        best_score = overall_scores[best_model]
        
        insight = AnalysisInsight(
            insight_type="overall_performance",
            title="æ•´é«”æ€§èƒ½æœ€ä½³æ¨¡å‹",
            description=f"{best_model}åœ¨ç¶œåˆè©•ä¼°ä¸­è¡¨ç¾æœ€ä½³ï¼Œç¶œåˆåˆ†æ•¸ç‚º{best_score:.3f}",
            evidence={
                "overall_scores": overall_scores,
                "best_model": best_model,
                "performance_details": {
                    "accuracy": self.model_performances[best_model].accuracy_score,
                    "avg_response_time": self.model_performances[best_model].avg_response_time,
                    "robustness": self.model_performances[best_model].robustness_score
                }
            },
            confidence=0.85,
            recommendations=[
                f"å°æ–¼ä¸€èˆ¬ç”¨é€”ï¼Œæ¨è–¦ä½¿ç”¨{best_model}",
                "è€ƒæ…®åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­éƒ¨ç½²æ­¤æ¨¡å‹",
                "å®šæœŸç›£æ§æ€§èƒ½ä»¥ç¢ºä¿æŒçºŒå„ªç•°è¡¨ç¾"
            ],
            impact_level="high"
        )
        
        self.analysis_insights.append(insight)
    
    def _generate_dimension_specific_insights(self) -> None:
        """ç”Ÿæˆç¶­åº¦ç‰¹å®šæ´å¯Ÿ"""
        for dimension, result in self.comparison_results.items():
            best_model_idx = result.rankings.index(1)
            best_model = result.models[best_model_idx]
            best_score = result.scores[best_model_idx]
            
            # åˆ¤æ–·æ˜¯å¦æœ‰é¡¯è‘—å·®ç•°
            has_significant_difference = result.p_value < 0.05
            
            if has_significant_difference:
                insight = AnalysisInsight(
                    insight_type="dimension_specific",
                    title=f"{dimension.value}ç¶­åº¦åˆ†æ",
                    description=f"{best_model}åœ¨{dimension.value}æ–¹é¢é¡¯è‘—å„ªæ–¼å…¶ä»–æ¨¡å‹",
                    evidence={
                        "dimension": dimension.value,
                        "best_model": best_model,
                        "best_score": best_score,
                        "p_value": result.p_value,
                        "effect_size": result.effect_size,
                        "all_scores": dict(zip(result.models, result.scores))
                    },
                    confidence=0.9 if result.p_value < 0.01 else 0.75,
                    recommendations=[
                        f"å°æ–¼{dimension.value}è¦æ±‚è¼ƒé«˜çš„å ´æ™¯ï¼Œå„ªå…ˆè€ƒæ…®{best_model}",
                        "ç›£æ§æ­¤ç¶­åº¦çš„æ€§èƒ½è®ŠåŒ–",
                        "è€ƒæ…®é‡å°æ­¤ç¶­åº¦é€²è¡Œæ¨¡å‹å„ªåŒ–"
                    ],
                    impact_level="medium"
                )
                
                self.analysis_insights.append(insight)
    
    def _generate_cost_benefit_insights(self) -> None:
        """ç”Ÿæˆæˆæœ¬æ•ˆç›Šæ´å¯Ÿ"""
        cost_efficiency_scores = {}
        
        for model, perf in self.model_performances.items():
            # æˆæœ¬æ•ˆç›Š = æ€§èƒ½ / æˆæœ¬
            cost_efficiency = perf.accuracy_score / (perf.cost_per_request + 0.0001)
            cost_efficiency_scores[model] = cost_efficiency
        
        best_cost_model = max(cost_efficiency_scores.keys(), key=lambda k: cost_efficiency_scores[k])
        
        insight = AnalysisInsight(
            insight_type="cost_benefit",
            title="æˆæœ¬æ•ˆç›Šåˆ†æ",
            description=f"{best_cost_model}æä¾›æœ€ä½³çš„æˆæœ¬æ•ˆç›Šæ¯”",
            evidence={
                "cost_efficiency_scores": cost_efficiency_scores,
                "best_cost_model": best_cost_model,
                "cost_details": {
                    model: {
                        "accuracy": perf.accuracy_score,
                        "cost_per_request": perf.cost_per_request,
                        "cost_efficiency": cost_efficiency_scores[model]
                    }
                    for model, perf in self.model_performances.items()
                }
            },
            confidence=0.8,
            recommendations=[
                f"å°æ–¼æˆæœ¬æ•æ„Ÿçš„æ‡‰ç”¨ï¼Œæ¨è–¦{best_cost_model}",
                "å®šæœŸè©•ä¼°æˆæœ¬æ•ˆç›Šæ¯”çš„è®ŠåŒ–",
                "è€ƒæ…®æ ¹æ“šä½¿ç”¨é‡èª¿æ•´æ¨¡å‹é¸æ“‡ç­–ç•¥"
            ],
            impact_level="high"
        )
        
        self.analysis_insights.append(insight)
    
    def _generate_use_case_recommendations(self) -> None:
        """ç”Ÿæˆä½¿ç”¨å ´æ™¯å»ºè­°"""
        recommendations = {
            "é«˜æº–ç¢ºæ€§è¦æ±‚": None,
            "ä½å»¶é²è¦æ±‚": None,
            "é«˜å¯é æ€§è¦æ±‚": None,
            "æˆæœ¬æ•æ„Ÿ": None
        }
        
        # æ‰¾å‡ºå„å ´æ™¯çš„æœ€ä½³æ¨¡å‹
        for model, perf in self.model_performances.items():
            # é«˜æº–ç¢ºæ€§
            if recommendations["é«˜æº–ç¢ºæ€§è¦æ±‚"] is None or perf.accuracy_score > self.model_performances[recommendations["é«˜æº–ç¢ºæ€§è¦æ±‚"]].accuracy_score:
                recommendations["é«˜æº–ç¢ºæ€§è¦æ±‚"] = model
            
            # ä½å»¶é²
            if recommendations["ä½å»¶é²è¦æ±‚"] is None or perf.avg_response_time < self.model_performances[recommendations["ä½å»¶é²è¦æ±‚"]].avg_response_time:
                recommendations["ä½å»¶é²è¦æ±‚"] = model
            
            # é«˜å¯é æ€§
            if recommendations["é«˜å¯é æ€§è¦æ±‚"] is None or perf.robustness_score > self.model_performances[recommendations["é«˜å¯é æ€§è¦æ±‚"]].robustness_score:
                recommendations["é«˜å¯é æ€§è¦æ±‚"] = model
            
            # æˆæœ¬æ•æ„Ÿ
            if recommendations["æˆæœ¬æ•æ„Ÿ"] is None or perf.cost_per_request < self.model_performances[recommendations["æˆæœ¬æ•æ„Ÿ"]].cost_per_request:
                recommendations["æˆæœ¬æ•æ„Ÿ"] = model
        
        insight = AnalysisInsight(
            insight_type="use_case_recommendations",
            title="ä½¿ç”¨å ´æ™¯å»ºè­°",
            description="åŸºæ–¼ä¸åŒéœ€æ±‚çš„æ¨¡å‹é¸æ“‡å»ºè­°",
            evidence={
                "scenario_recommendations": recommendations,
                "detailed_analysis": {
                    scenario: {
                        "recommended_model": model,
                        "performance_details": {
                            "accuracy": self.model_performances[model].accuracy_score,
                            "response_time": self.model_performances[model].avg_response_time,
                            "robustness": self.model_performances[model].robustness_score,
                            "cost": self.model_performances[model].cost_per_request
                        }
                    }
                    for scenario, model in recommendations.items() if model
                }
            },
            confidence=0.85,
            recommendations=[
                f"é«˜æº–ç¢ºæ€§å ´æ™¯ï¼šä½¿ç”¨{recommendations['é«˜æº–ç¢ºæ€§è¦æ±‚']}",
                f"ä½å»¶é²å ´æ™¯ï¼šä½¿ç”¨{recommendations['ä½å»¶é²è¦æ±‚']}",
                f"é«˜å¯é æ€§å ´æ™¯ï¼šä½¿ç”¨{recommendations['é«˜å¯é æ€§è¦æ±‚']}",
                f"æˆæœ¬æ•æ„Ÿå ´æ™¯ï¼šä½¿ç”¨{recommendations['æˆæœ¬æ•æ„Ÿ']}"
            ],
            impact_level="high"
        )
        
        self.analysis_insights.append(insight)
    
    def create_visualizations(self, output_dir: str = "analysis_charts") -> None:
        """å‰µå»ºå¯è¦–åŒ–åœ–è¡¨"""
        print(f"ğŸ“Š å‰µå»ºå¯è¦–åŒ–åœ–è¡¨...")
        
        Path(output_dir).mkdir(exist_ok=True)
        
        # 1. æ¨¡å‹æ€§èƒ½é›·é”åœ–
        self._create_performance_radar_chart(output_dir)
        
        # 2. éŸ¿æ‡‰æ™‚é–“åˆ†ä½ˆåœ–
        self._create_response_time_distribution(output_dir)
        
        # 3. æº–ç¢ºæ€§vsæˆæœ¬æ•£é»åœ–
        self._create_accuracy_cost_scatter(output_dir)
        
        # 4. ç¶­åº¦æ¯”è¼ƒç†±åŠ›åœ–
        self._create_dimension_heatmap(output_dir)
        
        # 5. éŒ¯èª¤ç‡æ¯”è¼ƒæŸ±ç‹€åœ–
        self._create_error_rate_comparison(output_dir)
        
        # 6. æ€§èƒ½è¶¨å‹¢åœ–
        self._create_performance_trends(output_dir)
        
        print(f"   âœ… åœ–è¡¨å·²ä¿å­˜åˆ° {output_dir} ç›®éŒ„")
    
    def _create_performance_radar_chart(self, output_dir: str) -> None:
        """å‰µå»ºæ€§èƒ½é›·é”åœ–"""
        if not self.model_performances:
            return
        
        # æº–å‚™æ•¸æ“š
        models = list(self.model_performances.keys())
        dimensions = ['æº–ç¢ºæ€§', 'é€Ÿåº¦', 'ä¸€è‡´æ€§', 'é­¯æ£’æ€§', 'æˆæœ¬æ•ˆç›Š']
        
        fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))
        
        angles = np.linspace(0, 2 * np.pi, len(dimensions), endpoint=False).tolist()
        angles += angles[:1]  # é–‰åˆåœ–å½¢
        
        colors = plt.cm.Set3(np.linspace(0, 1, len(models)))
        
        for i, model in enumerate(models):
            perf = self.model_performances[model]
            
            values = [
                perf.accuracy_score,
                1.0 / (1.0 + perf.avg_response_time),  # é€Ÿåº¦åˆ†æ•¸
                perf.consistency_score,
                perf.robustness_score,
                perf.accuracy_score / (perf.cost_per_request + 0.0001)  # æˆæœ¬æ•ˆç›Š
            ]
            
            # æ¨™æº–åŒ–åˆ°0-1ç¯„åœ
            max_values = [1.0, 1.0, 1.0, 1.0, max([p.accuracy_score / (p.cost_per_request + 0.0001) for p in self.model_performances.values()])]
            normalized_values = [v / max_v for v, max_v in zip(values, max_values)]
            normalized_values += normalized_values[:1]  # é–‰åˆåœ–å½¢
            
            ax.plot(angles, normalized_values, 'o-', linewidth=2, label=model, color=colors[i])
            ax.fill(angles, normalized_values, alpha=0.25, color=colors[i])
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(dimensions)
        ax.set_ylim(0, 1)
        ax.set_title('æ¨¡å‹æ€§èƒ½é›·é”åœ–', size=16, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax.grid(True)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/performance_radar.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_response_time_distribution(self, output_dir: str) -> None:
        """å‰µå»ºéŸ¿æ‡‰æ™‚é–“åˆ†ä½ˆåœ–"""
        fig, ax = plt.subplots(figsize=(12, 6))
        
        for model, perf in self.model_performances.items():
            if perf.response_times:
                ax.hist(perf.response_times, bins=30, alpha=0.7, label=model, density=True)
        
        ax.set_xlabel('éŸ¿æ‡‰æ™‚é–“ (ç§’)')
        ax.set_ylabel('å¯†åº¦')
        ax.set_title('éŸ¿æ‡‰æ™‚é–“åˆ†ä½ˆåœ–')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/response_time_distribution.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_accuracy_cost_scatter(self, output_dir: str) -> None:
        """å‰µå»ºæº–ç¢ºæ€§vsæˆæœ¬æ•£é»åœ–"""
        fig, ax = plt.subplots(figsize=(10, 8))
        
        models = []
        accuracies = []
        costs = []
        
        for model, perf in self.model_performances.items():
            models.append(model)
            accuracies.append(perf.accuracy_score)
            costs.append(perf.cost_per_request)
        
        scatter = ax.scatter(costs, accuracies, s=100, alpha=0.7, c=range(len(models)), cmap='viridis')
        
        # æ·»åŠ æ¨¡å‹æ¨™ç±¤
        for i, model in enumerate(models):
            ax.annotate(model, (costs[i], accuracies[i]), xytext=(5, 5), 
                       textcoords='offset points', fontsize=10)
        
        ax.set_xlabel('æ¯æ¬¡è«‹æ±‚æˆæœ¬ ($)')
        ax.set_ylabel('æº–ç¢ºæ€§åˆ†æ•¸')
        ax.set_title('æº–ç¢ºæ€§ vs æˆæœ¬åˆ†æ')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/accuracy_cost_scatter.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_dimension_heatmap(self, output_dir: str) -> None:
        """å‰µå»ºç¶­åº¦æ¯”è¼ƒç†±åŠ›åœ–"""
        if not self.comparison_results:
            return
        
        # æº–å‚™æ•¸æ“š
        models = list(self.model_performances.keys())
        dimensions = [d.value for d in self.comparison_results.keys()]
        
        data = []
        for dimension in self.comparison_results.keys():
            result = self.comparison_results[dimension]
            data.append(result.scores)
        
        # å‰µå»ºç†±åŠ›åœ–
        fig, ax = plt.subplots(figsize=(12, 8))
        
        im = ax.imshow(data, cmap='RdYlGn', aspect='auto')
        
        # è¨­ç½®æ¨™ç±¤
        ax.set_xticks(range(len(models)))
        ax.set_yticks(range(len(dimensions)))
        ax.set_xticklabels(models, rotation=45, ha='right')
        ax.set_yticklabels(dimensions)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for i in range(len(dimensions)):
            for j in range(len(models)):
                text = ax.text(j, i, f'{data[i][j]:.3f}', ha="center", va="center", color="black")
        
        ax.set_title('æ¨¡å‹å„ç¶­åº¦æ€§èƒ½ç†±åŠ›åœ–')
        
        # æ·»åŠ é¡è‰²æ¢
        cbar = plt.colorbar(im)
        cbar.set_label('æ€§èƒ½åˆ†æ•¸')
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/dimension_heatmap.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_error_rate_comparison(self, output_dir: str) -> None:
        """å‰µå»ºéŒ¯èª¤ç‡æ¯”è¼ƒæŸ±ç‹€åœ–"""
        models = list(self.model_performances.keys())
        error_rates = [perf.error_rate for perf in self.model_performances.values()]
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        bars = ax.bar(models, error_rates, color='lightcoral', alpha=0.7)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar, rate in zip(bars, error_rates):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                   f'{rate:.3f}', ha='center', va='bottom')
        
        ax.set_ylabel('éŒ¯èª¤ç‡')
        ax.set_title('æ¨¡å‹éŒ¯èª¤ç‡æ¯”è¼ƒ')
        ax.set_ylim(0, max(error_rates) * 1.2 if error_rates else 1)
        
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(f"{output_dir}/error_rate_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_performance_trends(self, output_dir: str) -> None:
        """å‰µå»ºæ€§èƒ½è¶¨å‹¢åœ–"""
        # é€™è£¡éœ€è¦æ™‚é–“åºåˆ—æ•¸æ“šï¼Œæš«æ™‚å‰µå»ºç¤ºä¾‹åœ–
        fig, ax = plt.subplots(figsize=(12, 6))
        
        # æ¨¡æ“¬è¶¨å‹¢æ•¸æ“š
        time_points = range(1, 11)
        
        for model, perf in self.model_performances.items():
            # æ¨¡æ“¬æ€§èƒ½è¶¨å‹¢ï¼ˆå¯¦éš›æ‡‰è©²å¾æ­·å²æ•¸æ“šä¸­ç²å–ï¼‰
            trend = [perf.accuracy_score + np.random.normal(0, 0.05) for _ in time_points]
            ax.plot(time_points, trend, marker='o', label=model, linewidth=2)
        
        ax.set_xlabel('æ™‚é–“é»')
        ax.set_ylabel('æº–ç¢ºæ€§åˆ†æ•¸')
        ax.set_title('æ¨¡å‹æ€§èƒ½è¶¨å‹¢')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/performance_trends.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def generate_comprehensive_report(self, output_file: str = None) -> str:
        """ç”Ÿæˆç¶œåˆå ±å‘Š"""
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"model_comparison_report_{timestamp}.json"
        
        print(f"ğŸ“„ ç”Ÿæˆç¶œåˆå ±å‘Š...")
        
        report = {
            "report_info": {
                "generation_time": datetime.now().isoformat(),
                "models_analyzed": list(self.model_performances.keys()),
                "total_models": len(self.model_performances),
                "analysis_dimensions": len(self.comparison_results)
            },
            "model_performances": {
                model: {
                    "total_tests": perf.total_tests,
                    "successful_tests": perf.successful_tests,
                    "accuracy_score": perf.accuracy_score,
                    "avg_response_time": perf.avg_response_time,
                    "consistency_score": perf.consistency_score,
                    "robustness_score": perf.robustness_score,
                    "error_rate": perf.error_rate,
                    "cost_per_request": perf.cost_per_request,
                    "category_performance": perf.category_performance,
                    "complexity_performance": perf.complexity_performance
                }
                for model, perf in self.model_performances.items()
            },
            "comparison_results": {
                dimension.value: {
                    "models": result.models,
                    "scores": result.scores,
                    "rankings": result.rankings,
                    "p_value": result.p_value,
                    "effect_size": result.effect_size,
                    "interpretation": result.interpretation
                }
                for dimension, result in self.comparison_results.items()
            },
            "analysis_insights": [
                {
                    "type": insight.insight_type,
                    "title": insight.title,
                    "description": insight.description,
                    "confidence": insight.confidence,
                    "recommendations": insight.recommendations,
                    "impact_level": insight.impact_level,
                    "evidence": insight.evidence
                }
                for insight in self.analysis_insights
            ],
            "summary": self._generate_executive_summary()
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"   âœ… å ±å‘Šå·²ä¿å­˜åˆ° {output_file}")
        return output_file
    
    def _generate_executive_summary(self) -> Dict[str, Any]:
        """ç”ŸæˆåŸ·è¡Œæ‘˜è¦"""
        if not self.model_performances:
            return {}
        
        # æ‰¾å‡ºå„ç¶­åº¦çš„æœ€ä½³æ¨¡å‹
        best_accuracy = max(self.model_performances.keys(), 
                          key=lambda k: self.model_performances[k].accuracy_score)
        best_speed = min(self.model_performances.keys(), 
                        key=lambda k: self.model_performances[k].avg_response_time)
        best_robustness = max(self.model_performances.keys(), 
                            key=lambda k: self.model_performances[k].robustness_score)
        
        # è¨ˆç®—æ•´é«”çµ±è¨ˆ
        total_tests = sum(perf.total_tests for perf in self.model_performances.values())
        avg_accuracy = statistics.mean([perf.accuracy_score for perf in self.model_performances.values()])
        avg_response_time = statistics.mean([perf.avg_response_time for perf in self.model_performances.values()])
        
        return {
            "total_tests_conducted": total_tests,
            "models_compared": len(self.model_performances),
            "average_accuracy": avg_accuracy,
            "average_response_time": avg_response_time,
            "best_performers": {
                "accuracy": {
                    "model": best_accuracy,
                    "score": self.model_performances[best_accuracy].accuracy_score
                },
                "speed": {
                    "model": best_speed,
                    "response_time": self.model_performances[best_speed].avg_response_time
                },
                "robustness": {
                    "model": best_robustness,
                    "score": self.model_performances[best_robustness].robustness_score
                }
            },
            "key_findings": [
                f"å…±åˆ†æäº†{len(self.model_performances)}å€‹æ¨¡å‹ï¼Œé€²è¡Œäº†{total_tests}æ¬¡æ¸¬è©¦",
                f"{best_accuracy}åœ¨æº–ç¢ºæ€§æ–¹é¢è¡¨ç¾æœ€ä½³",
                f"{best_speed}åœ¨éŸ¿æ‡‰é€Ÿåº¦æ–¹é¢è¡¨ç¾æœ€ä½³",
                f"{best_robustness}åœ¨é­¯æ£’æ€§æ–¹é¢è¡¨ç¾æœ€ä½³",
                f"å¹³å‡æº–ç¢ºç‡ç‚º{avg_accuracy:.1%}ï¼Œå¹³å‡éŸ¿æ‡‰æ™‚é–“ç‚º{avg_response_time:.3f}ç§’"
            ],
            "insights_generated": len(self.analysis_insights),
            "high_impact_insights": len([i for i in self.analysis_insights if i.impact_level == "high"])
        }
    
    def print_analysis_summary(self) -> None:
        """æ‰“å°åˆ†ææ‘˜è¦"""
        print("\n" + "=" * 80)
        print("ğŸ” LLMæ¨¡å‹æ¯”è¼ƒåˆ†ææ‘˜è¦")
        print("=" * 80)
        
        if not self.model_performances:
            print("âŒ æ²’æœ‰æ¨¡å‹æ€§èƒ½æ•¸æ“š")
            return
        
        # åŸºæœ¬çµ±è¨ˆ
        print(f"\nğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
        print(f"   åˆ†ææ¨¡å‹æ•¸é‡: {len(self.model_performances)}")
        print(f"   æ¯”è¼ƒç¶­åº¦æ•¸é‡: {len(self.comparison_results)}")
        print(f"   ç”Ÿæˆæ´å¯Ÿæ•¸é‡: {len(self.analysis_insights)}")
        
        # æ¨¡å‹æ€§èƒ½æ¦‚è¦½
        print(f"\nğŸ† æ¨¡å‹æ€§èƒ½æ¦‚è¦½:")
        for model, perf in self.model_performances.items():
            print(f"   {model}:")
            print(f"     æº–ç¢ºç‡: {perf.accuracy_score:.1%}")
            print(f"     å¹³å‡éŸ¿æ‡‰æ™‚é–“: {perf.avg_response_time:.3f}ç§’")
            print(f"     éŒ¯èª¤ç‡: {perf.error_rate:.1%}")
            print(f"     é­¯æ£’æ€§: {perf.robustness_score:.3f}")
        
        # ç¶­åº¦æ¯”è¼ƒçµæœ
        if self.comparison_results:
            print(f"\nğŸ“ˆ ç¶­åº¦æ¯”è¼ƒçµæœ:")
            for dimension, result in self.comparison_results.items():
                best_idx = result.rankings.index(1)
                best_model = result.models[best_idx]
                print(f"   {dimension.value}: {best_model} (åˆ†æ•¸: {result.scores[best_idx]:.3f})")
        
        # é—œéµæ´å¯Ÿ
        high_impact_insights = [i for i in self.analysis_insights if i.impact_level == "high"]
        if high_impact_insights:
            print(f"\nğŸ’¡ é—œéµæ´å¯Ÿ:")
            for insight in high_impact_insights[:3]:  # é¡¯ç¤ºå‰3å€‹
                print(f"   â€¢ {insight.title}: {insight.description}")
        
        print("\n" + "=" * 80)


def main():
    """ä¸»å‡½æ•¸ - æ¨¡å‹æ¯”è¼ƒåˆ†æå™¨å…¥å£é»"""
    print("ğŸ” LLMæ¨¡å‹æ¯”è¼ƒåˆ†æå™¨")
    print("=" * 60)
    
    try:
        # ç²å–æ¸¬è©¦çµæœæ–‡ä»¶
        print("\nğŸ“‚ è«‹æä¾›æ¸¬è©¦çµæœæ–‡ä»¶è·¯å¾‘:")
        files_input = input("è¼¸å…¥æ–‡ä»¶è·¯å¾‘ (ç”¨é€—è™Ÿåˆ†éš”å¤šå€‹æ–‡ä»¶): ").strip()
        
        if not files_input:
            print("âŒ æœªæä¾›æ–‡ä»¶è·¯å¾‘")
            return
        
        result_files = [f.strip() for f in files_input.split(",")]
        
        # å‰µå»ºåˆ†æå™¨
        analyzer = LLMModelComparisonAnalyzer()
        
        # è¼‰å…¥æ•¸æ“š
        analyzer.load_test_results(result_files)
        
        # æå–æ€§èƒ½æ•¸æ“š
        analyzer.extract_model_performances()
        
        # åŸ·è¡Œæ¯”è¼ƒåˆ†æ
        analyzer.perform_comparative_analysis()
        
        # ç”Ÿæˆæ´å¯Ÿ
        analyzer.generate_insights()
        
        # é¡¯ç¤ºæ‘˜è¦
        analyzer.print_analysis_summary()
        
        # å‰µå»ºå¯è¦–åŒ–
        create_viz = input("\næ˜¯å¦å‰µå»ºå¯è¦–åŒ–åœ–è¡¨ï¼Ÿ (y/n): ").strip().lower()
        if create_viz == 'y':
            analyzer.create_visualizations()
        
        # ç”Ÿæˆå ±å‘Š
        generate_report = input("æ˜¯å¦ç”Ÿæˆç¶œåˆå ±å‘Šï¼Ÿ (y/n): ").strip().lower()
        if generate_report == 'y':
            report_file = analyzer.generate_comprehensive_report()
            print(f"   ğŸ“„ å ±å‘Šæ–‡ä»¶: {report_file}")
        
        print("\nğŸ‰ åˆ†æå®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâŒ åˆ†æéç¨‹è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()