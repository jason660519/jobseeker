# AI Vision-Enhanced Scraping Implementation

## ğŸ¤– **AI è¦–è¦ºè¾¨è­˜çˆ¬èŸ²å¼•æ“æ¶æ§‹**

### **æ ¸å¿ƒæ¦‚å¿µï¼šè¦–è¦ºç†è§£ + æ™ºèƒ½çˆ¬èŸ²**

å‚³çµ±çˆ¬èŸ²ä¾è³´ DOM çµæ§‹å’Œ CSS é¸æ“‡å™¨ï¼Œå®¹æ˜“è¢«åçˆ¬èŸ²æ©Ÿåˆ¶ç ´è§£ã€‚AI è¦–è¦ºè¾¨è­˜çˆ¬èŸ²æ¨¡æ“¬äººçœ¼è­˜åˆ¥ï¼Œå¤§å¹…æå‡æˆåŠŸç‡å’Œé©æ‡‰æ€§ã€‚

---

## ğŸ”§ **æŠ€è¡“å¯¦ç¾æ¶æ§‹**

### **1. AI Vision Service æ ¸å¿ƒæ¨¡çµ„**

```python
# ai_vision/core.py
from dataclasses import dataclass
from enum import Enum
from typing import List, Dict, Optional, Union
import asyncio
import time

class ProcessingStrategy(Enum):
    API_ONLY = "api_only"      # ç´”é›²ç«¯ API
    LOCAL_ONLY = "local_only"  # ç´”æœ¬åœ°æ¨¡å‹  
    HYBRID = "hybrid"          # æ··åˆæ¨¡å¼

@dataclass
class JobListing:
    title: str
    company: str
    location: str
    salary: Optional[str] = None
    apply_button_location: Optional[dict] = None
    confidence: float = 0.0

@dataclass
class VisionAnalysisResult:
    processing_time_ms: int
    confidence_score: float
    strategy_used: ProcessingStrategy
    job_listings: List[JobListing]
    page_type: str = "unknown"
    errors: List[str] = None

class AIVisionService:
    def __init__(self, config):
        self.config = config
        self.openai_client = None
        self.local_model = None
        self._initialize_clients()
    
    async def analyze_job_page(
        self,
        image_input: Union[str, bytes],
        strategy: Optional[ProcessingStrategy] = None
    ) -> VisionAnalysisResult:
        """åˆ†ææ±‚è·ç¶²ç«™é é¢æˆªåœ–"""
        start_time = time.time()
        
        try:
            if strategy == ProcessingStrategy.HYBRID:
                result = await self._process_hybrid(image_input)
            elif strategy == ProcessingStrategy.API_ONLY:
                result = await self._process_with_apis(image_input)
            else:
                result = await self._process_with_local_model(image_input)
                
            result.processing_time_ms = int((time.time() - start_time) * 1000)
            return result
            
        except Exception as e:
            return VisionAnalysisResult(
                processing_time_ms=int((time.time() - start_time) * 1000),
                confidence_score=0.0,
                strategy_used=strategy,
                job_listings=[],
                errors=[str(e)]
            )
```

### **2. OpenAI GPT-4 Vision å®¢æˆ¶ç«¯**

```python
# ai_vision/clients/openai_client.py
import openai
import json
import base64
from PIL import Image
import io

class OpenAIVisionClient:
    def __init__(self, api_key: str):
        self.client = openai.AsyncOpenAI(api_key=api_key)
        
        self.job_analysis_prompt = """
        åˆ†æé€™å¼µæ±‚è·ç¶²ç«™æˆªåœ–ï¼Œæå–è·ä½ä¿¡æ¯ï¼š
        1. è·ä½æ¨™é¡Œã€å…¬å¸åç¨±ã€å·¥ä½œåœ°é»ã€è–ªè³‡
        2. ç”³è«‹æŒ‰éˆ•ä½ç½®åº§æ¨™
        3. é é¢é¡å‹åˆ¤æ–·
        
        è¿”å›JSONæ ¼å¼ï¼š
        {
            "job_listings": [{
                "title": "è·ä½æ¨™é¡Œ",
                "company": "å…¬å¸åç¨±",
                "location": "åœ°é»",
                "salary": "è–ªè³‡æˆ–null",
                "apply_button": {"x": 100, "y": 200, "confidence": 0.9}
            }],
            "page_type": "job_list|job_detail",
            "confidence": 0.9
        }
        """
    
    async def analyze_job_page(self, image: Image.Image) -> Dict:
        image_base64 = await self._image_to_base64(image)
        
        response = await self.client.chat.completions.create(
            model="gpt-4-vision-preview",
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": self.job_analysis_prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}
                ]
            }],
            max_tokens=2000,
            temperature=0.1
        )
        
        content = response.choices[0].message.content
        return json.loads(content)
    
    async def _image_to_base64(self, image: Image.Image) -> str:
        buffer = io.BytesIO()
        image.save(buffer, format='JPEG', quality=85)
        return base64.b64encode(buffer.getvalue()).decode('utf-8')
```

### **3. æœ¬åœ° AI æ¨¡å‹å¯¦ç¾**

```python
# ai_vision/models/local_models.py
import torch
from transformers import CLIPProcessor, CLIPModel
import easyocr
import cv2
import numpy as np
from PIL import Image

class LocalVisionModel:
    def __init__(self, device="cpu"):
        self.device = device
        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        self.ocr_reader = easyocr.Reader(['en', 'zh_sim', 'zh_tra'])
        
        if device == "cuda" and torch.cuda.is_available():
            self.clip_model = self.clip_model.cuda()
    
    async def analyze_image(self, image: Image.Image) -> Dict:
        # 1. OCR æ–‡å­—è­˜åˆ¥
        text_results = await self._extract_text_with_positions(image)
        
        # 2. CLIP èªç¾©åˆ†æ
        semantic_results = await self._semantic_analysis(image)
        
        # 3. æå–è·ä½ä¿¡æ¯
        job_listings = await self._extract_job_listings(text_results, semantic_results)
        
        return {
            "job_listings": job_listings,
            "confidence": self._calculate_confidence(text_results, job_listings)
        }
    
    async def _extract_text_with_positions(self, image: Image.Image) -> List[Dict]:
        opencv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        results = self.ocr_reader.readtext(opencv_image)
        
        text_data = []
        for (bbox, text, confidence) in results:
            if confidence > 0.5:
                x1, y1 = bbox[0]
                x2, y2 = bbox[2]
                text_data.append({
                    "text": text,
                    "bbox": {"x": int(x1), "y": int(y1), "width": int(x2-x1), "height": int(y2-y1)},
                    "confidence": confidence
                })
        return text_data
```

---

## ğŸš€ **æ™ºèƒ½çˆ¬èŸ²æ•´åˆ**

### **è¦–è¦ºå°å‘çˆ¬èŸ²å¼•æ“**

```python
# scraping/vision_scraper.py
from playwright.async_api import async_playwright
import asyncio
from ai_vision.core import AIVisionService, ProcessingStrategy

class VisionEnhancedScraper:
    def __init__(self, vision_service: AIVisionService):
        self.vision_service = vision_service
        self.playwright = None
        self.browser = None
    
    async def scrape_job_site(self, url: str, search_params: Dict) -> List[Dict]:
        """ä½¿ç”¨ AI è¦–è¦ºè¼”åŠ©çš„æ™ºèƒ½çˆ¬èŸ²"""
        
        async with async_playwright() as p:
            # å•Ÿå‹•ç€è¦½å™¨ (ååµæ¸¬é…ç½®)
            browser = await p.chromium.launch(
                headless=False,  # é–‹ç™¼éšæ®µå¯è¦–åŒ–
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                    '--no-sandbox'
                ]
            )
            
            page = await browser.new_page()
            
            # è¨­å®šäººé¡è¡Œç‚ºæ¨¡æ“¬
            await page.set_extra_http_headers({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            
            try:
                # 1. å°èˆªåˆ°ç›®æ¨™é é¢
                await page.goto(url, wait_until='networkidle')
                await self._random_delay(1, 3)
                
                # 2. æˆªåœ–åˆ†æ
                screenshot = await page.screenshot(full_page=True)
                vision_result = await self.vision_service.analyze_job_page(
                    screenshot, 
                    strategy=ProcessingStrategy.HYBRID
                )
                
                # 3. åŸºæ–¼ AI åˆ†æçµæœé€²è¡Œæ™ºèƒ½æ“ä½œ
                jobs_data = []
                
                for job in vision_result.job_listings:
                    if job.apply_button_location:
                        # é»æ“Šè·ä½è©³æƒ…
                        await self._smart_click(page, job.apply_button_location)
                        await self._random_delay(2, 4)
                        
                        # æå–è©³ç´°ä¿¡æ¯
                        detail_screenshot = await page.screenshot()
                        detail_result = await self.vision_service.analyze_job_page(detail_screenshot)
                        
                        jobs_data.append({
                            "title": job.title,
                            "company": job.company,
                            "location": job.location,
                            "salary": job.salary,
                            "detailed_info": detail_result,
                            "confidence": job.confidence
                        })
                        
                        # è¿”å›ä¸Šä¸€é 
                        await page.go_back()
                        await self._random_delay(1, 2)
                
                return jobs_data
                
            finally:
                await browser.close()
    
    async def _smart_click(self, page, location: Dict):
        """æ™ºèƒ½é»æ“Š (æ¨¡æ“¬äººé¡è¡Œç‚º)"""
        x = location['x'] + location['width'] // 2
        y = location['y'] + location['height'] // 2
        
        # æ»‘é¼ ç§»å‹•åˆ°ç›®æ¨™ä½ç½®
        await page.mouse.move(x, y)
        await self._random_delay(0.5, 1.5)
        
        # é»æ“Š
        await page.mouse.click(x, y)
    
    async def _random_delay(self, min_seconds: float, max_seconds: float):
        """éš¨æ©Ÿå»¶é²æ¨¡æ“¬äººé¡è¡Œç‚º"""
        import random
        delay = random.uniform(min_seconds, max_seconds)
        await asyncio.sleep(delay)
```

---

## ğŸ“Š **æ€§èƒ½ç›£æ§èˆ‡å„ªåŒ–**

### **æ™ºèƒ½ç­–ç•¥é¸æ“‡**

```python
# monitoring/strategy_optimizer.py
class StrategyOptimizer:
    def __init__(self):
        self.performance_history = {}
        self.cost_tracking = {}
    
    def select_optimal_strategy(self, context: Dict) -> ProcessingStrategy:
        """åŸºæ–¼æ­·å²æ€§èƒ½å’Œæˆæœ¬é¸æ“‡æœ€å„ªç­–ç•¥"""
        
        # æˆæœ¬æ•æ„Ÿæ¨¡å¼
        if context.get('cost_sensitive', False):
            return ProcessingStrategy.LOCAL_ONLY
            
        # é«˜æº–ç¢ºåº¦è¦æ±‚
        if context.get('accuracy_critical', False):
            return ProcessingStrategy.API_ONLY
            
        # åŸºæ–¼ç¶²ç«™é›£åº¦
        site_difficulty = self._assess_site_difficulty(context.get('url', ''))
        
        if site_difficulty > 0.8:
            return ProcessingStrategy.API_ONLY
        elif site_difficulty > 0.5:
            return ProcessingStrategy.HYBRID
        else:
            return ProcessingStrategy.LOCAL_ONLY
    
    def _assess_site_difficulty(self, url: str) -> float:
        """è©•ä¼°ç¶²ç«™çˆ¬å–é›£åº¦"""
        difficult_sites = [
            'linkedin.com',
            'glassdoor.com', 
            'indeed.com'
        ]
        
        for site in difficult_sites:
            if site in url:
                return 0.9
                
        return 0.3
```

### **æˆæœ¬æ§åˆ¶ç³»çµ±**

```yaml
Cost Control Configuration:
  Daily Limits:
    OpenAI API: $50
    Google Vision: $30
    Total API Budget: $100
  
  Strategy Rules:
    - Morning Hours (8-12): Prefer API_ONLY
    - Peak Hours (12-18): Use HYBRID  
    - Night Hours (18-8): Use LOCAL_ONLY
    
  Fallback Rules:
    - If API quota exceeded: Switch to LOCAL_ONLY
    - If local model fails: Limited API usage
    - Emergency mode: Pure local processing
```

---

## ğŸ¯ **å¯¦æ–½å„ªå…ˆç´šèˆ‡æ™‚ç¨‹**

### **Phase 1: åŸºç¤ AI æ•´åˆ (4 é€±)**
- Week 1: OpenAI Vision API æ•´åˆ
- Week 2: æœ¬åœ°æ¨¡å‹é…ç½® (CLIP + OCR)
- Week 3: æ··åˆè™•ç†é‚è¼¯é–‹ç™¼
- Week 4: åŸºç¤æ¸¬è©¦å’Œèª¿å„ª

### **Phase 2: æ™ºèƒ½çˆ¬èŸ²é–‹ç™¼ (6 é€±)**  
- Week 5-6: Playwright æ•´åˆå’Œååµæ¸¬
- Week 7-8: è¦–è¦ºå°å‘æ“ä½œé‚è¼¯
- Week 9-10: å¤šç¶²ç«™é©é…å’Œæ¸¬è©¦

### **Phase 3: å„ªåŒ–èˆ‡éƒ¨ç½² (4 é€±)**
- Week 11-12: æ€§èƒ½å„ªåŒ–å’Œæˆæœ¬æ§åˆ¶
- Week 13-14: ç”Ÿç”¢éƒ¨ç½²å’Œç›£æ§

é€™å€‹ AI è¦–è¦ºå¢å¼·æ–¹æ¡ˆå°‡å¤§å¹…æå‡ JobSpy çš„çˆ¬èŸ²æˆåŠŸç‡å’Œé©æ‡‰æ€§ï¼ŒåŒæ™‚æä¾›æˆæœ¬å¯æ§çš„æ··åˆè™•ç†ç­–ç•¥ã€‚