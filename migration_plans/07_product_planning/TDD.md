# JobSpy v2 æŠ€è¡“è¨­è¨ˆæ–‡æª” (TDD)

## ğŸ—ï¸ ç³»çµ±æ¶æ§‹æ¦‚è¿°

### æ¶æ§‹åŸå‰‡
- **å¾®æœå‹™åŒ–**: æ¨¡å¡ŠåŒ–è¨­è¨ˆï¼Œç¨ç«‹éƒ¨ç½²
- **ç•°æ­¥å„ªå…ˆ**: æé«˜éŸ¿æ‡‰æ€§å’Œååé‡
- **AI å¢å¼·**: æ™ºèƒ½åŒ–æ•¸æ“šè™•ç†å’Œç”¨æˆ¶é«”é©—
- **é›²åŸç”Ÿ**: å®¹å™¨åŒ–éƒ¨ç½²ï¼Œå½ˆæ€§æ“´å±•
- **å®‰å…¨ç¬¬ä¸€**: ç«¯åˆ°ç«¯å®‰å…¨ä¿è­·

### æŠ€è¡“å †ç–Š

#### å¾Œç«¯ (Backend)
```
FastAPI (Python 3.11+)
â”œâ”€â”€ API æ¡†æ¶: FastAPI + Pydantic
â”œâ”€â”€ ç•°æ­¥è™•ç†: asyncio + aiohttp
â”œâ”€â”€ æ•¸æ“šåº«: PostgreSQL + Redis
â”œâ”€â”€ AI é›†æˆ: OpenAI GPT-4V + Local VLM
â”œâ”€â”€ æ¶ˆæ¯éšŠåˆ—: Celery + Redis
â””â”€â”€ éƒ¨ç½²: Docker + Gunicorn
```

#### å‰ç«¯ (Frontend)
```
React 18 + TypeScript
â”œâ”€â”€ æ§‹å»ºå·¥å…·: Vite
â”œâ”€â”€ ç‹€æ…‹ç®¡ç†: Zustand
â”œâ”€â”€ æ•¸æ“šç²å–: TanStack Query
â”œâ”€â”€ UI æ¡†æ¶: Tailwind CSS + Headless UI
â”œâ”€â”€ å‹•ç•«: Framer Motion
â””â”€â”€ æ¸¬è©¦: Vitest + Testing Library
```

#### åŸºç¤è¨­æ–½ (Infrastructure)
```
å®¹å™¨åŒ– & ç·¨æ’
â”œâ”€â”€ é–‹ç™¼ç’°å¢ƒ: Docker Compose
â”œâ”€â”€ ç”Ÿç”¢ç’°å¢ƒ: Kubernetes
â”œâ”€â”€ ç›£æ§: Prometheus + Grafana
â”œâ”€â”€ æ—¥èªŒ: ELK Stack
â””â”€â”€ CI/CD: GitHub Actions
```

## ğŸ”„ ç³»çµ±æ¶æ§‹åœ–

```mermaid
graph TB
    subgraph "Frontend Layer"
        UI[React + TypeScript UI]
        PWA[Progressive Web App]
    end
    
    subgraph "API Gateway"
        Gateway[FastAPI Gateway]
        Auth[Authentication Service]
        RateLimit[Rate Limiting]
    end
    
    subgraph "Core Services"
        JobSearch[Job Search Service]
        AIVision[AI Vision Service]
        UserService[User Service]
        Analytics[Analytics Service]
    end
    
    subgraph "AI Layer"
        GPT4V[OpenAI GPT-4V]
        LocalVLM[Local VLM (CLIP)]
        NLP[NLP Processing]
    end
    
    subgraph "Data Layer"
        PostgreSQL[(PostgreSQL)]
        Redis[(Redis Cache)]
        MinIO[(MinIO Storage)]
    end
    
    subgraph "External APIs"
        Indeed[Indeed API]
        LinkedIn[LinkedIn API]
        Glassdoor[Glassdoor API]
    end
    
    UI --> Gateway
    PWA --> Gateway
    Gateway --> Auth
    Gateway --> RateLimit
    Gateway --> JobSearch
    Gateway --> AIVision
    Gateway --> UserService
    Gateway --> Analytics
    
    JobSearch --> GPT4V
    JobSearch --> LocalVLM
    AIVision --> GPT4V
    AIVision --> LocalVLM
    
    JobSearch --> PostgreSQL
    JobSearch --> Redis
    AIVision --> MinIO
    
    JobSearch --> Indeed
    JobSearch --> LinkedIn
    JobSearch --> Glassdoor
```

## ğŸ› ï¸ æ ¸å¿ƒæœå‹™è¨­è¨ˆ

### 1. Job Search Service

#### æœå‹™è²¬ä»»
- çµ±ä¸€æ±‚è·æœç´¢æ¥å£
- å¤šå¹³å°æ•¸æ“šèšåˆ
- æ™ºèƒ½è·¯ç”±å’Œè² è¼‰å‡è¡¡
- æœç´¢çµæœå„ªåŒ–

#### API è¨­è¨ˆ
```python
# æœç´¢æ¥å£
POST /api/v1/jobs/search
{
    "query": "software engineer in taipei",
    "location": "taipei, taiwan",
    "experience_level": "mid-level",
    "salary_range": {"min": 80000, "max": 150000},
    "job_type": "full-time",
    "remote_ok": true,
    "ai_enhance": true
}

# éŸ¿æ‡‰æ ¼å¼
{
    "jobs": [...],
    "total_count": 156,
    "search_metadata": {
        "query_analysis": {...},
        "sources_used": ["indeed", "linkedin"],
        "ai_enhancement_used": true,
        "processing_time_ms": 2340
    },
    "pagination": {...}
}
```

#### æ•¸æ“šæ¨¡å‹
```python
class JobListing(BaseModel):
    id: str
    title: str
    company: str
    location: str
    description: str
    requirements: List[str]
    salary_range: Optional[SalaryRange]
    job_type: JobType
    experience_level: ExperienceLevel
    remote_friendly: bool
    posted_date: datetime
    source: str
    source_url: str
    ai_analysis: Optional[AIAnalysis]
```

### 2. AI Vision Service

#### æœå‹™è²¬ä»»
- ç¶²ç«™æˆªåœ–åˆ†æ
- è·ä½ä¿¡æ¯æå–
- åæ©Ÿå™¨äººæª¢æ¸¬
- æˆæœ¬å„ªåŒ–æ§åˆ¶

#### ä¸‰å±¤ç­–ç•¥æ¶æ§‹
```python
class AIVisionStrategy:
    """ä¸‰å±¤AIè¦–è¦ºç­–ç•¥"""
    
    async def analyze_page(self, url: str) -> JobExtraction:
        # Layer 1: å˜—è©¦ API ç²å–
        if api_result := await self.try_api_extraction(url):
            return api_result
            
        # Layer 2: å˜—è©¦å‚³çµ±çˆ¬èŸ²
        if scraping_result := await self.try_scraping(url):
            return scraping_result
            
        # Layer 3: AI è¦–è¦ºåˆ†æ
        return await self.ai_vision_analysis(url)
    
    async def ai_vision_analysis(self, url: str) -> JobExtraction:
        # æˆæœ¬åˆ†æ
        cost_estimate = await self.estimate_cost(url)
        if cost_estimate > self.max_cost_threshold:
            return await self.local_vlm_analysis(url)
        
        # é›²ç«¯ AI åˆ†æ
        return await self.cloud_vlm_analysis(url)
```

#### è³‡æºæ± è¨­è¨ˆ
```python
class ResourcePools:
    """çµ±ä¸€è³‡æºæ± ç®¡ç†"""
    
    def __init__(self):
        self.ua_pool = UserAgentPool()
        self.proxy_pool = ProxyPool()
        self.token_pool = TokenPool()
        self.session_pool = SessionPool()
        self.worker_pool = WorkerPool()
        self.parser_pool = ParserPool()
    
    async def get_resources(self) -> ResourceBundle:
        """ç²å–å¯ç”¨è³‡æºçµ„åˆ"""
        return ResourceBundle(
            user_agent=await self.ua_pool.get_random(),
            proxy=await self.proxy_pool.get_healthy(),
            token=await self.token_pool.get_valid(),
            session=await self.session_pool.get_available(),
            worker=await self.worker_pool.assign(),
            parser=await self.parser_pool.get_optimal()
        )
```

### 3. æ•¸æ“šåº«è¨­è¨ˆ

#### PostgreSQL Schema
```sql
-- ç”¨æˆ¶è¡¨
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    profile JSONB,
    preferences JSONB,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- è·ä½è¡¨
CREATE TABLE job_listings (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    title VARCHAR(500) NOT NULL,
    company VARCHAR(255) NOT NULL,
    location VARCHAR(255),
    description TEXT,
    requirements TEXT[],
    salary_range JSONB,
    job_type VARCHAR(50),
    experience_level VARCHAR(50),
    remote_friendly BOOLEAN DEFAULT FALSE,
    source VARCHAR(100) NOT NULL,
    source_url VARCHAR(1000),
    raw_data JSONB,
    ai_analysis JSONB,
    posted_date TIMESTAMP,
    scraped_date TIMESTAMP DEFAULT NOW(),
    INDEX idx_title_company (title, company),
    INDEX idx_location (location),
    INDEX idx_posted_date (posted_date)
);

-- æœç´¢è¨˜éŒ„è¡¨
CREATE TABLE search_logs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id),
    query_text TEXT NOT NULL,
    filters JSONB,
    results_count INTEGER,
    processing_time_ms INTEGER,
    ai_used BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT NOW()
);

-- AI åˆ†æç·©å­˜è¡¨
CREATE TABLE ai_analysis_cache (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    url_hash VARCHAR(64) UNIQUE NOT NULL,
    analysis_result JSONB NOT NULL,
    cost_used DECIMAL(10,4),
    model_used VARCHAR(100),
    created_at TIMESTAMP DEFAULT NOW(),
    expires_at TIMESTAMP
);
```

#### Redis ç·©å­˜ç­–ç•¥
```python
class CacheStrategy:
    """ç·©å­˜ç­–ç•¥é…ç½®"""
    
    CACHE_CONFIGS = {
        "job_listings": {
            "ttl": 3600,  # 1å°æ™‚
            "key_pattern": "job:{source}:{job_id}"
        },
        "search_results": {
            "ttl": 1800,  # 30åˆ†é˜
            "key_pattern": "search:{query_hash}"
        },
        "ai_analysis": {
            "ttl": 86400,  # 24å°æ™‚
            "key_pattern": "ai:{url_hash}"
        },
        "user_sessions": {
            "ttl": 7200,  # 2å°æ™‚
            "key_pattern": "session:{user_id}"
        }
    }
```

## ğŸ”„ API è¨­è¨ˆè¦ç¯„

### RESTful API åŸå‰‡
- ä½¿ç”¨æ¨™æº– HTTP æ–¹æ³• (GET, POST, PUT, DELETE)
- èªç¾©åŒ– URL è¨­è¨ˆ
- çµ±ä¸€éŸ¿æ‡‰æ ¼å¼
- é©ç•¶çš„ HTTP ç‹€æ…‹ç¢¼

### API ç‰ˆæœ¬æ§åˆ¶
```python
# URL ç‰ˆæœ¬æ§åˆ¶
/api/v1/jobs/search
/api/v2/jobs/search

# Header ç‰ˆæœ¬æ§åˆ¶
API-Version: v1
Accept: application/vnd.jobspy.v1+json
```

### çµ±ä¸€éŸ¿æ‡‰æ ¼å¼
```python
class APIResponse(BaseModel):
    """çµ±ä¸€ API éŸ¿æ‡‰æ ¼å¼"""
    
    success: bool
    data: Optional[Any] = None
    error: Optional[ErrorDetail] = None
    metadata: Optional[Dict[str, Any]] = None
    timestamp: datetime = Field(default_factory=datetime.now)
    request_id: str = Field(default_factory=lambda: str(uuid.uuid4()))

# æˆåŠŸéŸ¿æ‡‰
{
    "success": true,
    "data": {...},
    "metadata": {
        "pagination": {...},
        "total_count": 100
    },
    "timestamp": "2024-01-01T12:00:00Z",
    "request_id": "123e4567-e89b-12d3-a456-426614174000"
}

# éŒ¯èª¤éŸ¿æ‡‰
{
    "success": false,
    "error": {
        "code": "INVALID_QUERY",
        "message": "æœç´¢æŸ¥è©¢æ ¼å¼ç„¡æ•ˆ",
        "details": {...}
    },
    "timestamp": "2024-01-01T12:00:00Z",
    "request_id": "123e4567-e89b-12d3-a456-426614174000"
}
```

## ğŸ” å®‰å…¨è¨­è¨ˆ

### èªè­‰èˆ‡æˆæ¬Š
```python
# JWT Token çµæ§‹
{
    "sub": "user_id",
    "email": "user@example.com",
    "role": "premium_user",
    "permissions": ["search", "ai_enhance", "analytics"],
    "exp": 1640995200,
    "iat": 1640908800
}

# API æ¬Šé™æ§åˆ¶
@require_permission("ai_enhance")
async def ai_vision_analyze(request: AnalysisRequest):
    pass

@rate_limit("100/hour")
async def search_jobs(request: SearchRequest):
    pass
```

### æ•¸æ“šä¿è­·
```python
class DataProtection:
    """æ•¸æ“šä¿è­·ç­–ç•¥"""
    
    @staticmethod
    def encrypt_pii(data: str) -> str:
        """åŠ å¯†å€‹äººè­˜åˆ¥ä¿¡æ¯"""
        return fernet.encrypt(data.encode()).decode()
    
    @staticmethod
    def hash_sensitive_data(data: str) -> str:
        """å“ˆå¸Œæ•æ„Ÿæ•¸æ“š"""
        return hashlib.sha256(data.encode()).hexdigest()
    
    @staticmethod
    def anonymize_logs(log_data: dict) -> dict:
        """æ—¥èªŒæ•¸æ“šåŒ¿ååŒ–"""
        return {k: "***" if k in PII_FIELDS else v 
                for k, v in log_data.items()}
```

## ğŸ“Š ç›£æ§èˆ‡æ—¥èªŒ

### æ€§èƒ½ç›£æ§æŒ‡æ¨™
```python
class Metrics:
    """æ€§èƒ½ç›£æ§æŒ‡æ¨™"""
    
    # æ¥­å‹™æŒ‡æ¨™
    SEARCH_REQUESTS_TOTAL = Counter("search_requests_total")
    AI_ANALYSIS_REQUESTS = Counter("ai_analysis_requests_total")
    JOB_EXTRACTION_SUCCESS_RATE = Gauge("job_extraction_success_rate")
    
    # æŠ€è¡“æŒ‡æ¨™
    API_RESPONSE_TIME = Histogram("api_response_time_seconds")
    DATABASE_QUERY_TIME = Histogram("database_query_time_seconds")
    CACHE_HIT_RATE = Gauge("cache_hit_rate")
    
    # è³‡æºæŒ‡æ¨™
    ACTIVE_SESSIONS = Gauge("active_sessions")
    AI_TOKENS_USED = Counter("ai_tokens_used_total")
    PROXY_POOL_HEALTH = Gauge("proxy_pool_health_score")
```

### çµæ§‹åŒ–æ—¥èªŒ
```python
import structlog

logger = structlog.get_logger()

# æ¥­å‹™äº‹ä»¶æ—¥èªŒ
await logger.ainfo(
    "job_search_completed",
    user_id=user.id,
    query=search_query,
    results_count=len(results),
    processing_time_ms=processing_time,
    ai_enhanced=ai_used,
    sources_used=sources
)

# éŒ¯èª¤æ—¥èªŒ
await logger.aerror(
    "ai_analysis_failed",
    url=target_url,
    error_type=error.__class__.__name__,
    error_message=str(error),
    fallback_used=True
)
```

## ğŸš€ éƒ¨ç½²èˆ‡æ“´å±•

### å®¹å™¨åŒ–é…ç½®
```dockerfile
# å¤šéšæ®µæ§‹å»º
FROM python:3.11-slim as builder
COPY requirements.txt .
RUN pip install --user -r requirements.txt

FROM python:3.11-slim
COPY --from=builder /root/.local /root/.local
COPY ./app /app
WORKDIR /app
CMD ["gunicorn", "main:app", "-k", "uvicorn.workers.UvicornWorker"]
```

### Kubernetes éƒ¨ç½²
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jobspy-backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: jobspy-backend
  template:
    metadata:
      labels:
        app: jobspy-backend
    spec:
      containers:
      - name: backend
        image: jobspy/backend:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: jobspy-secrets
              key: database-url
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
```

### è‡ªå‹•æ“´å±•ç­–ç•¥
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: jobspy-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: jobspy-backend
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

## ğŸ§ª æ¸¬è©¦ç­–ç•¥

### æ¸¬è©¦é‡‘å­—å¡”
```python
# å–®å…ƒæ¸¬è©¦ (70%)
class TestJobSearchService:
    async def test_search_with_valid_query(self):
        service = JobSearchService()
        results = await service.search("python developer")
        assert len(results) > 0
        assert all(result.title for result in results)

# é›†æˆæ¸¬è©¦ (20%)
class TestAPIIntegration:
    async def test_end_to_end_search_flow(self):
        async with AsyncClient() as client:
            response = await client.post("/api/v1/jobs/search", json={
                "query": "software engineer",
                "location": "taipei"
            })
            assert response.status_code == 200
            data = response.json()
            assert data["success"] is True

# E2E æ¸¬è©¦ (10%)
class TestUserJourney:
    def test_complete_user_search_journey(self):
        # ä½¿ç”¨ Playwright é€²è¡Œç«¯åˆ°ç«¯æ¸¬è©¦
        pass
```

### æ€§èƒ½æ¸¬è©¦
```python
# è² è¼‰æ¸¬è©¦
import asyncio
import aiohttp
from locust import HttpUser, task

class JobSearchUser(HttpUser):
    @task
    def search_jobs(self):
        self.client.post("/api/v1/jobs/search", json={
            "query": "software engineer",
            "location": "taiwan"
        })
```

## ğŸ“‹ é–‹ç™¼å·¥ä½œæµ

### Git å·¥ä½œæµ
```bash
# åŠŸèƒ½åˆ†æ”¯é–‹ç™¼
git checkout -b feature/ai-vision-service
git commit -m "feat: implement AI vision analysis"
git push origin feature/ai-vision-service

# Pull Request å¯©æŸ¥
# CI/CD è‡ªå‹•æ¸¬è©¦
# ä»£ç¢¼åˆä½µåˆ° main
```

### CI/CD æµæ°´ç·š
```yaml
name: CI/CD Pipeline
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    - name: Install dependencies
      run: pip install -r requirements.txt
    - name: Run tests
      run: pytest --cov=app
    - name: Run linting
      run: |
        black --check app/
        isort --check-only app/
        mypy app/

  deploy:
    needs: test
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
    - name: Deploy to production
      run: |
        kubectl apply -f k8s/
        kubectl rollout status deployment/jobspy-backend
```

## ğŸ¯ æ€§èƒ½å„ªåŒ–ç­–ç•¥

### æ•¸æ“šåº«å„ªåŒ–
```sql
-- ç´¢å¼•å„ªåŒ–
CREATE INDEX CONCURRENTLY idx_job_listings_search 
ON job_listings USING GIN (
    to_tsvector('english', title || ' ' || description)
);

-- åˆ†å€è¡¨
CREATE TABLE job_listings_2024 PARTITION OF job_listings
FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');
```

### ç·©å­˜ç­–ç•¥
```python
class MultiLevelCache:
    """å¤šç´šç·©å­˜ç­–ç•¥"""
    
    def __init__(self):
        self.l1_cache = {}  # å…§å­˜ç·©å­˜
        self.l2_cache = redis_client  # Redis ç·©å­˜
        self.l3_cache = postgresql  # æ•¸æ“šåº«
    
    async def get(self, key: str):
        # L1: å…§å­˜ç·©å­˜
        if key in self.l1_cache:
            return self.l1_cache[key]
        
        # L2: Redis ç·©å­˜
        if value := await self.l2_cache.get(key):
            self.l1_cache[key] = value
            return value
        
        # L3: æ•¸æ“šåº«æŸ¥è©¢
        value = await self.l3_cache.get(key)
        if value:
            await self.l2_cache.set(key, value, ttl=3600)
            self.l1_cache[key] = value
        
        return value
```

### ç•°æ­¥è™•ç†å„ªåŒ–
```python
class OptimizedJobSearch:
    """å„ªåŒ–çš„ç•°æ­¥æ±‚è·æœç´¢"""
    
    async def parallel_search(self, query: str, sources: List[str]):
        """ä¸¦è¡Œæœç´¢å¤šå€‹ä¾†æº"""
        tasks = [
            self.search_source(query, source) 
            for source in sources
        ]
        
        # ä½¿ç”¨ asyncio.gather ä¸¦è¡ŒåŸ·è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # éæ¿¾ç•°å¸¸çµæœ
        valid_results = [
            result for result in results 
            if not isinstance(result, Exception)
        ]
        
        return self.merge_and_dedupe(valid_results)
```

é€™å€‹æŠ€è¡“è¨­è¨ˆæ–‡æª”æä¾›äº† JobSpy v2 çš„å®Œæ•´æŠ€è¡“æ¶æ§‹å’Œå¯¦ç¾ç­–ç•¥ã€‚æ¥ä¸‹ä¾†æˆ‘å€‘å¯ä»¥å‰µå»ºå…·é«”çš„é–‹ç™¼è·¯ç·šåœ–å’ŒåŸå‹é©—è­‰è¨ˆåŠƒã€‚ä½ å¸Œæœ›æˆ‘ç¹¼çºŒå‰µå»ºé–‹ç™¼è·¯ç·šåœ–é‚„æ˜¯æœ‰å…¶ä»–ç‰¹å®šçš„æŠ€è¡“ç´°ç¯€éœ€è¦è¨è«–ï¼Ÿ