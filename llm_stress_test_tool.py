#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMå£“åŠ›æ¸¬è©¦å·¥å…·
ç”¨æ–¼æ¸¬è©¦LLMæ¨¡å‹åœ¨é«˜è² è¼‰ã€ä¸¦ç™¼å’Œæ¥µç«¯æ¢ä»¶ä¸‹çš„è¡¨ç¾

Author: JobSpy Team
Date: 2025-01-27
"""

import sys
import os
import json
import time
import asyncio
import threading
import statistics
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Callable
from dataclasses import dataclass, asdict, field
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from enum import Enum
import random
import psutil
import gc
from pathlib import Path
import queue
import multiprocessing as mp

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from jobseeker.llm_intent_analyzer import LLMIntentAnalyzer, LLMProvider
from jobseeker.llm_config import LLMConfigManager


class StressTestType(Enum):
    """å£“åŠ›æ¸¬è©¦é¡å‹"""
    CONCURRENT_LOAD = "concurrent_load"  # ä¸¦ç™¼è² è¼‰æ¸¬è©¦
    SUSTAINED_LOAD = "sustained_load"  # æŒçºŒè² è¼‰æ¸¬è©¦
    BURST_LOAD = "burst_load"  # çªç™¼è² è¼‰æ¸¬è©¦
    MEMORY_STRESS = "memory_stress"  # è¨˜æ†¶é«”å£“åŠ›æ¸¬è©¦
    RATE_LIMITING = "rate_limiting"  # é€Ÿç‡é™åˆ¶æ¸¬è©¦
    TIMEOUT_STRESS = "timeout_stress"  # è¶…æ™‚å£“åŠ›æ¸¬è©¦
    ERROR_RECOVERY = "error_recovery"  # éŒ¯èª¤æ¢å¾©æ¸¬è©¦
    SCALABILITY = "scalability"  # æ“´å±•æ€§æ¸¬è©¦


class LoadPattern(Enum):
    """è² è¼‰æ¨¡å¼"""
    CONSTANT = "constant"  # æ†å®šè² è¼‰
    RAMP_UP = "ramp_up"  # é€æ¼¸å¢åŠ 
    RAMP_DOWN = "ramp_down"  # é€æ¼¸æ¸›å°‘
    SPIKE = "spike"  # å°–å³°è² è¼‰
    WAVE = "wave"  # æ³¢æµªå¼è² è¼‰
    RANDOM = "random"  # éš¨æ©Ÿè² è¼‰


@dataclass
class StressTestConfig:
    """å£“åŠ›æ¸¬è©¦é…ç½®"""
    test_type: StressTestType
    load_pattern: LoadPattern
    duration_seconds: int
    max_concurrent_requests: int
    requests_per_second: float
    ramp_up_time: int = 0
    ramp_down_time: int = 0
    burst_interval: int = 10
    burst_duration: int = 5
    timeout_seconds: float = 30.0
    memory_limit_mb: int = 1024
    error_threshold: float = 0.1
    retry_attempts: int = 3
    cool_down_time: int = 5
    enable_monitoring: bool = True
    save_detailed_logs: bool = True


@dataclass
class StressTestResult:
    """å£“åŠ›æ¸¬è©¦çµæœ"""
    test_id: str
    provider: str
    config: StressTestConfig
    start_time: str
    end_time: str
    total_duration: float
    total_requests: int
    successful_requests: int
    failed_requests: int
    timeout_requests: int
    error_requests: int
    success_rate: float
    avg_response_time: float
    median_response_time: float
    p95_response_time: float
    p99_response_time: float
    min_response_time: float
    max_response_time: float
    throughput: float  # æ¯ç§’æˆåŠŸè«‹æ±‚æ•¸
    peak_memory_usage: float  # MB
    avg_memory_usage: float  # MB
    cpu_usage_stats: Dict[str, float]
    error_distribution: Dict[str, int]
    response_time_distribution: List[float]
    latency_percentiles: Dict[str, float]
    stability_score: float
    performance_degradation: float
    resource_efficiency: float
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class SystemMetrics:
    """ç³»çµ±æŒ‡æ¨™"""
    timestamp: float
    cpu_percent: float
    memory_percent: float
    memory_used_mb: float
    active_threads: int
    open_files: int
    network_connections: int


class SystemMonitor:
    """ç³»çµ±ç›£æ§å™¨"""
    
    def __init__(self, interval: float = 1.0):
        self.interval = interval
        self.metrics: List[SystemMetrics] = []
        self.monitoring = False
        self.monitor_thread = None
        self.process = psutil.Process()
    
    def start_monitoring(self) -> None:
        """é–‹å§‹ç›£æ§"""
        if self.monitoring:
            return
        
        self.monitoring = True
        self.metrics.clear()
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
    
    def stop_monitoring(self) -> None:
        """åœæ­¢ç›£æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=2.0)
    
    def _monitor_loop(self) -> None:
        """ç›£æ§å¾ªç’°"""
        while self.monitoring:
            try:
                # æ”¶é›†ç³»çµ±æŒ‡æ¨™
                cpu_percent = self.process.cpu_percent()
                memory_info = self.process.memory_info()
                memory_percent = self.process.memory_percent()
                
                # ç·šç¨‹å’Œæ–‡ä»¶æè¿°ç¬¦
                try:
                    num_threads = self.process.num_threads()
                    num_fds = self.process.num_fds() if hasattr(self.process, 'num_fds') else 0
                except (psutil.AccessDenied, AttributeError):
                    num_threads = 0
                    num_fds = 0
                
                # ç¶²è·¯é€£æ¥
                try:
                    connections = len(self.process.connections())
                except (psutil.AccessDenied, psutil.NoSuchProcess):
                    connections = 0
                
                metrics = SystemMetrics(
                    timestamp=time.time(),
                    cpu_percent=cpu_percent,
                    memory_percent=memory_percent,
                    memory_used_mb=memory_info.rss / 1024 / 1024,
                    active_threads=num_threads,
                    open_files=num_fds,
                    network_connections=connections
                )
                
                self.metrics.append(metrics)
                
                # é™åˆ¶è¨˜éŒ„æ•¸é‡ï¼Œé¿å…è¨˜æ†¶é«”æº¢å‡º
                if len(self.metrics) > 10000:
                    self.metrics = self.metrics[-5000:]
                
                time.sleep(self.interval)
                
            except Exception as e:
                print(f"ç›£æ§éŒ¯èª¤: {e}")
                time.sleep(self.interval)
    
    def get_peak_memory(self) -> float:
        """ç²å–å³°å€¼è¨˜æ†¶é«”ä½¿ç”¨é‡"""
        if not self.metrics:
            return 0.0
        return max(m.memory_used_mb for m in self.metrics)
    
    def get_avg_memory(self) -> float:
        """ç²å–å¹³å‡è¨˜æ†¶é«”ä½¿ç”¨é‡"""
        if not self.metrics:
            return 0.0
        return statistics.mean(m.memory_used_mb for m in self.metrics)
    
    def get_cpu_stats(self) -> Dict[str, float]:
        """ç²å–CPUçµ±è¨ˆ"""
        if not self.metrics:
            return {"avg": 0.0, "max": 0.0, "min": 0.0}
        
        cpu_values = [m.cpu_percent for m in self.metrics]
        return {
            "avg": statistics.mean(cpu_values),
            "max": max(cpu_values),
            "min": min(cpu_values)
        }


class LLMStressTestTool:
    """LLMå£“åŠ›æ¸¬è©¦å·¥å…·"""
    
    def __init__(self, config_file: Optional[str] = None):
        """åˆå§‹åŒ–å£“åŠ›æ¸¬è©¦å·¥å…·"""
        self.config_manager = LLMConfigManager()
        self.system_monitor = SystemMonitor()
        self.test_queries = self._generate_test_queries()
        self.results: List[StressTestResult] = []
        self.active_requests = 0
        self.request_lock = threading.Lock()
        
        # è¨­ç½®éš¨æ©Ÿç¨®å­
        random.seed(42)
        np.random.seed(42)
    
    def _generate_test_queries(self) -> List[str]:
        """ç”Ÿæˆæ¸¬è©¦æŸ¥è©¢"""
        return [
            # ç°¡å–®æŸ¥è©¢
            "æ‰¾å·¥ä½œ",
            "æˆ‘æƒ³æ‰¾è»Ÿé«”å·¥ç¨‹å¸«çš„å·¥ä½œ",
            "Looking for data scientist position",
            "ä»Šå¤©å¤©æ°£å¦‚ä½•ï¼Ÿ",
            "æˆ‘æƒ³è½‰è·",
            
            # ä¸­ç­‰è¤‡é›œåº¦æŸ¥è©¢
            "æˆ‘æƒ³åœ¨å°åŒ—æ‰¾å¹´è–ª80è¬ä»¥ä¸Šçš„Pythoné–‹ç™¼å·¥ç¨‹å¸«è·ä½",
            "å°‹æ‰¾æœ‰å½ˆæ€§å·¥æ™‚çš„å‰ç«¯å·¥ç¨‹å¸«å·¥ä½œï¼Œè¦æ±‚æœƒReactå’ŒVue",
            "I want to find a remote machine learning engineer job with 5+ years experience",
            "ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ",
            "æˆ‘æƒ³å­¸ç¿’æ–°æŠ€èƒ½ä¾†æå‡è·å ´ç«¶çˆ­åŠ›",
            
            # è¤‡é›œæŸ¥è©¢
            "æˆ‘æ˜¯ä¸€å€‹æœ‰10å¹´ç¶“é©—çš„å…¨ç«¯å·¥ç¨‹å¸«ï¼Œç²¾é€šå¤šç¨®ç¨‹å¼èªè¨€ï¼Œç¾åœ¨æƒ³æ‰¾æŠ€è¡“ä¸»ç®¡è·ä½ï¼Œè–ªè³‡æœŸæœ›150Kä»¥ä¸Šï¼Œåœ°é»å°åŒ—æˆ–æ–°ç«¹",
            "Looking for senior software architect position in fintech company, remote work preferred, salary 200K+, equity options",
            "æˆ‘æƒ³äº†è§£è³‡æ–™ç§‘å­¸é ˜åŸŸçš„è·æ¶¯ç™¼å±•è·¯å¾‘å’Œæ‰€éœ€æŠ€èƒ½",
            "å¦‚ä½•åœ¨ä¸åŒçš„ç§‘æŠ€å…¬å¸ä¹‹é–“åšé¸æ“‡ï¼Ÿ",
            "æˆ‘æƒ³å‰µæ¥­ï¼Œéœ€è¦ä»€éº¼æº–å‚™ï¼Ÿ",
            
            # é‚Šç•Œæ¡ˆä¾‹
            "",
            "ï¼Ÿï¼ã€‚",
            "123456",
            "å·¥ä½œ" * 50,
            "a" * 1000,
            
            # å¤šèªè¨€æŸ¥è©¢
            "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã®ä»•äº‹ã‚’æ¢ã—ã¦ã„ã¾ã™",
            "ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ ì¼ìë¦¬ë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤",
            "Je cherche un emploi d'ingÃ©nieur logiciel",
            "Ich suche eine Stelle als Softwareentwickler",
            "æˆ‘æƒ³æ‰¾è½¯ä»¶å·¥ç¨‹å¸ˆçš„å·¥ä½œ",  # ç°¡é«”ä¸­æ–‡
        ]
    
    def run_stress_test(self, provider: LLMProvider, config: StressTestConfig) -> StressTestResult:
        """é‹è¡Œå£“åŠ›æ¸¬è©¦"""
        test_id = f"stress_{provider.value}_{config.test_type.value}_{int(time.time())}"
        
        print(f"ğŸš€ é–‹å§‹å£“åŠ›æ¸¬è©¦: {test_id}")
        print(f"   æä¾›å•†: {provider.value}")
        print(f"   æ¸¬è©¦é¡å‹: {config.test_type.value}")
        print(f"   è² è¼‰æ¨¡å¼: {config.load_pattern.value}")
        print(f"   æŒçºŒæ™‚é–“: {config.duration_seconds}ç§’")
        print(f"   æœ€å¤§ä¸¦ç™¼: {config.max_concurrent_requests}")
        print(f"   æ¯ç§’è«‹æ±‚: {config.requests_per_second}")
        
        # é–‹å§‹ç³»çµ±ç›£æ§
        if config.enable_monitoring:
            self.system_monitor.start_monitoring()
        
        start_time = time.time()
        start_time_str = datetime.now().isoformat()
        
        # åŸ·è¡Œæ¸¬è©¦
        try:
            if config.test_type == StressTestType.CONCURRENT_LOAD:
                test_results = self._run_concurrent_load_test(provider, config)
            elif config.test_type == StressTestType.SUSTAINED_LOAD:
                test_results = self._run_sustained_load_test(provider, config)
            elif config.test_type == StressTestType.BURST_LOAD:
                test_results = self._run_burst_load_test(provider, config)
            elif config.test_type == StressTestType.MEMORY_STRESS:
                test_results = self._run_memory_stress_test(provider, config)
            elif config.test_type == StressTestType.RATE_LIMITING:
                test_results = self._run_rate_limiting_test(provider, config)
            elif config.test_type == StressTestType.TIMEOUT_STRESS:
                test_results = self._run_timeout_stress_test(provider, config)
            elif config.test_type == StressTestType.ERROR_RECOVERY:
                test_results = self._run_error_recovery_test(provider, config)
            elif config.test_type == StressTestType.SCALABILITY:
                test_results = self._run_scalability_test(provider, config)
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„æ¸¬è©¦é¡å‹: {config.test_type}")
        
        except Exception as e:
            print(f"âŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {str(e)}")
            test_results = {
                "requests": [],
                "errors": [str(e)],
                "timeouts": [],
                "response_times": []
            }
        
        finally:
            # åœæ­¢ç³»çµ±ç›£æ§
            if config.enable_monitoring:
                self.system_monitor.stop_monitoring()
        
        end_time = time.time()
        end_time_str = datetime.now().isoformat()
        total_duration = end_time - start_time
        
        # åˆ†æçµæœ
        result = self._analyze_stress_test_results(
            test_id, provider, config, start_time_str, end_time_str, 
            total_duration, test_results
        )
        
        self.results.append(result)
        
        print(f"âœ… å£“åŠ›æ¸¬è©¦å®Œæˆ: {test_id}")
        print(f"   ç¸½è«‹æ±‚æ•¸: {result.total_requests}")
        print(f"   æˆåŠŸç‡: {result.success_rate:.1%}")
        print(f"   å¹³å‡éŸ¿æ‡‰æ™‚é–“: {result.avg_response_time:.2f}ç§’")
        print(f"   ååé‡: {result.throughput:.1f} è«‹æ±‚/ç§’")
        
        return result
    
    def _run_concurrent_load_test(self, provider: LLMProvider, config: StressTestConfig) -> Dict[str, Any]:
        """é‹è¡Œä¸¦ç™¼è² è¼‰æ¸¬è©¦"""
        analyzer = LLMIntentAnalyzer(provider=provider)
        
        requests_data = []
        errors = []
        timeouts = []
        response_times = []
        
        # è¨ˆç®—ç¸½è«‹æ±‚æ•¸
        total_requests = int(config.duration_seconds * config.requests_per_second)
        
        # ä½¿ç”¨ç·šç¨‹æ± åŸ·è¡Œä¸¦ç™¼è«‹æ±‚
        with ThreadPoolExecutor(max_workers=config.max_concurrent_requests) as executor:
            futures = []
            
            for i in range(total_requests):
                # é¸æ“‡æ¸¬è©¦æŸ¥è©¢
                query = random.choice(self.test_queries)
                
                # æäº¤ä»»å‹™
                future = executor.submit(self._execute_single_request, analyzer, query, config.timeout_seconds)
                futures.append((future, time.time(), query))
                
                # æ§åˆ¶è«‹æ±‚é€Ÿç‡
                if i < total_requests - 1:
                    time.sleep(1.0 / config.requests_per_second)
            
            # æ”¶é›†çµæœ
            for future, submit_time, query in futures:
                try:
                    result = future.result(timeout=config.timeout_seconds + 5)
                    requests_data.append({
                        "query": query,
                        "submit_time": submit_time,
                        "result": result
                    })
                    
                    if result["success"]:
                        response_times.append(result["response_time"])
                    else:
                        errors.append(result["error"])
                        
                except Exception as e:
                    timeouts.append(str(e))
                    errors.append(str(e))
        
        return {
            "requests": requests_data,
            "errors": errors,
            "timeouts": timeouts,
            "response_times": response_times
        }
    
    def _run_sustained_load_test(self, provider: LLMProvider, config: StressTestConfig) -> Dict[str, Any]:
        """é‹è¡ŒæŒçºŒè² è¼‰æ¸¬è©¦"""
        analyzer = LLMIntentAnalyzer(provider=provider)
        
        requests_data = []
        errors = []
        timeouts = []
        response_times = []
        
        start_time = time.time()
        request_count = 0
        
        # æŒçºŒç™¼é€è«‹æ±‚ç›´åˆ°æ™‚é–“çµæŸ
        while time.time() - start_time < config.duration_seconds:
            query = random.choice(self.test_queries)
            
            try:
                result = self._execute_single_request(analyzer, query, config.timeout_seconds)
                
                requests_data.append({
                    "query": query,
                    "submit_time": time.time(),
                    "result": result
                })
                
                if result["success"]:
                    response_times.append(result["response_time"])
                else:
                    errors.append(result["error"])
                
                request_count += 1
                
                # æ§åˆ¶è«‹æ±‚é€Ÿç‡
                time.sleep(1.0 / config.requests_per_second)
                
            except Exception as e:
                errors.append(str(e))
                timeouts.append(str(e))
        
        return {
            "requests": requests_data,
            "errors": errors,
            "timeouts": timeouts,
            "response_times": response_times
        }
    
    def _run_burst_load_test(self, provider: LLMProvider, config: StressTestConfig) -> Dict[str, Any]:
        """é‹è¡Œçªç™¼è² è¼‰æ¸¬è©¦"""
        analyzer = LLMIntentAnalyzer(provider=provider)
        
        requests_data = []
        errors = []
        timeouts = []
        response_times = []
        
        start_time = time.time()
        
        while time.time() - start_time < config.duration_seconds:
            # çªç™¼æœŸé–“
            burst_start = time.time()
            burst_requests = []
            
            # åœ¨çªç™¼æœŸé–“ç™¼é€å¤§é‡ä¸¦ç™¼è«‹æ±‚
            with ThreadPoolExecutor(max_workers=config.max_concurrent_requests) as executor:
                futures = []
                
                # åœ¨çªç™¼æŒçºŒæ™‚é–“å…§ç™¼é€è«‹æ±‚
                while time.time() - burst_start < config.burst_duration:
                    query = random.choice(self.test_queries)
                    future = executor.submit(self._execute_single_request, analyzer, query, config.timeout_seconds)
                    futures.append((future, time.time(), query))
                    
                    time.sleep(0.1)  # çŸ­æš«é–“éš”
                
                # æ”¶é›†çªç™¼æœŸé–“çš„çµæœ
                for future, submit_time, query in futures:
                    try:
                        result = future.result(timeout=config.timeout_seconds + 5)
                        requests_data.append({
                            "query": query,
                            "submit_time": submit_time,
                            "result": result
                        })
                        
                        if result["success"]:
                            response_times.append(result["response_time"])
                        else:
                            errors.append(result["error"])
                            
                    except Exception as e:
                        timeouts.append(str(e))
                        errors.append(str(e))
            
            # å†·å»æœŸé–“
            time.sleep(config.burst_interval - config.burst_duration)
        
        return {
            "requests": requests_data,
            "errors": errors,
            "timeouts": timeouts,
            "response_times": response_times
        }
    
    def _run_memory_stress_test(self, provider: LLMProvider, config: StressTestConfig) -> Dict[str, Any]:
        """é‹è¡Œè¨˜æ†¶é«”å£“åŠ›æ¸¬è©¦"""
        analyzer = LLMIntentAnalyzer(provider=provider)
        
        requests_data = []
        errors = []
        timeouts = []
        response_times = []
        
        # å‰µå»ºå¤§é‡åˆ†æå™¨å¯¦ä¾‹ä¾†å¢åŠ è¨˜æ†¶é«”å£“åŠ›
        analyzers = [LLMIntentAnalyzer(provider=provider) for _ in range(10)]
        
        # å‰µå»ºå¤§é‡æ¸¬è©¦æ•¸æ“š
        large_queries = []
        for base_query in self.test_queries:
            # å‰µå»ºæ›´é•·çš„æŸ¥è©¢ä¾†å¢åŠ è¨˜æ†¶é«”ä½¿ç”¨
            large_query = base_query + " " + "é¡å¤–å…§å®¹ " * 100
            large_queries.extend([large_query] * 10)
        
        start_time = time.time()
        request_count = 0
        
        while time.time() - start_time < config.duration_seconds:
            # éš¨æ©Ÿé¸æ“‡åˆ†æå™¨å’ŒæŸ¥è©¢
            analyzer_instance = random.choice(analyzers)
            query = random.choice(large_queries)
            
            try:
                result = self._execute_single_request(analyzer_instance, query, config.timeout_seconds)
                
                requests_data.append({
                    "query": query[:100] + "...",  # æˆªæ–·é¡¯ç¤º
                    "submit_time": time.time(),
                    "result": result
                })
                
                if result["success"]:
                    response_times.append(result["response_time"])
                else:
                    errors.append(result["error"])
                
                request_count += 1
                
                # æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨é‡
                current_memory = psutil.Process().memory_info().rss / 1024 / 1024
                if current_memory > config.memory_limit_mb:
                    print(f"âš ï¸ è¨˜æ†¶é«”ä½¿ç”¨é‡è¶…éé™åˆ¶: {current_memory:.1f}MB > {config.memory_limit_mb}MB")
                    # å¼·åˆ¶åƒåœ¾å›æ”¶
                    gc.collect()
                
                time.sleep(1.0 / config.requests_per_second)
                
            except Exception as e:
                errors.append(str(e))
                timeouts.append(str(e))
        
        return {
            "requests": requests_data,
            "errors": errors,
            "timeouts": timeouts,
            "response_times": response_times
        }
    
    def _run_rate_limiting_test(self, provider: LLMProvider, config: StressTestConfig) -> Dict[str, Any]:
        """é‹è¡Œé€Ÿç‡é™åˆ¶æ¸¬è©¦"""
        analyzer = LLMIntentAnalyzer(provider=provider)
        
        requests_data = []
        errors = []
        timeouts = []
        response_times = []
        rate_limit_errors = []
        
        # é€æ¼¸å¢åŠ è«‹æ±‚é€Ÿç‡ç›´åˆ°è§¸ç™¼é™åˆ¶
        current_rps = 1.0
        max_rps = config.requests_per_second
        rps_increment = 0.5
        
        start_time = time.time()
        
        while time.time() - start_time < config.duration_seconds and current_rps <= max_rps:
            print(f"   æ¸¬è©¦é€Ÿç‡: {current_rps:.1f} è«‹æ±‚/ç§’")
            
            # åœ¨ç•¶å‰é€Ÿç‡ä¸‹æ¸¬è©¦10ç§’
            rate_test_start = time.time()
            rate_test_duration = min(10, config.duration_seconds - (time.time() - start_time))
            
            while time.time() - rate_test_start < rate_test_duration:
                query = random.choice(self.test_queries)
                
                try:
                    result = self._execute_single_request(analyzer, query, config.timeout_seconds)
                    
                    requests_data.append({
                        "query": query,
                        "submit_time": time.time(),
                        "result": result,
                        "rps": current_rps
                    })
                    
                    if result["success"]:
                        response_times.append(result["response_time"])
                    else:
                        error_msg = result["error"]
                        errors.append(error_msg)
                        
                        # æª¢æŸ¥æ˜¯å¦ç‚ºé€Ÿç‡é™åˆ¶éŒ¯èª¤
                        if any(keyword in error_msg.lower() for keyword in 
                               ["rate limit", "too many requests", "quota", "throttle"]):
                            rate_limit_errors.append({
                                "rps": current_rps,
                                "error": error_msg,
                                "time": time.time()
                            })
                    
                    time.sleep(1.0 / current_rps)
                    
                except Exception as e:
                    errors.append(str(e))
                    timeouts.append(str(e))
            
            # å¢åŠ è«‹æ±‚é€Ÿç‡
            current_rps += rps_increment
        
        return {
            "requests": requests_data,
            "errors": errors,
            "timeouts": timeouts,
            "response_times": response_times,
            "rate_limit_errors": rate_limit_errors
        }
    
    def _run_timeout_stress_test(self, provider: LLMProvider, config: StressTestConfig) -> Dict[str, Any]:
        """é‹è¡Œè¶…æ™‚å£“åŠ›æ¸¬è©¦"""
        analyzer = LLMIntentAnalyzer(provider=provider)
        
        requests_data = []
        errors = []
        timeouts = []
        response_times = []
        
        # ä½¿ç”¨ä¸åŒçš„è¶…æ™‚è¨­ç½®
        timeout_values = [1.0, 2.0, 5.0, 10.0, 15.0, 30.0]
        
        start_time = time.time()
        request_count = 0
        
        while time.time() - start_time < config.duration_seconds:
            query = random.choice(self.test_queries)
            timeout = random.choice(timeout_values)
            
            try:
                result = self._execute_single_request(analyzer, query, timeout)
                
                requests_data.append({
                    "query": query,
                    "submit_time": time.time(),
                    "result": result,
                    "timeout_setting": timeout
                })
                
                if result["success"]:
                    response_times.append(result["response_time"])
                else:
                    errors.append(result["error"])
                
                request_count += 1
                time.sleep(1.0 / config.requests_per_second)
                
            except Exception as e:
                error_msg = str(e)
                errors.append(error_msg)
                
                if "timeout" in error_msg.lower():
                    timeouts.append({
                        "query": query,
                        "timeout_setting": timeout,
                        "error": error_msg,
                        "time": time.time()
                    })
        
        return {
            "requests": requests_data,
            "errors": errors,
            "timeouts": timeouts,
            "response_times": response_times
        }
    
    def _run_error_recovery_test(self, provider: LLMProvider, config: StressTestConfig) -> Dict[str, Any]:
        """é‹è¡ŒéŒ¯èª¤æ¢å¾©æ¸¬è©¦"""
        analyzer = LLMIntentAnalyzer(provider=provider)
        
        requests_data = []
        errors = []
        timeouts = []
        response_times = []
        recovery_attempts = []
        
        # æ•…æ„å¼•å…¥éŒ¯èª¤çš„æŸ¥è©¢
        error_queries = [
            None,  # ç©ºæŸ¥è©¢
            "",  # ç©ºå­—ç¬¦ä¸²
            "a" * 10000,  # è¶…é•·æŸ¥è©¢
            "\x00\x01\x02",  # ç‰¹æ®Šå­—ç¬¦
            {"invalid": "object"},  # éŒ¯èª¤é¡å‹
        ]
        
        start_time = time.time()
        
        while time.time() - start_time < config.duration_seconds:
            # éš¨æ©Ÿæ±ºå®šæ˜¯å¦ä½¿ç”¨éŒ¯èª¤æŸ¥è©¢
            if random.random() < 0.3:  # 30%æ©Ÿç‡ä½¿ç”¨éŒ¯èª¤æŸ¥è©¢
                query = random.choice(error_queries)
            else:
                query = random.choice(self.test_queries)
            
            # å˜—è©¦åŸ·è¡Œè«‹æ±‚ï¼ŒåŒ…å«é‡è©¦æ©Ÿåˆ¶
            for attempt in range(config.retry_attempts + 1):
                try:
                    if isinstance(query, str) or query is None:
                        result = self._execute_single_request(analyzer, query, config.timeout_seconds)
                    else:
                        # æ•…æ„è§¸ç™¼éŒ¯èª¤
                        raise ValueError(f"ç„¡æ•ˆçš„æŸ¥è©¢é¡å‹: {type(query)}")
                    
                    requests_data.append({
                        "query": str(query)[:100] if query else "None",
                        "submit_time": time.time(),
                        "result": result,
                        "attempt": attempt + 1
                    })
                    
                    if result["success"]:
                        response_times.append(result["response_time"])
                    else:
                        errors.append(result["error"])
                    
                    break  # æˆåŠŸåŸ·è¡Œï¼Œè·³å‡ºé‡è©¦å¾ªç’°
                    
                except Exception as e:
                    error_msg = str(e)
                    
                    if attempt < config.retry_attempts:
                        # è¨˜éŒ„é‡è©¦å˜—è©¦
                        recovery_attempts.append({
                            "query": str(query)[:100] if query else "None",
                            "attempt": attempt + 1,
                            "error": error_msg,
                            "time": time.time()
                        })
                        
                        # ç­‰å¾…å¾Œé‡è©¦
                        time.sleep(0.5 * (attempt + 1))  # æŒ‡æ•¸é€€é¿
                    else:
                        # æœ€çµ‚å¤±æ•—
                        errors.append(error_msg)
                        timeouts.append(error_msg)
            
            time.sleep(1.0 / config.requests_per_second)
        
        return {
            "requests": requests_data,
            "errors": errors,
            "timeouts": timeouts,
            "response_times": response_times,
            "recovery_attempts": recovery_attempts
        }
    
    def _run_scalability_test(self, provider: LLMProvider, config: StressTestConfig) -> Dict[str, Any]:
        """é‹è¡Œæ“´å±•æ€§æ¸¬è©¦"""
        analyzer = LLMIntentAnalyzer(provider=provider)
        
        requests_data = []
        errors = []
        timeouts = []
        response_times = []
        scalability_metrics = []
        
        # é€æ¼¸å¢åŠ ä¸¦ç™¼æ•¸
        concurrent_levels = [1, 2, 4, 8, 16, 32, min(64, config.max_concurrent_requests)]
        test_duration_per_level = config.duration_seconds // len(concurrent_levels)
        
        for concurrent_level in concurrent_levels:
            if concurrent_level > config.max_concurrent_requests:
                break
            
            print(f"   æ¸¬è©¦ä¸¦ç™¼ç´šåˆ¥: {concurrent_level}")
            
            level_start_time = time.time()
            level_requests = []
            level_errors = []
            level_response_times = []
            
            # åœ¨ç•¶å‰ä¸¦ç™¼ç´šåˆ¥ä¸‹æ¸¬è©¦
            with ThreadPoolExecutor(max_workers=concurrent_level) as executor:
                futures = []
                
                while time.time() - level_start_time < test_duration_per_level:
                    # æäº¤ä¸¦ç™¼è«‹æ±‚
                    for _ in range(concurrent_level):
                        query = random.choice(self.test_queries)
                        future = executor.submit(self._execute_single_request, analyzer, query, config.timeout_seconds)
                        futures.append((future, time.time(), query))
                    
                    time.sleep(1.0)  # æ¯ç§’ä¸€æ‰¹
                
                # æ”¶é›†ç•¶å‰ç´šåˆ¥çš„çµæœ
                for future, submit_time, query in futures:
                    try:
                        result = future.result(timeout=config.timeout_seconds + 5)
                        
                        level_requests.append({
                            "query": query,
                            "submit_time": submit_time,
                            "result": result,
                            "concurrent_level": concurrent_level
                        })
                        
                        if result["success"]:
                            level_response_times.append(result["response_time"])
                        else:
                            level_errors.append(result["error"])
                            
                    except Exception as e:
                        level_errors.append(str(e))
                        timeouts.append(str(e))
            
            # è¨ˆç®—ç•¶å‰ç´šåˆ¥çš„æŒ‡æ¨™
            level_duration = time.time() - level_start_time
            level_throughput = len(level_response_times) / level_duration if level_duration > 0 else 0
            level_avg_response_time = statistics.mean(level_response_times) if level_response_times else 0
            level_error_rate = len(level_errors) / len(level_requests) if level_requests else 0
            
            scalability_metrics.append({
                "concurrent_level": concurrent_level,
                "throughput": level_throughput,
                "avg_response_time": level_avg_response_time,
                "error_rate": level_error_rate,
                "total_requests": len(level_requests),
                "successful_requests": len(level_response_times)
            })
            
            # ç´¯ç©åˆ°ç¸½çµæœ
            requests_data.extend(level_requests)
            errors.extend(level_errors)
            response_times.extend(level_response_times)
        
        return {
            "requests": requests_data,
            "errors": errors,
            "timeouts": timeouts,
            "response_times": response_times,
            "scalability_metrics": scalability_metrics
        }
    
    def _execute_single_request(self, analyzer: LLMIntentAnalyzer, query: str, timeout: float) -> Dict[str, Any]:
        """åŸ·è¡Œå–®å€‹è«‹æ±‚"""
        start_time = time.time()
        
        try:
            with self.request_lock:
                self.active_requests += 1
            
            # è™•ç†ç‰¹æ®Šæƒ…æ³
            if query is None:
                raise ValueError("æŸ¥è©¢ä¸èƒ½ç‚ºNone")
            
            if not isinstance(query, str):
                raise TypeError(f"æŸ¥è©¢å¿…é ˆæ˜¯å­—ç¬¦ä¸²ï¼Œå¾—åˆ°: {type(query)}")
            
            # åŸ·è¡Œåˆ†æ
            result = analyzer.analyze_intent(query)
            response_time = time.time() - start_time
            
            # æª¢æŸ¥è¶…æ™‚
            if response_time > timeout:
                raise TimeoutError(f"è«‹æ±‚è¶…æ™‚: {response_time:.2f}s > {timeout}s")
            
            return {
                "success": True,
                "response_time": response_time,
                "result": {
                    "is_job_related": result.is_job_related,
                    "intent_type": result.intent_type.value if hasattr(result.intent_type, 'value') else str(result.intent_type),
                    "confidence": result.confidence
                },
                "error": None
            }
            
        except Exception as e:
            response_time = time.time() - start_time
            return {
                "success": False,
                "response_time": response_time,
                "result": None,
                "error": str(e)
            }
        
        finally:
            with self.request_lock:
                self.active_requests -= 1
    
    def _analyze_stress_test_results(self, test_id: str, provider: LLMProvider, 
                                   config: StressTestConfig, start_time: str, 
                                   end_time: str, total_duration: float, 
                                   test_results: Dict[str, Any]) -> StressTestResult:
        """åˆ†æå£“åŠ›æ¸¬è©¦çµæœ"""
        requests = test_results.get("requests", [])
        errors = test_results.get("errors", [])
        timeouts = test_results.get("timeouts", [])
        response_times = test_results.get("response_times", [])
        
        # åŸºæœ¬çµ±è¨ˆ
        total_requests = len(requests)
        successful_requests = len([r for r in requests if r["result"]["success"]])
        failed_requests = total_requests - successful_requests
        timeout_requests = len(timeouts)
        error_requests = len(errors)
        
        success_rate = successful_requests / total_requests if total_requests > 0 else 0.0
        
        # éŸ¿æ‡‰æ™‚é–“çµ±è¨ˆ
        if response_times:
            avg_response_time = statistics.mean(response_times)
            median_response_time = statistics.median(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            p95_response_time = np.percentile(response_times, 95)
            p99_response_time = np.percentile(response_times, 99)
        else:
            avg_response_time = median_response_time = min_response_time = max_response_time = 0.0
            p95_response_time = p99_response_time = 0.0
        
        # ååé‡
        throughput = successful_requests / total_duration if total_duration > 0 else 0.0
        
        # ç³»çµ±è³‡æºçµ±è¨ˆ
        peak_memory = self.system_monitor.get_peak_memory()
        avg_memory = self.system_monitor.get_avg_memory()
        cpu_stats = self.system_monitor.get_cpu_stats()
        
        # éŒ¯èª¤åˆ†ä½ˆ
        error_distribution = {}
        for error in errors:
            error_type = self._classify_error(error)
            error_distribution[error_type] = error_distribution.get(error_type, 0) + 1
        
        # å»¶é²ç™¾åˆ†ä½æ•¸
        latency_percentiles = {}
        if response_times:
            for p in [50, 75, 90, 95, 99, 99.9]:
                latency_percentiles[f"p{p}"] = np.percentile(response_times, p)
        
        # ç©©å®šæ€§åˆ†æ•¸ï¼ˆåŸºæ–¼éŸ¿æ‡‰æ™‚é–“è®Šç•°æ€§ï¼‰
        if len(response_times) > 1:
            cv = statistics.stdev(response_times) / avg_response_time if avg_response_time > 0 else 0
            stability_score = max(0.0, 1.0 - cv)  # è®Šç•°ä¿‚æ•¸è¶Šå°ï¼Œç©©å®šæ€§è¶Šé«˜
        else:
            stability_score = 1.0 if response_times else 0.0
        
        # æ€§èƒ½é€€åŒ–ï¼ˆæ¯”è¼ƒå‰å¾ŒæœŸé–“çš„éŸ¿æ‡‰æ™‚é–“ï¼‰
        performance_degradation = 0.0
        if len(response_times) > 10:
            early_times = response_times[:len(response_times)//3]
            late_times = response_times[-len(response_times)//3:]
            
            if early_times and late_times:
                early_avg = statistics.mean(early_times)
                late_avg = statistics.mean(late_times)
                performance_degradation = (late_avg - early_avg) / early_avg if early_avg > 0 else 0.0
        
        # è³‡æºæ•ˆç‡ï¼ˆæˆåŠŸè«‹æ±‚æ•¸ / å¹³å‡è¨˜æ†¶é«”ä½¿ç”¨é‡ï¼‰
        resource_efficiency = successful_requests / avg_memory if avg_memory > 0 else 0.0
        
        return StressTestResult(
            test_id=test_id,
            provider=provider.value,
            config=config,
            start_time=start_time,
            end_time=end_time,
            total_duration=total_duration,
            total_requests=total_requests,
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            timeout_requests=timeout_requests,
            error_requests=error_requests,
            success_rate=success_rate,
            avg_response_time=avg_response_time,
            median_response_time=median_response_time,
            p95_response_time=p95_response_time,
            p99_response_time=p99_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            throughput=throughput,
            peak_memory_usage=peak_memory,
            avg_memory_usage=avg_memory,
            cpu_usage_stats=cpu_stats,
            error_distribution=error_distribution,
            response_time_distribution=response_times,
            latency_percentiles=latency_percentiles,
            stability_score=stability_score,
            performance_degradation=performance_degradation,
            resource_efficiency=resource_efficiency,
            metadata={
                "test_type": config.test_type.value,
                "load_pattern": config.load_pattern.value,
                "max_concurrent": config.max_concurrent_requests,
                "target_rps": config.requests_per_second,
                "additional_data": test_results
            }
        )
    
    def _classify_error(self, error_msg: str) -> str:
        """åˆ†é¡éŒ¯èª¤é¡å‹"""
        error_msg_lower = error_msg.lower()
        
        if "timeout" in error_msg_lower:
            return "timeout"
        elif "rate limit" in error_msg_lower or "too many requests" in error_msg_lower:
            return "rate_limit"
        elif "connection" in error_msg_lower:
            return "connection"
        elif "authentication" in error_msg_lower or "unauthorized" in error_msg_lower:
            return "auth"
        elif "quota" in error_msg_lower or "limit" in error_msg_lower:
            return "quota"
        elif "server error" in error_msg_lower or "500" in error_msg_lower:
            return "server_error"
        elif "bad request" in error_msg_lower or "400" in error_msg_lower:
            return "client_error"
        else:
            return "other"
    
    def save_stress_test_results(self, filename_prefix: str = "stress_test") -> str:
        """ä¿å­˜å£“åŠ›æ¸¬è©¦çµæœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{filename_prefix}_results_{timestamp}.json"
        
        # æº–å‚™ä¿å­˜çš„æ•¸æ“š
        save_data = {
            "test_summary": {
                "timestamp": datetime.now().isoformat(),
                "total_tests": len(self.results),
                "test_types": list(set(r.config.test_type.value for r in self.results)),
                "providers": list(set(r.provider for r in self.results))
            },
            "results": [asdict(result) for result in self.results]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ’¾ å£“åŠ›æ¸¬è©¦çµæœå·²ä¿å­˜: {filename}")
        return filename
    
    def print_stress_test_summary(self) -> None:
        """æ‰“å°å£“åŠ›æ¸¬è©¦æ‘˜è¦"""
        if not self.results:
            print("âŒ æ²’æœ‰å£“åŠ›æ¸¬è©¦çµæœå¯é¡¯ç¤º")
            return
        
        print("\nğŸ“Š LLMå£“åŠ›æ¸¬è©¦æ‘˜è¦")
        print("=" * 60)
        
        for result in self.results:
            print(f"\nğŸ§ª æ¸¬è©¦: {result.test_id}")
            print(f"   æä¾›å•†: {result.provider}")
            print(f"   æ¸¬è©¦é¡å‹: {result.config.test_type.value}")
            print(f"   æŒçºŒæ™‚é–“: {result.total_duration:.1f}ç§’")
            print(f"   ç¸½è«‹æ±‚æ•¸: {result.total_requests}")
            print(f"   æˆåŠŸç‡: {result.success_rate:.1%}")
            print(f"   å¹³å‡éŸ¿æ‡‰æ™‚é–“: {result.avg_response_time:.2f}ç§’")
            print(f"   P95éŸ¿æ‡‰æ™‚é–“: {result.p95_response_time:.2f}ç§’")
            print(f"   ååé‡: {result.throughput:.1f} è«‹æ±‚/ç§’")
            print(f"   å³°å€¼è¨˜æ†¶é«”: {result.peak_memory_usage:.1f}MB")
            print(f"   ç©©å®šæ€§åˆ†æ•¸: {result.stability_score:.2f}")
            
            if result.error_distribution:
                print(f"   éŒ¯èª¤åˆ†ä½ˆ: {result.error_distribution}")


def main():
    """ä¸»å‡½æ•¸ - å£“åŠ›æ¸¬è©¦å·¥å…·å…¥å£é»"""
    print("ğŸš€ LLMå£“åŠ›æ¸¬è©¦å·¥å…·")
    print("=" * 60)
    
    # å‰µå»ºå£“åŠ›æ¸¬è©¦å·¥å…·
    stress_tester = LLMStressTestTool()
    
    # é¸æ“‡æ¸¬è©¦é¡å‹
    print("\nè«‹é¸æ“‡å£“åŠ›æ¸¬è©¦é¡å‹:")
    test_types = list(StressTestType)
    for i, test_type in enumerate(test_types, 1):
        print(f"{i}. {test_type.value}")
    
    try:
        choice = input("\nè«‹è¼¸å…¥é¸æ“‡ (1-8): ").strip()
        test_type_index = int(choice) - 1
        
        if 0 <= test_type_index < len(test_types):
            selected_test_type = test_types[test_type_index]
        else:
            print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œä½¿ç”¨ä¸¦ç™¼è² è¼‰æ¸¬è©¦")
            selected_test_type = StressTestType.CONCURRENT_LOAD
        
        # é¸æ“‡æä¾›å•†
        config_manager = LLMConfigManager()
        available_providers = config_manager.get_available_providers()
        
        if not available_providers:
            print("âŒ æ²’æœ‰å¯ç”¨çš„LLMæä¾›å•†")
            return
        
        print("\nå¯ç”¨çš„LLMæä¾›å•†:")
        for i, provider in enumerate(available_providers, 1):
            print(f"{i}. {provider.value}")
        
        provider_choice = input("\nè«‹é¸æ“‡æä¾›å•† (1-{}): ".format(len(available_providers))).strip()
        provider_index = int(provider_choice) - 1
        
        if 0 <= provider_index < len(available_providers):
            selected_provider = available_providers[provider_index]
        else:
            print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œä½¿ç”¨ç¬¬ä¸€å€‹å¯ç”¨æä¾›å•†")
            selected_provider = available_providers[0]
        
        # å‰µå»ºæ¸¬è©¦é…ç½®
        config = StressTestConfig(
            test_type=selected_test_type,
            load_pattern=LoadPattern.CONSTANT,
            duration_seconds=60,  # 1åˆ†é˜æ¸¬è©¦
            max_concurrent_requests=10,
            requests_per_second=2.0,
            timeout_seconds=30.0
        )
        
        # é‹è¡Œå£“åŠ›æ¸¬è©¦
        result = stress_tester.run_stress_test(selected_provider, config)
        
        # é¡¯ç¤ºæ‘˜è¦
        stress_tester.print_stress_test_summary()
        
        # ä¿å­˜çµæœ
        stress_tester.save_stress_test_results()
        
        print("\nâœ… å£“åŠ›æ¸¬è©¦å®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâŒ æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()