#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
104äººåŠ›éŠ€è¡Œå¢å¼·ç‰ˆçˆ¬èŸ²
ä½¿ç”¨æ›¿ä»£æœå°‹ç­–ç•¥ï¼Œç›´æ¥è¨ªå•æœå°‹URLé¿å…å½ˆå‡ºè¦–çª—å•é¡Œ
"""

import asyncio
import json
import urllib.parse
import time
import random
from datetime import datetime
from typing import List, Dict, Optional, Any
from playwright.async_api import async_playwright, Page, Browser, BrowserContext

from .constant import BASE_URL, SEARCH_URL, USER_AGENTS, ANT_DETECTION_CONFIG


class EnhancedTW104Scraper:
    """104äººåŠ›éŠ€è¡Œå¢å¼·ç‰ˆçˆ¬èŸ²"""
    
    def __init__(self, headless: bool = True, timeout: int = 30000):
        self.headless = headless
        self.timeout = timeout
        self.base_url = BASE_URL
        self.search_base_url = SEARCH_URL
        self.user_agents = USER_AGENTS
        self.anti_detection_config = ANT_DETECTION_CONFIG
        
        # æœå°‹çµæœå­˜å„²
        self.job_results = []
        self.search_metadata = {}
        
    async def search_jobs(
        self, 
        keyword: str, 
        location: Optional[str] = None,
        job_category: Optional[str] = None,
        max_pages: int = 5,
        delay_range: tuple = (1, 3)
    ) -> List[Dict[str, Any]]:
        """
        æœå°‹è·ä½
        
        Args:
            keyword: æœå°‹é—œéµå­—
            location: åœ°å€ä»£ç¢¼ (å¯é¸)
            job_category: è·å‹™é¡åˆ¥ä»£ç¢¼ (å¯é¸)
            max_pages: æœ€å¤§æœå°‹é æ•¸
            delay_range: è«‹æ±‚é–“éš”ç¯„åœ (ç§’)
            
        Returns:
            è·ä½åˆ—è¡¨
        """
        print(f"ğŸ” é–‹å§‹æœå°‹104äººåŠ›éŠ€è¡Œè·ä½: {keyword}")
        
        try:
            async with async_playwright() as p:
                browser = await self._launch_browser(p)
                context = await self._create_context(browser)
                page = await context.new_page()
                
                # æœå°‹æ‰€æœ‰é é¢
                all_jobs = []
                for page_num in range(1, max_pages + 1):
                    print(f"ğŸ“„ æœå°‹ç¬¬ {page_num} é ...")
                    
                    # æ§‹å»ºæœå°‹URL
                    search_url = self._build_search_url(
                        keyword, location, job_category, page_num
                    )
                    
                    # æœå°‹ç•¶å‰é é¢
                    page_jobs = await self._search_single_page(page, search_url)
                    
                    if not page_jobs:
                        print(f"âš ï¸  ç¬¬ {page_num} é æ²’æœ‰æ‰¾åˆ°è·ä½ï¼Œåœæ­¢æœå°‹")
                        break
                    
                    all_jobs.extend(page_jobs)
                    print(f"âœ… ç¬¬ {page_num} é æ‰¾åˆ° {len(page_jobs)} å€‹è·ä½")
                    
                    # éš¨æ©Ÿå»¶é²
                    if page_num < max_pages:
                        delay = random.uniform(*delay_range)
                        print(f"â³ ç­‰å¾… {delay:.1f} ç§’...")
                        await asyncio.sleep(delay)
                
                await browser.close()
                
                # ä¿å­˜çµæœ
                self.job_results = all_jobs
                self.search_metadata = {
                    "keyword": keyword,
                    "location": location,
                    "job_category": job_category,
                    "total_jobs": len(all_jobs),
                    "search_time": datetime.now().isoformat()
                }
                
                print(f"ğŸ‰ æœå°‹å®Œæˆï¼ç¸½å…±æ‰¾åˆ° {len(all_jobs)} å€‹è·ä½")
                return all_jobs
                
        except Exception as e:
            print(f"âŒ æœå°‹å¤±æ•—: {e}")
            return []
    
    async def _launch_browser(self, playwright) -> Browser:
        """å•Ÿå‹•ç€è¦½å™¨"""
        return await playwright.chromium.launch(
            headless=self.headless,
            args=[
                '--no-sandbox',
                '--disable-blink-features=AutomationControlled',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--disable-popup-blocking',
                '--disable-notifications',
                '--disable-extensions',
                '--disable-dev-shm-usage',
                '--no-first-run',
                '--no-default-browser-check'
            ]
        )
    
    async def _create_context(self, browser: Browser) -> BrowserContext:
        """å‰µå»ºç€è¦½å™¨ä¸Šä¸‹æ–‡"""
        user_agent = random.choice(self.user_agents)
        
        return await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent=user_agent,
            locale='zh-TW',
            timezone_id='Asia/Taipei',
            extra_http_headers={
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
        )
    
    def _build_search_url(
        self, 
        keyword: str, 
        location: Optional[str] = None,
        job_category: Optional[str] = None,
        page: int = 1
    ) -> str:
        """æ§‹å»ºæœå°‹URL"""
        params = {
            'keyword': keyword,
            'order': '1',  # ç›¸é—œæ€§æ’åº
            'asc': '0',    # é™åº
            'page': str(page)
        }
        
        if location:
            params['area'] = location
        
        if job_category:
            params['jobcat'] = job_category
        
        query_string = '&'.join([f"{k}={urllib.parse.quote(str(v))}" for k, v in params.items()])
        return f"{self.search_base_url}?{query_string}"
    
    async def _search_single_page(self, page: Page, search_url: str) -> List[Dict[str, Any]]:
        """æœå°‹å–®ä¸€é é¢"""
        try:
            # è¨ªå•æœå°‹é é¢
            await page.goto(search_url, timeout=self.timeout)
            await page.wait_for_load_state('networkidle')
            
            # ç­‰å¾…è·ä½åˆ—è¡¨è¼‰å…¥
            await page.wait_for_selector('.job-list', timeout=10000)
            
            # æå–è·ä½è³‡è¨Š
            jobs = await self._extract_job_info(page)
            
            return jobs
            
        except Exception as e:
            print(f"âŒ æœå°‹é é¢å¤±æ•—: {e}")
            return []
    
    async def _extract_job_info(self, page: Page) -> List[Dict[str, Any]]:
        """æå–è·ä½è³‡è¨Š"""
        try:
            # ç­‰å¾…è·ä½å…ƒç´ è¼‰å…¥
            await page.wait_for_selector('.job', timeout=5000)
            
            # æå–è·ä½è³‡è¨Š
            jobs = await page.evaluate("""
                () => {
                    const jobElements = document.querySelectorAll('.job');
                    const jobs = [];
                    
                    jobElements.forEach((element, index) => {
                        try {
                            // ç²å–å®Œæ•´æ–‡æœ¬å…§å®¹
                            const fullText = element.textContent || '';
                            
                            // è·ä½æ¨™é¡Œ - å¾å®Œæ•´æ–‡æœ¬ä¸­æå–
                            let title = '';
                            let jobUrl = '';
                            
                            // å˜—è©¦å¾éˆæ¥ä¸­ç²å–æ¨™é¡Œ
                            const titleLink = element.querySelector('a[href*="/job/"]');
                            if (titleLink) {
                                title = titleLink.textContent.trim();
                                jobUrl = titleLink.href;
                            } else {
                                // å¦‚æœæ²’æœ‰éˆæ¥ï¼Œå˜—è©¦å¾æ–‡æœ¬ä¸­æå–æ¨™é¡Œ
                                const lines = fullText.split('\\n').map(line => line.trim()).filter(line => line);
                                if (lines.length > 0) {
                                    title = lines[0];
                                }
                            }
                            
                            // å…¬å¸åç¨± - å¾æ–‡æœ¬ä¸­æå–
                            let company = '';
                            let companyUrl = '';
                            
                            // å˜—è©¦å¾éˆæ¥ä¸­ç²å–å…¬å¸åç¨±
                            const companyLink = element.querySelector('a[href*="/company/"]');
                            if (companyLink) {
                                company = companyLink.textContent.trim();
                                companyUrl = companyLink.href;
                            } else {
                                // å¾æ–‡æœ¬ä¸­æå–å…¬å¸åç¨±ï¼ˆé€šå¸¸åœ¨ç¬¬äºŒè¡Œï¼‰
                                const lines = fullText.split('\\n').map(line => line.trim()).filter(line => line);
                                if (lines.length > 1) {
                                    company = lines[1];
                                }
                            }
                            
                            // å·¥ä½œåœ°é» - å¾æ–‡æœ¬ä¸­æå–
                            let location = '';
                            const locationMatch = fullText.match(/å°åŒ—å¸‚|æ–°åŒ—å¸‚|æ¡ƒåœ’å¸‚|å°ä¸­å¸‚|å°å—å¸‚|é«˜é›„å¸‚|åŸºéš†å¸‚|æ–°ç«¹å¸‚|å˜‰ç¾©å¸‚|æ–°ç«¹ç¸£|è‹—æ —ç¸£|å½°åŒ–ç¸£|å—æŠ•ç¸£|é›²æ—ç¸£|å˜‰ç¾©ç¸£|å±æ±ç¸£|å®œè˜­ç¸£|èŠ±è“®ç¸£|å°æ±ç¸£|æ¾æ¹–ç¸£|é‡‘é–€ç¸£|é€£æ±Ÿç¸£/);
                            if (locationMatch) {
                                location = locationMatch[0];
                            }
                            
                            // è–ªè³‡ - å¾æ–‡æœ¬ä¸­æå–
                            let salary = '';
                            const salaryMatch = fullText.match(/\\$[0-9,]+|é¢è­°|å¾…é‡é¢è­°|è–ªè³‡é¢è­°|æœˆè–ª[0-9,]+|å¹´è–ª[0-9,]+/);
                            if (salaryMatch) {
                                salary = salaryMatch[0];
                            }
                            
                            // å·¥ä½œç¶“é©— - å¾æ–‡æœ¬ä¸­æå–
                            let experience = '';
                            const expMatch = fullText.match(/[0-9]+å¹´ä»¥ä¸Š|[0-9]+å¹´ä»¥ä¸‹|ç„¡ç¶“é©—|1å¹´ä»¥ä¸‹|2å¹´ä»¥ä¸‹|3å¹´ä»¥ä¸‹|4å¹´ä»¥ä¸‹|5å¹´ä»¥ä¸‹|6å¹´ä»¥ä¸‹|7å¹´ä»¥ä¸‹|8å¹´ä»¥ä¸‹|9å¹´ä»¥ä¸‹|10å¹´ä»¥ä¸‹/);
                            if (expMatch) {
                                experience = expMatch[0];
                            }
                            
                            // å­¸æ­·è¦æ±‚ - å¾æ–‡æœ¬ä¸­æå–
                            let education = '';
                            const eduMatch = fullText.match(/å¤§å­¸|ç¢©å£«|åšå£«|å°ˆç§‘|é«˜ä¸­|åœ‹ä¸­|ä¸æ‹˜|ä¸é™/);
                            if (eduMatch) {
                                education = eduMatch[0];
                            }
                            
                            // ç™¼å¸ƒæ™‚é–“ - å¾æ–‡æœ¬ä¸­æå–
                            let publishTime = '';
                            const timeMatch = fullText.match(/[0-9]+å¤©å‰|[0-9]+å°æ™‚å‰|[0-9]+åˆ†é˜å‰|ä»Šå¤©|æ˜¨å¤©|æœ¬é€±|ä¸Šé€±/);
                            if (timeMatch) {
                                publishTime = timeMatch[0];
                            }
                            
                            // è·ä½ID
                            const jobId = jobUrl ? jobUrl.split('/').pop() : '';
                            
                            // è·ä½æè¿° - å–å®Œæ•´æ–‡æœ¬çš„å‰200å€‹å­—ç¬¦
                            const description = fullText.length > 200 ? fullText.substring(0, 200) + '...' : fullText;
                            
                            if (title && company) {
                                jobs.push({
                                    job_id: jobId,
                                    title: title,
                                    company: company,
                                    location: location,
                                    salary: salary,
                                    experience: experience,
                                    education: education,
                                    description: description,
                                    publish_time: publishTime,
                                    job_url: jobUrl,
                                    company_url: companyUrl,
                                    scraped_at: new Date().toISOString()
                                });
                            }
                        } catch (error) {
                            console.error('Error extracting job info:', error);
                        }
                    });
                    
                    return jobs;
                }
            """)
            
            return jobs
            
        except Exception as e:
            print(f"âŒ æå–è·ä½è³‡è¨Šå¤±æ•—: {e}")
            return []
    
    async def get_job_details(self, job_url: str) -> Dict[str, Any]:
        """ç²å–è·ä½è©³ç´°è³‡è¨Š"""
        try:
            async with async_playwright() as p:
                browser = await self._launch_browser(p)
                context = await self._create_context(browser)
                page = await context.new_page()
                
                await page.goto(job_url, timeout=self.timeout)
                await page.wait_for_load_state('networkidle')
                
                # æå–è©³ç´°è³‡è¨Š
                details = await page.evaluate("""
                    () => {
                        const details = {};
                        
                        // è·ä½æè¿°
                        const descriptionElement = document.querySelector('.job-description, .job-content, .content');
                        details.description = descriptionElement ? descriptionElement.textContent.trim() : '';
                        
                        // å·¥ä½œå…§å®¹
                        const contentElement = document.querySelector('.job-content, .work-content, .content');
                        details.work_content = contentElement ? contentElement.textContent.trim() : '';
                        
                        // è·ä½è¦æ±‚
                        const requirementsElement = document.querySelector('.job-requirements, .requirements, .qualifications');
                        details.requirements = requirementsElement ? requirementsElement.textContent.trim() : '';
                        
                        // å…¬å¸è³‡è¨Š
                        const companyInfoElement = document.querySelector('.company-info, .company-details');
                        details.company_info = companyInfoElement ? companyInfoElement.textContent.trim() : '';
                        
                        // ç¦åˆ©å¾…é‡
                        const benefitsElement = document.querySelector('.job-benefits, .benefits, .welfare');
                        details.benefits = benefitsElement ? benefitsElement.textContent.trim() : '';
                        
                        return details;
                    }
                """)
                
                await browser.close()
                return details
                
        except Exception as e:
            print(f"âŒ ç²å–è·ä½è©³ç´°è³‡è¨Šå¤±æ•—: {e}")
            return {}
    
    def save_results(self, filename: Optional[str] = None) -> str:
        """ä¿å­˜æœå°‹çµæœ"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"tw104_jobs_{timestamp}.json"
        
        filepath = f"tests/results/{filename}"
        
        data = {
            "metadata": self.search_metadata,
            "jobs": self.job_results,
            "total_count": len(self.job_results),
            "saved_at": datetime.now().isoformat()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æœå°‹çµæœå·²ä¿å­˜è‡³: {filepath}")
        return filepath
    
    def get_statistics(self) -> Dict[str, Any]:
        """ç²å–æœå°‹çµ±è¨ˆè³‡è¨Š"""
        if not self.job_results:
            return {"error": "æ²’æœ‰æœå°‹çµæœ"}
        
        # çµ±è¨ˆå…¬å¸
        companies = [job.get('company', '') for job in self.job_results if job.get('company')]
        company_count = len(set(companies))
        
        # çµ±è¨ˆåœ°å€
        locations = [job.get('location', '') for job in self.job_results if job.get('location')]
        location_count = len(set(locations))
        
        # çµ±è¨ˆè–ªè³‡ç¯„åœ
        salaries = [job.get('salary', '') for job in self.job_results if job.get('salary')]
        salary_count = len([s for s in salaries if s])
        
        return {
            "total_jobs": len(self.job_results),
            "unique_companies": company_count,
            "unique_locations": location_count,
            "jobs_with_salary": salary_count,
            "search_metadata": self.search_metadata
        }


# ä½¿ç”¨ç¯„ä¾‹
async def main():
    """ä½¿ç”¨ç¯„ä¾‹"""
    scraper = EnhancedTW104Scraper(headless=True)
    
    # æœå°‹è·ä½
    jobs = await scraper.search_jobs(
        keyword="Pythonå·¥ç¨‹å¸«",
        location="6001001000",  # å°åŒ—å¸‚
        max_pages=3
    )
    
    # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
    stats = scraper.get_statistics()
    print(f"ğŸ“Š æœå°‹çµ±è¨ˆ: {stats}")
    
    # ä¿å­˜çµæœ
    scraper.save_results()


if __name__ == "__main__":
    asyncio.run(main())
