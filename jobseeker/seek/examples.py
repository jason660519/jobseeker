#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Seekçˆ¬èŸ²å¼•æ“ä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå„ç¨®ä½¿ç”¨å ´æ™¯å’Œæœ€ä½³å¯¦è¸

Author: JobSpy Team
Date: 2024
"""

import asyncio
import json
from pathlib import Path
from typing import List, Dict, Any

from .seek_scraper_enhanced import SeekScraperEnhanced
from .config import SeekCrawlerConfig, ConfigTemplates
from .models import JobPost


class SeekExamples:
    """
    Seekçˆ¬èŸ²å¼•æ“ä½¿ç”¨ç¤ºä¾‹é¡
    """
    
    def __init__(self):
        self.scraper: SeekScraperEnhanced = None
        self.config: SeekCrawlerConfig = None
    
    async def example_basic_search(self):
        """
        ç¤ºä¾‹1: åŸºæœ¬æœç´¢åŠŸèƒ½
        æ¼”ç¤ºæœ€ç°¡å–®çš„è·ä½æœç´¢
        """
        print("\n=== ç¤ºä¾‹1: åŸºæœ¬æœç´¢ ===")
        
        # å‰µå»ºåŸºæœ¬é…ç½®
        config = ConfigTemplates.development()
        
        # åˆå§‹åŒ–çˆ¬èŸ²
        scraper = SeekScraperEnhanced(
            scraping_mode='traditional',
            headless=True,
            enable_ocr=False,
            storage_path=config.storage.base_path
        )
        
        try:
            # åˆå§‹åŒ–
            if await scraper.initialize():
                print("âœ… çˆ¬èŸ²åˆå§‹åŒ–æˆåŠŸ")
                
                # æœç´¢Pythoné–‹ç™¼è·ä½
                jobs = await scraper.scrape_jobs(
                    search_term="python developer",
                    location="Sydney",
                    max_pages=2,
                    results_wanted=20
                )
                
                print(f"ğŸ¯ æ‰¾åˆ° {len(jobs)} å€‹è·ä½")
                
                # é¡¯ç¤ºå‰3å€‹è·ä½
                for i, job in enumerate(jobs[:3], 1):
                    print(f"\nè·ä½ {i}:")
                    print(f"  æ¨™é¡Œ: {job.title}")
                    print(f"  å…¬å¸: {job.company}")
                    print(f"  åœ°é»: {job.location}")
                    print(f"  è–ªè³‡: {job.salary or 'æœªæä¾›'}")
                
                # å°å‡ºçµæœ
                output_file = await scraper.export_results(jobs, 'json', 'basic_search_results.json')
                print(f"\nğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {output_file}")
            
        finally:
            await scraper.cleanup()
    
    async def example_enhanced_search_with_ocr(self):
        """
        ç¤ºä¾‹2: å¢å¼·æœç´¢èˆ‡OCRåŠŸèƒ½
        æ¼”ç¤ºä½¿ç”¨OCRè™•ç†åœ–ç‰‡å…§å®¹
        """
        print("\n=== ç¤ºä¾‹2: å¢å¼·æœç´¢èˆ‡OCR ===")
        
        # å‰µå»ºç”Ÿç”¢ç’°å¢ƒé…ç½®
        config = ConfigTemplates.production()
        config.ocr.enabled = True
        config.scraping.enable_screenshots = True
        
        scraper = SeekScraperEnhanced(
            scraping_mode='enhanced',
            headless=True,
            enable_ocr=True,
            enable_screenshots=True,
            storage_path=config.storage.base_path
        )
        
        try:
            if await scraper.initialize():
                print("âœ… å¢å¼·çˆ¬èŸ²åˆå§‹åŒ–æˆåŠŸ")
                
                # æœç´¢æ•¸æ“šç§‘å­¸è·ä½
                jobs = await scraper.scrape_jobs(
                    search_term="data scientist",
                    location="Melbourne",
                    job_type="full-time",
                    max_pages=3,
                    results_wanted=30
                )
                
                print(f"ğŸ¯ æ‰¾åˆ° {len(jobs)} å€‹è·ä½")
                
                # åˆ†æè–ªè³‡åˆ†å¸ƒ
                salary_jobs = [job for job in jobs if job.salary]
                print(f"ğŸ’° æœ‰è–ªè³‡ä¿¡æ¯çš„è·ä½: {len(salary_jobs)} å€‹")
                
                # å°å‡ºå¤šç¨®æ ¼å¼
                json_file = await scraper.export_results(jobs, 'json', 'enhanced_search_results.json')
                csv_file = await scraper.export_results(jobs, 'csv', 'enhanced_search_results.csv')
                
                print(f"\nğŸ’¾ çµæœå·²ä¿å­˜:")
                print(f"  JSON: {json_file}")
                print(f"  CSV: {csv_file}")
                
                # é¡¯ç¤ºæ€§èƒ½å ±å‘Š
                performance = scraper.get_performance_report()
                print(f"\nğŸ“Š æ€§èƒ½å ±å‘Š:")
                print(f"  æˆåŠŸç‡: {performance.get('success_rate', 0)}%")
                print(f"  å¹³å‡éŸ¿æ‡‰æ™‚é–“: {performance.get('average_response_time', 0):.2f}ç§’")
        
        finally:
            await scraper.cleanup()
    
    async def example_hybrid_mode_batch_search(self):
        """
        ç¤ºä¾‹3: æ··åˆæ¨¡å¼æ‰¹é‡æœç´¢
        æ¼”ç¤ºæ‰¹é‡æœç´¢å¤šå€‹é—œéµè©
        """
        print("\n=== ç¤ºä¾‹3: æ··åˆæ¨¡å¼æ‰¹é‡æœç´¢ ===")
        
        # æœç´¢é—œéµè©åˆ—è¡¨
        search_terms = [
            "software engineer",
            "frontend developer",
            "backend developer",
            "devops engineer",
            "product manager"
        ]
        
        locations = ["Sydney", "Melbourne", "Brisbane"]
        
        config = ConfigTemplates.production()
        scraper = SeekScraperEnhanced(
            scraping_mode='hybrid',
            headless=True,
            enable_ocr=False,
            storage_path=config.storage.base_path,
            max_workers=5
        )
        
        all_jobs = []
        
        try:
            if await scraper.initialize():
                print("âœ… æ··åˆæ¨¡å¼çˆ¬èŸ²åˆå§‹åŒ–æˆåŠŸ")
                
                # æ‰¹é‡æœç´¢
                for term in search_terms:
                    for location in locations:
                        print(f"\nğŸ” æœç´¢: {term} @ {location}")
                        
                        jobs = await scraper.scrape_jobs(
                            search_term=term,
                            location=location,
                            max_pages=2,
                            results_wanted=15
                        )
                        
                        print(f"  æ‰¾åˆ° {len(jobs)} å€‹è·ä½")
                        all_jobs.extend(jobs)
                        
                        # æ·»åŠ æœç´¢æ¨™ç±¤
                        for job in jobs:
                            job.search_term = term
                            job.search_location = location
                
                print(f"\nğŸ¯ ç¸½è¨ˆæ‰¾åˆ° {len(all_jobs)} å€‹è·ä½")
                
                # å»é‡è™•ç†
                unique_jobs = self._deduplicate_jobs(all_jobs)
                print(f"ğŸ”„ å»é‡å¾Œ: {len(unique_jobs)} å€‹å”¯ä¸€è·ä½")
                
                # æŒ‰è·ä½é¡å‹åˆ†çµ„çµ±è¨ˆ
                job_stats = self._analyze_job_distribution(unique_jobs)
                print(f"\nğŸ“Š è·ä½åˆ†å¸ƒçµ±è¨ˆ:")
                for term, count in job_stats.items():
                    print(f"  {term}: {count} å€‹")
                
                # å°å‡ºçµæœ
                output_file = await scraper.export_results(
                    unique_jobs, 'json', 'batch_search_results.json'
                )
                print(f"\nğŸ’¾ æ‰¹é‡æœç´¢çµæœå·²ä¿å­˜åˆ°: {output_file}")
        
        finally:
            await scraper.cleanup()
    
    async def example_custom_configuration(self):
        """
        ç¤ºä¾‹4: è‡ªå®šç¾©é…ç½®
        æ¼”ç¤ºå¦‚ä½•å‰µå»ºå’Œä½¿ç”¨è‡ªå®šç¾©é…ç½®
        """
        print("\n=== ç¤ºä¾‹4: è‡ªå®šç¾©é…ç½® ===")
        
        # å‰µå»ºè‡ªå®šç¾©é…ç½®
        config = SeekCrawlerConfig()
        
        # è‡ªå®šç¾©ç€è¦½å™¨è¨­ç½®
        config.browser.headless = False  # é¡¯ç¤ºç€è¦½å™¨
        config.browser.window_size = (1920, 1080)
        config.browser.user_agent = "Custom Seek Scraper 1.0"
        
        # è‡ªå®šç¾©çˆ¬èŸ²è¨­ç½®
        config.scraping.mode = 'enhanced'
        config.scraping.max_pages = 5
        config.scraping.results_wanted = 50
        config.scraping.enable_screenshots = True
        config.scraping.rate_limit_delay = 3.0
        
        # è‡ªå®šç¾©OCRè¨­ç½®
        config.ocr.enabled = True
        config.ocr.language = 'eng+chi_sim'  # è‹±æ–‡+ç°¡é«”ä¸­æ–‡
        config.ocr.confidence_threshold = 0.8
        
        # è‡ªå®šç¾©å­˜å„²è¨­ç½®
        config.storage.base_path = "./custom_seek_data"
        config.storage.keep_raw_data = True
        config.storage.compress_data = True
        
        # è‡ªå®šç¾©æ€§èƒ½è¨­ç½®
        config.performance.max_concurrent_requests = 2
        config.performance.max_memory_usage_mb = 2048
        config.performance.enable_caching = True
        
        # ä¿å­˜é…ç½®
        config.save_to_file("custom_config.json")
        print("ğŸ’¾ è‡ªå®šç¾©é…ç½®å·²ä¿å­˜åˆ°: custom_config.json")
        
        # ä½¿ç”¨è‡ªå®šç¾©é…ç½®å‰µå»ºçˆ¬èŸ²
        scraper = SeekScraperEnhanced(
            scraping_mode=config.scraping.mode,
            headless=config.browser.headless,
            enable_ocr=config.ocr.enabled,
            enable_screenshots=config.scraping.enable_screenshots,
            storage_path=config.storage.base_path,
            rate_limit_delay=config.scraping.rate_limit_delay
        )
        
        try:
            if await scraper.initialize():
                print("âœ… è‡ªå®šç¾©é…ç½®çˆ¬èŸ²åˆå§‹åŒ–æˆåŠŸ")
                
                # åŸ·è¡Œæœç´¢
                jobs = await scraper.scrape_jobs(
                    search_term="machine learning engineer",
                    location="Sydney",
                    max_pages=config.scraping.max_pages,
                    results_wanted=config.scraping.results_wanted
                )
                
                print(f"ğŸ¯ æ‰¾åˆ° {len(jobs)} å€‹è·ä½")
                
                # é¡¯ç¤ºé…ç½®æ•ˆæœ
                print(f"\nâš™ï¸ é…ç½®æ•ˆæœ:")
                print(f"  ç€è¦½å™¨å¯è¦‹: {'æ˜¯' if not config.browser.headless else 'å¦'}")
                print(f"  OCRèªè¨€: {config.ocr.language}")
                print(f"  å­˜å„²è·¯å¾‘: {config.storage.base_path}")
                print(f"  ä¸¦ç™¼è«‹æ±‚: {config.performance.max_concurrent_requests}")
        
        finally:
            await scraper.cleanup()
    
    async def example_error_handling_and_retry(self):
        """
        ç¤ºä¾‹5: éŒ¯èª¤è™•ç†èˆ‡é‡è©¦æ©Ÿåˆ¶
        æ¼”ç¤ºå¦‚ä½•è™•ç†ç¶²çµ¡éŒ¯èª¤å’Œå¯¦ç¾é‡è©¦
        """
        print("\n=== ç¤ºä¾‹5: éŒ¯èª¤è™•ç†èˆ‡é‡è©¦ ===")
        
        config = ConfigTemplates.testing()
        config.scraping.max_retries = 3
        config.scraping.retry_delay = 5.0
        
        scraper = SeekScraperEnhanced(
            scraping_mode='traditional',
            headless=True,
            storage_path=config.storage.base_path
        )
        
        try:
            if await scraper.initialize():
                print("âœ… éŒ¯èª¤è™•ç†ç¤ºä¾‹çˆ¬èŸ²åˆå§‹åŒ–æˆåŠŸ")
                
                # æ¨¡æ“¬å¯èƒ½å¤±æ•—çš„æœç´¢
                search_terms = [
                    "valid search term",
                    "",  # ç©ºæœç´¢è©ï¼Œå¯èƒ½å°è‡´éŒ¯èª¤
                    "very specific rare job title that might not exist"
                ]
                
                successful_searches = 0
                failed_searches = 0
                
                for term in search_terms:
                    try:
                        print(f"\nğŸ” å˜—è©¦æœç´¢: '{term}'")
                        
                        if not term.strip():
                            print("âš ï¸ è·³éç©ºæœç´¢è©")
                            continue
                        
                        jobs = await scraper.scrape_jobs(
                            search_term=term,
                            location="Sydney",
                            max_pages=1,
                            results_wanted=10
                        )
                        
                        print(f"âœ… æˆåŠŸ: æ‰¾åˆ° {len(jobs)} å€‹è·ä½")
                        successful_searches += 1
                        
                    except Exception as e:
                        print(f"âŒ æœç´¢å¤±æ•—: {e}")
                        failed_searches += 1
                        
                        # è¨˜éŒ„éŒ¯èª¤ä½†ç¹¼çºŒåŸ·è¡Œ
                        continue
                
                print(f"\nğŸ“Š æœç´¢çµæœçµ±è¨ˆ:")
                print(f"  æˆåŠŸ: {successful_searches}")
                print(f"  å¤±æ•—: {failed_searches}")
                print(f"  æˆåŠŸç‡: {successful_searches/(successful_searches+failed_searches)*100:.1f}%")
        
        finally:
            await scraper.cleanup()
    
    def _deduplicate_jobs(self, jobs: List[JobPost]) -> List[JobPost]:
        """
        å»é™¤é‡è¤‡è·ä½
        
        Args:
            jobs: è·ä½åˆ—è¡¨
            
        Returns:
            List[JobPost]: å»é‡å¾Œçš„è·ä½åˆ—è¡¨
        """
        seen = set()
        unique_jobs = []
        
        for job in jobs:
            # ä½¿ç”¨æ¨™é¡Œ+å…¬å¸+åœ°é»ä½œç‚ºå”¯ä¸€æ¨™è­˜
            job_key = f"{job.title}|{job.company}|{job.location}"
            if job_key not in seen:
                seen.add(job_key)
                unique_jobs.append(job)
        
        return unique_jobs
    
    def _analyze_job_distribution(self, jobs: List[JobPost]) -> Dict[str, int]:
        """
        åˆ†æè·ä½åˆ†å¸ƒ
        
        Args:
            jobs: è·ä½åˆ—è¡¨
            
        Returns:
            Dict[str, int]: è·ä½é¡å‹åˆ†å¸ƒçµ±è¨ˆ
        """
        distribution = {}
        
        for job in jobs:
            search_term = getattr(job, 'search_term', 'unknown')
            distribution[search_term] = distribution.get(search_term, 0) + 1
        
        return dict(sorted(distribution.items(), key=lambda x: x[1], reverse=True))
    
    async def run_all_examples(self):
        """
        é‹è¡Œæ‰€æœ‰ç¤ºä¾‹
        """
        print("ğŸš€ é–‹å§‹é‹è¡ŒSeekçˆ¬èŸ²å¼•æ“ç¤ºä¾‹")
        print("=" * 50)
        
        examples = [
            self.example_basic_search,
            self.example_enhanced_search_with_ocr,
            self.example_hybrid_mode_batch_search,
            self.example_custom_configuration,
            self.example_error_handling_and_retry
        ]
        
        for i, example in enumerate(examples, 1):
            try:
                print(f"\n\nğŸ”„ é‹è¡Œç¤ºä¾‹ {i}/{len(examples)}")
                await example()
                print(f"âœ… ç¤ºä¾‹ {i} å®Œæˆ")
            except Exception as e:
                print(f"âŒ ç¤ºä¾‹ {i} å¤±æ•—: {e}")
                continue
        
        print("\n\nğŸ‰ æ‰€æœ‰ç¤ºä¾‹é‹è¡Œå®Œæˆï¼")


async def main():
    """
    ä¸»å‡½æ•¸
    """
    examples = SeekExamples()
    
    # å¯ä»¥é¸æ“‡é‹è¡Œå–®å€‹ç¤ºä¾‹æˆ–æ‰€æœ‰ç¤ºä¾‹
    choice = input("\né¸æ“‡é‹è¡Œæ¨¡å¼:\n1. é‹è¡Œæ‰€æœ‰ç¤ºä¾‹\n2. åŸºæœ¬æœç´¢ç¤ºä¾‹\n3. å¢å¼·æœç´¢ç¤ºä¾‹\n4. æ‰¹é‡æœç´¢ç¤ºä¾‹\n5. è‡ªå®šç¾©é…ç½®ç¤ºä¾‹\n6. éŒ¯èª¤è™•ç†ç¤ºä¾‹\nè«‹è¼¸å…¥é¸é … (1-6): ")
    
    try:
        if choice == '1':
            await examples.run_all_examples()
        elif choice == '2':
            await examples.example_basic_search()
        elif choice == '3':
            await examples.example_enhanced_search_with_ocr()
        elif choice == '4':
            await examples.example_hybrid_mode_batch_search()
        elif choice == '5':
            await examples.example_custom_configuration()
        elif choice == '6':
            await examples.example_error_handling_and_retry()
        else:
            print("âŒ ç„¡æ•ˆé¸é …")
            return
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ¶ä¸­æ–·æ“ä½œ")
    except Exception as e:
        print(f"âŒ é‹è¡Œå¤±æ•—: {e}")


if __name__ == '__main__':
    asyncio.run(main())