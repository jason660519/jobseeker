#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Seekçˆ¬èŸ²å¼•æ“å‘½ä»¤è¡Œæ¥å£
æä¾›ç°¡å–®æ˜“ç”¨çš„å‘½ä»¤è¡Œå·¥å…·ä¾†é‹è¡ŒSeekçˆ¬èŸ²

Author: JobSpy Team
Date: 2024

Usage:
    python cli.py search "python developer" --location "Sydney" --pages 3
    python cli.py config --template development
    python cli.py export --format json --output results.json
"""

import argparse
import asyncio
import json
import sys
from pathlib import Path
from typing import Optional

from .seek_scraper_enhanced import SeekScraperEnhanced
from .config import SeekCrawlerConfig, ConfigTemplates


class SeekCLI:
    """
    Seekçˆ¬èŸ²å‘½ä»¤è¡Œæ¥å£é¡
    """
    
    def __init__(self):
        self.scraper: Optional[SeekScraperEnhanced] = None
        self.config: Optional[SeekCrawlerConfig] = None
    
    def create_parser(self) -> argparse.ArgumentParser:
        """
        å‰µå»ºå‘½ä»¤è¡Œåƒæ•¸è§£æå™¨
        
        Returns:
            argparse.ArgumentParser: åƒæ•¸è§£æå™¨
        """
        parser = argparse.ArgumentParser(
            description='Seekçˆ¬èŸ²å¼•æ“ - æ™ºèƒ½åŒ–æ±‚è·ç¶²ç«™æ•¸æ“šæŠ“å–å·¥å…·',
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
ç¤ºä¾‹ç”¨æ³•:
  %(prog)s search "python developer" --location "Sydney" --pages 3
  %(prog)s search "data scientist" --mode enhanced --ocr
  %(prog)s config --template production --save config.json
  %(prog)s export --input results.json --format csv
            """
        )
        
        # æ·»åŠ å­å‘½ä»¤
        subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
        
        # æœç´¢å‘½ä»¤
        search_parser = subparsers.add_parser('search', help='æœç´¢è·ä½')
        self._add_search_args(search_parser)
        
        # é…ç½®å‘½ä»¤
        config_parser = subparsers.add_parser('config', help='é…ç½®ç®¡ç†')
        self._add_config_args(config_parser)
        
        # å°å‡ºå‘½ä»¤
        export_parser = subparsers.add_parser('export', help='æ•¸æ“šå°å‡º')
        self._add_export_args(export_parser)
        
        # ç‹€æ…‹å‘½ä»¤
        status_parser = subparsers.add_parser('status', help='æŸ¥çœ‹ç‹€æ…‹')
        self._add_status_args(status_parser)
        
        return parser
    
    def _add_search_args(self, parser: argparse.ArgumentParser):
        """
        æ·»åŠ æœç´¢å‘½ä»¤åƒæ•¸
        
        Args:
            parser: åƒæ•¸è§£æå™¨
        """
        # å¿…éœ€åƒæ•¸
        parser.add_argument('search_term', help='æœç´¢é—œéµè©')
        
        # å¯é¸åƒæ•¸
        parser.add_argument('--location', '-l', default='', help='å·¥ä½œåœ°é»')
        parser.add_argument('--job-type', '-t', default='', help='å·¥ä½œé¡å‹')
        parser.add_argument('--pages', '-p', type=int, default=5, help='æœ€å¤§æŠ“å–é æ•¸')
        parser.add_argument('--results', '-r', type=int, default=100, help='æœŸæœ›çµæœæ•¸é‡')
        
        # æ¨¡å¼é¸é …
        parser.add_argument('--mode', '-m', 
                          choices=['traditional', 'enhanced', 'hybrid'],
                          default='hybrid', help='çˆ¬èŸ²æ¨¡å¼')
        
        # åŠŸèƒ½é–‹é—œ
        parser.add_argument('--headless', action='store_true', default=True, help='ç„¡é ­ç€è¦½å™¨æ¨¡å¼')
        parser.add_argument('--no-headless', dest='headless', action='store_false', help='é¡¯ç¤ºç€è¦½å™¨')
        parser.add_argument('--ocr', action='store_true', default=False, help='å•Ÿç”¨OCRåŠŸèƒ½')
        parser.add_argument('--screenshots', action='store_true', default=True, help='å•Ÿç”¨æˆªåœ–')
        parser.add_argument('--no-screenshots', dest='screenshots', action='store_false', help='ç¦ç”¨æˆªåœ–')
        
        # è¼¸å‡ºé¸é …
        parser.add_argument('--output', '-o', help='è¼¸å‡ºæ–‡ä»¶è·¯å¾‘')
        parser.add_argument('--format', '-f', 
                          choices=['json', 'csv', 'excel'],
                          default='json', help='è¼¸å‡ºæ ¼å¼')
        
        # é…ç½®é¸é …
        parser.add_argument('--config', '-c', help='é…ç½®æ–‡ä»¶è·¯å¾‘')
        parser.add_argument('--storage-path', '-s', help='æ•¸æ“šå­˜å„²è·¯å¾‘')
        
        # æ€§èƒ½é¸é …
        parser.add_argument('--workers', '-w', type=int, default=3, help='æœ€å¤§å·¥ä½œç·šç¨‹æ•¸')
        parser.add_argument('--delay', '-d', type=float, default=2.0, help='è«‹æ±‚é–“éš”å»¶é²(ç§’)')
        
        # èª¿è©¦é¸é …
        parser.add_argument('--verbose', '-v', action='store_true', help='è©³ç´°è¼¸å‡º')
        parser.add_argument('--debug', action='store_true', help='èª¿è©¦æ¨¡å¼')
    
    def _add_config_args(self, parser: argparse.ArgumentParser):
        """
        æ·»åŠ é…ç½®å‘½ä»¤åƒæ•¸
        
        Args:
            parser: åƒæ•¸è§£æå™¨
        """
        parser.add_argument('--template', '-t',
                          choices=['development', 'production', 'testing'],
                          help='ä½¿ç”¨é…ç½®æ¨¡æ¿')
        parser.add_argument('--save', '-s', help='ä¿å­˜é…ç½®åˆ°æ–‡ä»¶')
        parser.add_argument('--load', '-l', help='å¾æ–‡ä»¶åŠ è¼‰é…ç½®')
        parser.add_argument('--show', action='store_true', help='é¡¯ç¤ºç•¶å‰é…ç½®')
        parser.add_argument('--validate', action='store_true', help='é©—è­‰é…ç½®')
    
    def _add_export_args(self, parser: argparse.ArgumentParser):
        """
        æ·»åŠ å°å‡ºå‘½ä»¤åƒæ•¸
        
        Args:
            parser: åƒæ•¸è§£æå™¨
        """
        parser.add_argument('--input', '-i', required=True, help='è¼¸å…¥æ•¸æ“šæ–‡ä»¶')
        parser.add_argument('--output', '-o', help='è¼¸å‡ºæ–‡ä»¶è·¯å¾‘')
        parser.add_argument('--format', '-f',
                          choices=['json', 'csv', 'excel'],
                          default='json', help='è¼¸å‡ºæ ¼å¼')
        parser.add_argument('--filter', help='æ•¸æ“šéæ¿¾æ¢ä»¶(JSONæ ¼å¼)')
    
    def _add_status_args(self, parser: argparse.ArgumentParser):
        """
        æ·»åŠ ç‹€æ…‹å‘½ä»¤åƒæ•¸
        
        Args:
            parser: åƒæ•¸è§£æå™¨
        """
        parser.add_argument('--performance', '-p', action='store_true', help='é¡¯ç¤ºæ€§èƒ½çµ±è¨ˆ')
        parser.add_argument('--storage', '-s', action='store_true', help='é¡¯ç¤ºå­˜å„²ä¿¡æ¯')
        parser.add_argument('--logs', '-l', action='store_true', help='é¡¯ç¤ºæœ€è¿‘æ—¥èªŒ')
    
    async def handle_search(self, args) -> int:
        """
        è™•ç†æœç´¢å‘½ä»¤
        
        Args:
            args: å‘½ä»¤è¡Œåƒæ•¸
            
        Returns:
            int: é€€å‡ºä»£ç¢¼
        """
        try:
            print(f"ğŸ” é–‹å§‹æœç´¢: {args.search_term}")
            if args.location:
                print(f"ğŸ“ åœ°é»: {args.location}")
            print(f"âš™ï¸ æ¨¡å¼: {args.mode}")
            print(f"ğŸ“„ æœ€å¤§é æ•¸: {args.pages}")
            print()
            
            # å‰µå»ºé…ç½®
            if args.config:
                self.config = SeekCrawlerConfig(args.config)
            else:
                self.config = SeekCrawlerConfig()
            
            # æ‡‰ç”¨å‘½ä»¤è¡Œåƒæ•¸è¦†è“‹
            self.config.scraping.mode = args.mode
            self.config.scraping.max_pages = args.pages
            self.config.scraping.results_wanted = args.results
            self.config.browser.headless = args.headless
            self.config.ocr.enabled = args.ocr
            self.config.scraping.enable_screenshots = args.screenshots
            
            if args.storage_path:
                self.config.storage.base_path = args.storage_path
            
            if args.debug:
                self.config.logging.level = 'DEBUG'
            elif args.verbose:
                self.config.logging.level = 'INFO'
            
            # å‰µå»ºçˆ¬èŸ²å¯¦ä¾‹
            self.scraper = SeekScraperEnhanced(
                scraping_mode=self.config.scraping.mode,
                headless=self.config.browser.headless,
                enable_ocr=self.config.ocr.enabled,
                enable_screenshots=self.config.scraping.enable_screenshots,
                storage_path=self.config.storage.base_path,
                max_workers=args.workers,
                rate_limit_delay=args.delay
            )
            
            # åˆå§‹åŒ–çˆ¬èŸ²
            if not await self.scraper.initialize():
                print("âŒ çˆ¬èŸ²åˆå§‹åŒ–å¤±æ•—")
                return 1
            
            print("âœ… çˆ¬èŸ²åˆå§‹åŒ–æˆåŠŸ")
            
            # åŸ·è¡Œæœç´¢
            jobs = await self.scraper.scrape_jobs(
                search_term=args.search_term,
                location=args.location,
                job_type=args.job_type,
                max_pages=args.pages,
                results_wanted=args.results
            )
            
            print(f"\nğŸ¯ æœç´¢å®Œæˆ: æ‰¾åˆ° {len(jobs)} å€‹è·ä½")
            
            if jobs:
                # å°å‡ºçµæœ
                output_file = args.output
                if not output_file:
                    from datetime import datetime
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    output_file = f"seek_results_{timestamp}.{args.format}"
                
                exported_file = await self.scraper.export_results(
                    jobs, args.format, output_file
                )
                
                if exported_file:
                    print(f"ğŸ’¾ çµæœå·²å°å‡ºåˆ°: {exported_file}")
                
                # é¡¯ç¤ºæ€§èƒ½å ±å‘Š
                if args.verbose or args.debug:
                    performance_report = self.scraper.get_performance_report()
                    print("\nğŸ“Š æ€§èƒ½å ±å‘Š:")
                    print(f"  æˆåŠŸç‡: {performance_report.get('success_rate', 0)}%")
                    print(f"  å¹³å‡éŸ¿æ‡‰æ™‚é–“: {performance_report.get('average_response_time', 0):.2f}ç§’")
                    print(f"  æ¯åˆ†é˜è·ä½æ•¸: {performance_report.get('jobs_per_minute', 0):.1f}")
            
            return 0
            
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ç”¨æˆ¶ä¸­æ–·æ“ä½œ")
            return 130
        except Exception as e:
            print(f"âŒ æœç´¢å¤±æ•—: {e}")
            if args.debug:
                import traceback
                traceback.print_exc()
            return 1
        finally:
            if self.scraper:
                await self.scraper.cleanup()
    
    def handle_config(self, args) -> int:
        """
        è™•ç†é…ç½®å‘½ä»¤
        
        Args:
            args: å‘½ä»¤è¡Œåƒæ•¸
            
        Returns:
            int: é€€å‡ºä»£ç¢¼
        """
        try:
            if args.template:
                print(f"ğŸ“‹ ä½¿ç”¨é…ç½®æ¨¡æ¿: {args.template}")
                
                if args.template == 'development':
                    config = ConfigTemplates.development()
                elif args.template == 'production':
                    config = ConfigTemplates.production()
                elif args.template == 'testing':
                    config = ConfigTemplates.testing()
                
                if args.save:
                    config.save_to_file(args.save)
                    print(f"ğŸ’¾ é…ç½®å·²ä¿å­˜åˆ°: {args.save}")
                
                if args.show:
                    self._show_config(config)
            
            elif args.load:
                print(f"ğŸ“‚ åŠ è¼‰é…ç½®æ–‡ä»¶: {args.load}")
                config = SeekCrawlerConfig(args.load)
                
                if args.show:
                    self._show_config(config)
            
            elif args.show:
                config = SeekCrawlerConfig()
                self._show_config(config)
            
            elif args.validate:
                config = SeekCrawlerConfig()
                print("âœ… é…ç½®é©—è­‰é€šé")
            
            return 0
            
        except Exception as e:
            print(f"âŒ é…ç½®æ“ä½œå¤±æ•—: {e}")
            return 1
    
    def handle_export(self, args) -> int:
        """
        è™•ç†å°å‡ºå‘½ä»¤
        
        Args:
            args: å‘½ä»¤è¡Œåƒæ•¸
            
        Returns:
            int: é€€å‡ºä»£ç¢¼
        """
        try:
            print(f"ğŸ“¤ å°å‡ºæ•¸æ“š: {args.input} -> {args.format}")
            
            # è®€å–è¼¸å…¥æ–‡ä»¶
            input_path = Path(args.input)
            if not input_path.exists():
                print(f"âŒ è¼¸å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
                return 1
            
            with open(input_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # TODO: å¯¦ç¾æ•¸æ“šè½‰æ›å’Œå°å‡ºé‚è¼¯
            print(f"ğŸ“Š æ•¸æ“šåŒ…å« {len(data.get('jobs', []))} å€‹è·ä½")
            
            output_file = args.output or f"exported_data.{args.format}"
            print(f"ğŸ’¾ æ•¸æ“šå·²å°å‡ºåˆ°: {output_file}")
            
            return 0
            
        except Exception as e:
            print(f"âŒ å°å‡ºå¤±æ•—: {e}")
            return 1
    
    def handle_status(self, args) -> int:
        """
        è™•ç†ç‹€æ…‹å‘½ä»¤
        
        Args:
            args: å‘½ä»¤è¡Œåƒæ•¸
            
        Returns:
            int: é€€å‡ºä»£ç¢¼
        """
        try:
            config = SeekCrawlerConfig()
            
            print("ğŸ“Š Seekçˆ¬èŸ²å¼•æ“ç‹€æ…‹")
            print("=" * 40)
            
            if args.storage or not any([args.performance, args.logs]):
                storage_path = Path(config.storage.base_path)
                print(f"\nğŸ’¾ å­˜å„²ä¿¡æ¯:")
                print(f"  åŸºç¤è·¯å¾‘: {storage_path}")
                print(f"  è·¯å¾‘å­˜åœ¨: {'âœ…' if storage_path.exists() else 'âŒ'}")
                
                if storage_path.exists():
                    total_size = sum(f.stat().st_size for f in storage_path.rglob('*') if f.is_file())
                    print(f"  ç¸½å¤§å°: {total_size / 1024 / 1024:.2f} MB")
                    
                    for subdir_name, subdir_path in config.storage.subdirs.items():
                        full_path = storage_path / subdir_path
                        if full_path.exists():
                            file_count = len(list(full_path.rglob('*')))
                            print(f"  {subdir_name}: {file_count} å€‹æ–‡ä»¶")
            
            if args.performance:
                print(f"\nâš¡ æ€§èƒ½é…ç½®:")
                print(f"  æœ€å¤§ä¸¦ç™¼è«‹æ±‚: {config.performance.max_concurrent_requests}")
                print(f"  å…§å­˜é™åˆ¶: {config.performance.max_memory_usage_mb} MB")
                print(f"  ç·©å­˜å•Ÿç”¨: {'âœ…' if config.performance.enable_caching else 'âŒ'}")
            
            if args.logs:
                logs_path = config.get_storage_path('logs')
                print(f"\nğŸ“ æ—¥èªŒä¿¡æ¯:")
                print(f"  æ—¥èªŒè·¯å¾‘: {logs_path}")
                
                if logs_path.exists():
                    log_files = list(logs_path.glob('*.log'))
                    print(f"  æ—¥èªŒæ–‡ä»¶æ•¸: {len(log_files)}")
                    
                    if log_files:
                        latest_log = max(log_files, key=lambda f: f.stat().st_mtime)
                        print(f"  æœ€æ–°æ—¥èªŒ: {latest_log.name}")
            
            return 0
            
        except Exception as e:
            print(f"âŒ ç‹€æ…‹æŸ¥è©¢å¤±æ•—: {e}")
            return 1
    
    def _show_config(self, config: SeekCrawlerConfig):
        """
        é¡¯ç¤ºé…ç½®ä¿¡æ¯
        
        Args:
            config: é…ç½®å°è±¡
        """
        print("\nâš™ï¸ ç•¶å‰é…ç½®:")
        print("=" * 30)
        print(f"çˆ¬èŸ²æ¨¡å¼: {config.scraping.mode}")
        print(f"ç„¡é ­ç€è¦½å™¨: {config.browser.headless}")
        print(f"OCRå•Ÿç”¨: {config.ocr.enabled}")
        print(f"æœ€å¤§é æ•¸: {config.scraping.max_pages}")
        print(f"æœŸæœ›çµæœæ•¸: {config.scraping.results_wanted}")
        print(f"å­˜å„²è·¯å¾‘: {config.storage.base_path}")
        print(f"æ—¥èªŒç´šåˆ¥: {config.logging.level}")
        print(f"ä¸¦ç™¼è«‹æ±‚æ•¸: {config.performance.max_concurrent_requests}")
    
    async def run(self, args=None) -> int:
        """
        é‹è¡ŒCLIæ‡‰ç”¨
        
        Args:
            args: å‘½ä»¤è¡Œåƒæ•¸åˆ—è¡¨ï¼ˆå¯é¸ï¼‰
            
        Returns:
            int: é€€å‡ºä»£ç¢¼
        """
        parser = self.create_parser()
        parsed_args = parser.parse_args(args)
        
        if not parsed_args.command:
            parser.print_help()
            return 1
        
        try:
            if parsed_args.command == 'search':
                return await self.handle_search(parsed_args)
            elif parsed_args.command == 'config':
                return self.handle_config(parsed_args)
            elif parsed_args.command == 'export':
                return self.handle_export(parsed_args)
            elif parsed_args.command == 'status':
                return self.handle_status(parsed_args)
            else:
                print(f"âŒ æœªçŸ¥å‘½ä»¤: {parsed_args.command}")
                return 1
        
        except Exception as e:
            print(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")
            return 1


def main():
    """
    ä¸»å…¥å£å‡½æ•¸
    """
    cli = SeekCLI()
    
    try:
        exit_code = asyncio.run(cli.run())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ¶ä¸­æ–·æ“ä½œ")
        sys.exit(130)
    except Exception as e:
        print(f"âŒ ç¨‹åºç•°å¸¸: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()