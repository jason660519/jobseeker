#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMåŸºæº–æ¸¬è©¦å¥—ä»¶
æä¾›æ¨™æº–åŒ–çš„LLMæ¨¡å‹åŸºæº–æ¸¬è©¦ï¼ŒåŒ…å«å¤šç¶­åº¦è©•ä¼°æŒ‡æ¨™

Author: JobSpy Team
Date: 2025-01-27
"""

import sys
import os
import json
import time
import asyncio
import statistics
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Union, Callable
from dataclasses import dataclass, asdict, field
from collections import defaultdict, Counter
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from enum import Enum
import hashlib
import random
from pathlib import Path

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from jobseeker.llm_intent_analyzer import LLMIntentAnalyzer, LLMProvider
from jobseeker.llm_config import LLMConfigManager


class BenchmarkCategory(Enum):
    """åŸºæº–æ¸¬è©¦é¡åˆ¥"""
    ACCURACY = "accuracy"  # æº–ç¢ºæ€§æ¸¬è©¦
    SPEED = "speed"  # é€Ÿåº¦æ¸¬è©¦
    CONSISTENCY = "consistency"  # ä¸€è‡´æ€§æ¸¬è©¦
    ROBUSTNESS = "robustness"  # é­¯æ£’æ€§æ¸¬è©¦
    SCALABILITY = "scalability"  # æ“´å±•æ€§æ¸¬è©¦
    EDGE_CASES = "edge_cases"  # é‚Šç•Œæ¡ˆä¾‹æ¸¬è©¦
    MULTILINGUAL = "multilingual"  # å¤šèªè¨€æ¸¬è©¦
    CONTEXT_UNDERSTANDING = "context_understanding"  # ä¸Šä¸‹æ–‡ç†è§£æ¸¬è©¦
    SEMANTIC_SIMILARITY = "semantic_similarity"  # èªç¾©ç›¸ä¼¼æ€§æ¸¬è©¦
    ADVERSARIAL = "adversarial"  # å°æŠ—æ€§æ¸¬è©¦


class TestDifficulty(Enum):
    """æ¸¬è©¦é›£åº¦ç­‰ç´š"""
    TRIVIAL = "trivial"  # æ¥µç°¡å–®
    EASY = "easy"  # ç°¡å–®
    MEDIUM = "medium"  # ä¸­ç­‰
    HARD = "hard"  # å›°é›£
    EXPERT = "expert"  # å°ˆå®¶ç´š


class LanguageVariant(Enum):
    """èªè¨€è®Šé«”"""
    ZH_TW = "zh_tw"  # ç¹é«”ä¸­æ–‡
    ZH_CN = "zh_cn"  # ç°¡é«”ä¸­æ–‡
    EN_US = "en_us"  # ç¾å¼è‹±èª
    EN_GB = "en_gb"  # è‹±å¼è‹±èª
    JA = "ja"  # æ—¥èª
    KO = "ko"  # éŸ“èª
    ES = "es"  # è¥¿ç­ç‰™èª
    FR = "fr"  # æ³•èª
    DE = "de"  # å¾·èª
    MIXED = "mixed"  # æ··åˆèªè¨€


@dataclass
class BenchmarkTestCase:
    """åŸºæº–æ¸¬è©¦æ¡ˆä¾‹"""
    id: str
    category: BenchmarkCategory
    difficulty: TestDifficulty
    language: LanguageVariant
    query: str
    expected_job_related: bool
    expected_intent_type: Optional[str] = None
    expected_entities: Optional[Dict[str, List[str]]] = None
    context: Optional[str] = None
    tags: List[str] = field(default_factory=list)
    weight: float = 1.0  # æ¸¬è©¦æ¬Šé‡
    timeout: float = 30.0  # è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
    description: str = ""
    reference_answer: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class BenchmarkResult:
    """åŸºæº–æ¸¬è©¦çµæœ"""
    test_case_id: str
    provider: str
    success: bool
    accuracy_score: float
    response_time: float
    confidence: float
    predicted_job_related: bool
    predicted_intent_type: Optional[str]
    extracted_entities: Optional[Dict[str, List[str]]]
    error_message: Optional[str]
    reasoning: str
    timestamp: str
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class BenchmarkMetrics:
    """åŸºæº–æ¸¬è©¦æŒ‡æ¨™"""
    provider: str
    category: BenchmarkCategory
    total_tests: int
    passed_tests: int
    failed_tests: int
    accuracy: float
    weighted_accuracy: float
    avg_response_time: float
    median_response_time: float
    p95_response_time: float
    avg_confidence: float
    confidence_std: float
    error_rate: float
    consistency_score: float
    throughput: float  # æ¯ç§’è™•ç†çš„æŸ¥è©¢æ•¸
    difficulty_breakdown: Dict[str, float]
    language_breakdown: Dict[str, float]


class LLMBenchmarkTestSuite:
    """LLMåŸºæº–æ¸¬è©¦å¥—ä»¶"""
    
    def __init__(self, config_file: Optional[str] = None):
        """åˆå§‹åŒ–æ¸¬è©¦å¥—ä»¶"""
        self.config_manager = LLMConfigManager()
        self.test_cases: List[BenchmarkTestCase] = []
        self.results: List[BenchmarkResult] = []
        self.metrics: Dict[str, Dict[BenchmarkCategory, BenchmarkMetrics]] = {}
        self.config_file = config_file
        
        # è¼‰å…¥æ¸¬è©¦æ¡ˆä¾‹
        self._load_benchmark_test_cases()
        
        # è¨­ç½®éš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯é‡ç¾æ€§
        random.seed(42)
        np.random.seed(42)
    
    def _load_benchmark_test_cases(self) -> None:
        """è¼‰å…¥åŸºæº–æ¸¬è©¦æ¡ˆä¾‹"""
        print("ğŸ“‹ è¼‰å…¥åŸºæº–æ¸¬è©¦æ¡ˆä¾‹...")
        
        # æº–ç¢ºæ€§æ¸¬è©¦æ¡ˆä¾‹
        self._add_accuracy_test_cases()
        
        # é€Ÿåº¦æ¸¬è©¦æ¡ˆä¾‹
        self._add_speed_test_cases()
        
        # ä¸€è‡´æ€§æ¸¬è©¦æ¡ˆä¾‹
        self._add_consistency_test_cases()
        
        # é­¯æ£’æ€§æ¸¬è©¦æ¡ˆä¾‹
        self._add_robustness_test_cases()
        
        # å¤šèªè¨€æ¸¬è©¦æ¡ˆä¾‹
        self._add_multilingual_test_cases()
        
        # é‚Šç•Œæ¡ˆä¾‹æ¸¬è©¦
        self._add_edge_cases_test_cases()
        
        # ä¸Šä¸‹æ–‡ç†è§£æ¸¬è©¦
        self._add_context_understanding_test_cases()
        
        # èªç¾©ç›¸ä¼¼æ€§æ¸¬è©¦
        self._add_semantic_similarity_test_cases()
        
        # å°æŠ—æ€§æ¸¬è©¦
        self._add_adversarial_test_cases()
        
        print(f"   âœ… è¼‰å…¥äº† {len(self.test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹")
    
    def _add_accuracy_test_cases(self) -> None:
        """æ·»åŠ æº–ç¢ºæ€§æ¸¬è©¦æ¡ˆä¾‹"""
        accuracy_cases = [
            # åŸºç¤æ±‚è·æŸ¥è©¢
            BenchmarkTestCase(
                id="acc_001",
                category=BenchmarkCategory.ACCURACY,
                difficulty=TestDifficulty.EASY,
                language=LanguageVariant.ZH_TW,
                query="æˆ‘æƒ³æ‰¾è»Ÿé«”å·¥ç¨‹å¸«çš„å·¥ä½œ",
                expected_job_related=True,
                expected_intent_type="job_search",
                expected_entities={"job_titles": ["è»Ÿé«”å·¥ç¨‹å¸«"]},
                description="åŸºç¤æ±‚è·æ„åœ–è­˜åˆ¥",
                weight=1.0
            ),
            BenchmarkTestCase(
                id="acc_002",
                category=BenchmarkCategory.ACCURACY,
                difficulty=TestDifficulty.EASY,
                language=LanguageVariant.EN_US,
                query="Looking for data scientist positions",
                expected_job_related=True,
                expected_intent_type="job_search",
                expected_entities={"job_titles": ["data scientist"]},
                description="è‹±æ–‡åŸºç¤æ±‚è·æŸ¥è©¢",
                weight=1.0
            ),
            # è¤‡é›œæ±‚è·æŸ¥è©¢
            BenchmarkTestCase(
                id="acc_003",
                category=BenchmarkCategory.ACCURACY,
                difficulty=TestDifficulty.MEDIUM,
                language=LanguageVariant.ZH_TW,
                query="åœ¨å°åŒ—æ‰¾å¹´è–ª100è¬ä»¥ä¸Šçš„è³‡æ·±Pythoné–‹ç™¼å·¥ç¨‹å¸«è·ä½ï¼Œè¦æ±‚æœ‰5å¹´ä»¥ä¸Šç¶“é©—",
                expected_job_related=True,
                expected_intent_type="job_search",
                expected_entities={
                    "job_titles": ["Pythoné–‹ç™¼å·¥ç¨‹å¸«"],
                    "locations": ["å°åŒ—"],
                    "skills": ["Python"],
                    "salary_range": ["100è¬"]
                },
                description="è¤‡é›œå¤šæ¢ä»¶æ±‚è·æŸ¥è©¢",
                weight=1.5
            ),
            # éæ±‚è·æŸ¥è©¢
            BenchmarkTestCase(
                id="acc_004",
                category=BenchmarkCategory.ACCURACY,
                difficulty=TestDifficulty.EASY,
                language=LanguageVariant.ZH_TW,
                query="ä»Šå¤©å¤©æ°£å¦‚ä½•ï¼Ÿ",
                expected_job_related=False,
                expected_intent_type="general_inquiry",
                description="éæ±‚è·ç›¸é—œæŸ¥è©¢",
                weight=1.0
            ),
            # æ¨¡ç³Šæ±‚è·æŸ¥è©¢
            BenchmarkTestCase(
                id="acc_005",
                category=BenchmarkCategory.ACCURACY,
                difficulty=TestDifficulty.HARD,
                language=LanguageVariant.ZH_TW,
                query="æˆ‘æƒ³æ›å€‹ç’°å¢ƒï¼Œæ‰¾å€‹æœ‰æŒ‘æˆ°æ€§çš„å·¥ä½œ",
                expected_job_related=True,
                expected_intent_type="job_search",
                description="æ¨¡ç³Šæ±‚è·æ„åœ–",
                weight=2.0
            ),
            # æŠ€èƒ½å°å‘æŸ¥è©¢
            BenchmarkTestCase(
                id="acc_006",
                category=BenchmarkCategory.ACCURACY,
                difficulty=TestDifficulty.MEDIUM,
                language=LanguageVariant.ZH_TW,
                query="æˆ‘æœƒReactã€Node.jså’ŒMongoDBï¼Œæœ‰ä»€éº¼é©åˆçš„è·ä½å—ï¼Ÿ",
                expected_job_related=True,
                expected_intent_type="skill_based_search",
                expected_entities={
                    "skills": ["React", "Node.js", "MongoDB"]
                },
                description="æŠ€èƒ½å°å‘æ±‚è·æŸ¥è©¢",
                weight=1.5
            )
        ]
        
        self.test_cases.extend(accuracy_cases)
    
    def _add_speed_test_cases(self) -> None:
        """æ·»åŠ é€Ÿåº¦æ¸¬è©¦æ¡ˆä¾‹"""
        speed_cases = [
            BenchmarkTestCase(
                id="speed_001",
                category=BenchmarkCategory.SPEED,
                difficulty=TestDifficulty.EASY,
                language=LanguageVariant.ZH_TW,
                query="æ‰¾å·¥ä½œ",
                expected_job_related=True,
                timeout=5.0,
                description="æ¥µç°¡æŸ¥è©¢é€Ÿåº¦æ¸¬è©¦",
                weight=1.0
            ),
            BenchmarkTestCase(
                id="speed_002",
                category=BenchmarkCategory.SPEED,
                difficulty=TestDifficulty.MEDIUM,
                language=LanguageVariant.ZH_TW,
                query="æˆ‘æƒ³åœ¨å°åŒ—æ‰¾ä¸€ä»½è»Ÿé«”é–‹ç™¼çš„å·¥ä½œï¼Œè–ªæ°´å¸Œæœ›åœ¨60Kä»¥ä¸Šï¼Œå…¬å¸è¦æ¨¡ä¸è¦å¤ªå°ï¼Œæœ€å¥½æœ‰å½ˆæ€§å·¥æ™‚å’Œé ç«¯å·¥ä½œçš„æ©Ÿæœƒ",
                expected_job_related=True,
                timeout=10.0,
                description="ä¸­ç­‰é•·åº¦æŸ¥è©¢é€Ÿåº¦æ¸¬è©¦",
                weight=1.0
            ),
            BenchmarkTestCase(
                id="speed_003",
                category=BenchmarkCategory.SPEED,
                difficulty=TestDifficulty.HARD,
                language=LanguageVariant.ZH_TW,
                query="æˆ‘æ˜¯ä¸€å€‹æœ‰10å¹´ç¶“é©—çš„å…¨ç«¯å·¥ç¨‹å¸«ï¼Œç²¾é€šJavaScriptã€Pythonã€Javaã€C#ç­‰å¤šç¨®ç¨‹å¼èªè¨€ï¼Œç†Ÿæ‚‰Reactã€Vueã€Angularç­‰å‰ç«¯æ¡†æ¶ï¼Œä¹Ÿæœ‰è±å¯Œçš„å¾Œç«¯é–‹ç™¼ç¶“é©—ï¼ŒåŒ…æ‹¬Node.jsã€Djangoã€Spring Bootç­‰ï¼Œè³‡æ–™åº«æ–¹é¢ç†Ÿæ‚‰MySQLã€PostgreSQLã€MongoDBã€Redisï¼Œé›²ç«¯æœå‹™æœ‰AWSã€Azureã€GCPçš„ä½¿ç”¨ç¶“é©—ï¼ŒDevOpså·¥å…·å¦‚Dockerã€Kubernetesã€Jenkinsä¹Ÿéƒ½æœ‰æ¥è§¸ï¼Œç¾åœ¨æƒ³æ‰¾ä¸€ä»½æŠ€è¡“ä¸»ç®¡æˆ–æ¶æ§‹å¸«çš„è·ä½ï¼Œå¸Œæœ›èƒ½åœ¨ä¸€å®¶æœ‰æŠ€è¡“æŒ‘æˆ°çš„å…¬å¸å·¥ä½œï¼Œè–ªè³‡æœŸæœ›åœ¨150Kä»¥ä¸Šï¼Œåœ°é»å¸Œæœ›åœ¨å°åŒ—æˆ–æ–°ç«¹ï¼Œå…¬å¸æ–‡åŒ–è¦é–‹æ”¾ï¼Œæœ‰å­¸ç¿’æˆé•·çš„æ©Ÿæœƒ",
                expected_job_related=True,
                timeout=15.0,
                description="é•·æ–‡æœ¬æŸ¥è©¢é€Ÿåº¦æ¸¬è©¦",
                weight=1.0
            )
        ]
        
        self.test_cases.extend(speed_cases)
    
    def _add_consistency_test_cases(self) -> None:
        """æ·»åŠ ä¸€è‡´æ€§æ¸¬è©¦æ¡ˆä¾‹ï¼ˆåŒä¸€æŸ¥è©¢å¤šæ¬¡åŸ·è¡Œï¼‰"""
        base_queries = [
            "æˆ‘æƒ³æ‰¾å‰ç«¯å·¥ç¨‹å¸«çš„å·¥ä½œ",
            "Looking for marketing manager position",
            "ä»Šå¤©è‚¡å¸‚å¦‚ä½•ï¼Ÿ",
            "æˆ‘æƒ³è½‰è·åˆ°AIé ˜åŸŸ"
        ]
        
        for i, query in enumerate(base_queries):
            is_job_related = i < 2 or i == 3  # å‰å…©å€‹å’Œæœ€å¾Œä¸€å€‹æ˜¯æ±‚è·ç›¸é—œ
            
            # ç‚ºæ¯å€‹æŸ¥è©¢å‰µå»ºå¤šå€‹ä¸€è‡´æ€§æ¸¬è©¦æ¡ˆä¾‹
            for j in range(5):  # æ¯å€‹æŸ¥è©¢æ¸¬è©¦5æ¬¡
                self.test_cases.append(BenchmarkTestCase(
                    id=f"cons_{i+1:03d}_{j+1}",
                    category=BenchmarkCategory.CONSISTENCY,
                    difficulty=TestDifficulty.MEDIUM,
                    language=LanguageVariant.ZH_TW if i % 2 == 0 else LanguageVariant.EN_US,
                    query=query,
                    expected_job_related=is_job_related,
                    description=f"ä¸€è‡´æ€§æ¸¬è©¦ - æŸ¥è©¢{i+1} ç¬¬{j+1}æ¬¡",
                    weight=1.0,
                    metadata={"consistency_group": f"group_{i+1}", "attempt": j+1}
                ))
    
    def _add_robustness_test_cases(self) -> None:
        """æ·»åŠ é­¯æ£’æ€§æ¸¬è©¦æ¡ˆä¾‹"""
        robustness_cases = [
            # æ‹¼å¯«éŒ¯èª¤
            BenchmarkTestCase(
                id="rob_001",
                category=BenchmarkCategory.ROBUSTNESS,
                difficulty=TestDifficulty.MEDIUM,
                language=LanguageVariant.ZH_TW,
                query="æˆ‘æƒ³æ‰¾è»Ÿä½“å·¥ç¨‹å¸«çš„å·¥åš",  # æ•…æ„æ‹¼éŒ¯
                expected_job_related=True,
                description="æ‹¼å¯«éŒ¯èª¤é­¯æ£’æ€§æ¸¬è©¦",
                weight=1.5
            ),
            # ç‰¹æ®Šå­—ç¬¦
            BenchmarkTestCase(
                id="rob_002",
                category=BenchmarkCategory.ROBUSTNESS,
                difficulty=TestDifficulty.MEDIUM,
                language=LanguageVariant.ZH_TW,
                query="æˆ‘æƒ³æ‰¾@#$%è»Ÿé«”å·¥ç¨‹å¸«&*()çš„å·¥ä½œ!!!",
                expected_job_related=True,
                description="ç‰¹æ®Šå­—ç¬¦é­¯æ£’æ€§æ¸¬è©¦",
                weight=1.5
            ),
            # æ¥µé•·æŸ¥è©¢
            BenchmarkTestCase(
                id="rob_003",
                category=BenchmarkCategory.ROBUSTNESS,
                difficulty=TestDifficulty.HARD,
                language=LanguageVariant.ZH_TW,
                query="æ‰¾å·¥ä½œ" * 100,  # é‡è¤‡200æ¬¡
                expected_job_related=True,
                description="æ¥µé•·æŸ¥è©¢é­¯æ£’æ€§æ¸¬è©¦",
                weight=2.0
            ),
            # ç©ºç™½å’Œç‰¹æ®Šç©ºæ ¼
            BenchmarkTestCase(
                id="rob_004",
                category=BenchmarkCategory.ROBUSTNESS,
                difficulty=TestDifficulty.MEDIUM,
                language=LanguageVariant.ZH_TW,
                query="   æˆ‘æƒ³æ‰¾    è»Ÿé«”å·¥ç¨‹å¸«   çš„å·¥ä½œ   ",
                expected_job_related=True,
                description="ç©ºç™½å­—ç¬¦é­¯æ£’æ€§æ¸¬è©¦",
                weight=1.0
            ),
            # æ•¸å­—å’Œç¬¦è™Ÿæ··åˆ
            BenchmarkTestCase(
                id="rob_005",
                category=BenchmarkCategory.ROBUSTNESS,
                difficulty=TestDifficulty.MEDIUM,
                language=LanguageVariant.ZH_TW,
                query="æˆ‘æƒ³æ‰¾123è»Ÿé«”å·¥ç¨‹å¸«456çš„å·¥ä½œ789",
                expected_job_related=True,
                description="æ•¸å­—ç¬¦è™Ÿæ··åˆé­¯æ£’æ€§æ¸¬è©¦",
                weight=1.5
            )
        ]
        
        self.test_cases.extend(robustness_cases)
    
    def _add_multilingual_test_cases(self) -> None:
        """æ·»åŠ å¤šèªè¨€æ¸¬è©¦æ¡ˆä¾‹"""
        multilingual_cases = [
            # ç¹é«”ä¸­æ–‡
            BenchmarkTestCase(
                id="ml_001",
                category=BenchmarkCategory.MULTILINGUAL,
                difficulty=TestDifficulty.EASY,
                language=LanguageVariant.ZH_TW,
                query="æˆ‘æƒ³æ‰¾è³‡æ–™ç§‘å­¸å®¶çš„å·¥ä½œ",
                expected_job_related=True,
                description="ç¹é«”ä¸­æ–‡æ±‚è·æŸ¥è©¢",
                weight=1.0
            ),
            # ç°¡é«”ä¸­æ–‡
            BenchmarkTestCase(
                id="ml_002",
                category=BenchmarkCategory.MULTILINGUAL,
                difficulty=TestDifficulty.EASY,
                language=LanguageVariant.ZH_CN,
                query="æˆ‘æƒ³æ‰¾æ•°æ®ç§‘å­¦å®¶çš„å·¥ä½œ",
                expected_job_related=True,
                description="ç°¡é«”ä¸­æ–‡æ±‚è·æŸ¥è©¢",
                weight=1.0
            ),
            # è‹±æ–‡
            BenchmarkTestCase(
                id="ml_003",
                category=BenchmarkCategory.MULTILINGUAL,
                difficulty=TestDifficulty.EASY,
                language=LanguageVariant.EN_US,
                query="I want to find a data scientist job",
                expected_job_related=True,
                description="è‹±æ–‡æ±‚è·æŸ¥è©¢",
                weight=1.0
            ),
            # æ—¥æ–‡
            BenchmarkTestCase(
                id="ml_004",
                category=BenchmarkCategory.MULTILINGUAL,
                difficulty=TestDifficulty.MEDIUM,
                language=LanguageVariant.JA,
                query="ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã®ä»•äº‹ã‚’æ¢ã—ã¦ã„ã¾ã™",
                expected_job_related=True,
                description="æ—¥æ–‡æ±‚è·æŸ¥è©¢",
                weight=1.5
            ),
            # éŸ“æ–‡
            BenchmarkTestCase(
                id="ml_005",
                category=BenchmarkCategory.MULTILINGUAL,
                difficulty=TestDifficulty.MEDIUM,
                language=LanguageVariant.KO,
                query="ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ ì¼ìë¦¬ë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤",
                expected_job_related=True,
                description="éŸ“æ–‡æ±‚è·æŸ¥è©¢",
                weight=1.5
            ),
            # æ··åˆèªè¨€
            BenchmarkTestCase(
                id="ml_006",
                category=BenchmarkCategory.MULTILINGUAL,
                difficulty=TestDifficulty.HARD,
                language=LanguageVariant.MIXED,
                query="æˆ‘æƒ³æ‰¾software engineerçš„å·¥ä½œï¼Œè¦æ±‚æœƒPythonå’Œæ©Ÿæ¢°å­¸ç¿’",
                expected_job_related=True,
                description="ä¸­è‹±æ··åˆæ±‚è·æŸ¥è©¢",
                weight=2.0
            )
        ]
        
        self.test_cases.extend(multilingual_cases)
    
    def _add_edge_cases_test_cases(self) -> None:
        """æ·»åŠ é‚Šç•Œæ¡ˆä¾‹æ¸¬è©¦"""
        edge_cases = [
            # æ¥µçŸ­æŸ¥è©¢
            BenchmarkTestCase(
                id="edge_001",
                category=BenchmarkCategory.EDGE_CASES,
                difficulty=TestDifficulty.HARD,
                language=LanguageVariant.ZH_TW,
                query="å·¥ä½œ",
                expected_job_related=True,
                description="æ¥µçŸ­æŸ¥è©¢é‚Šç•Œæ¸¬è©¦",
                weight=2.0
            ),
            # ç©ºæŸ¥è©¢
            BenchmarkTestCase(
                id="edge_002",
                category=BenchmarkCategory.EDGE_CASES,
                difficulty=TestDifficulty.EXPERT,
                language=LanguageVariant.ZH_TW,
                query="",
                expected_job_related=False,
                description="ç©ºæŸ¥è©¢é‚Šç•Œæ¸¬è©¦",
                weight=3.0
            ),
            # åªæœ‰æ¨™é»ç¬¦è™Ÿ
            BenchmarkTestCase(
                id="edge_003",
                category=BenchmarkCategory.EDGE_CASES,
                difficulty=TestDifficulty.EXPERT,
                language=LanguageVariant.ZH_TW,
                query="ï¼Ÿï¼ã€‚ï¼Œï¼›ï¼š",
                expected_job_related=False,
                description="ç´”æ¨™é»ç¬¦è™Ÿé‚Šç•Œæ¸¬è©¦",
                weight=3.0
            ),
            # æ•¸å­—æŸ¥è©¢
            BenchmarkTestCase(
                id="edge_004",
                category=BenchmarkCategory.EDGE_CASES,
                difficulty=TestDifficulty.HARD,
                language=LanguageVariant.ZH_TW,
                query="123456789",
                expected_job_related=False,
                description="ç´”æ•¸å­—é‚Šç•Œæ¸¬è©¦",
                weight=2.0
            ),
            # é‡è¤‡å­—ç¬¦
            BenchmarkTestCase(
                id="edge_005",
                category=BenchmarkCategory.EDGE_CASES,
                difficulty=TestDifficulty.HARD,
                language=LanguageVariant.ZH_TW,
                query="å·¥å·¥å·¥å·¥å·¥å·¥å·¥å·¥å·¥å·¥ä½œä½œä½œä½œä½œä½œä½œä½œä½œä½œ",
                expected_job_related=True,
                description="é‡è¤‡å­—ç¬¦é‚Šç•Œæ¸¬è©¦",
                weight=2.0
            )
        ]
        
        self.test_cases.extend(edge_cases)
    
    def _add_context_understanding_test_cases(self) -> None:
        """æ·»åŠ ä¸Šä¸‹æ–‡ç†è§£æ¸¬è©¦æ¡ˆä¾‹"""
        context_cases = [
            BenchmarkTestCase(
                id="ctx_001",
                category=BenchmarkCategory.CONTEXT_UNDERSTANDING,
                difficulty=TestDifficulty.MEDIUM,
                language=LanguageVariant.ZH_TW,
                query="æˆ‘æƒ³æ›å·¥ä½œ",
                context="ç”¨æˆ¶ä¹‹å‰æåˆ°å°ç›®å‰çš„è»Ÿé«”é–‹ç™¼å·¥ä½œä¸æ»¿æ„",
                expected_job_related=True,
                description="åŸºæ–¼ä¸Šä¸‹æ–‡çš„æ±‚è·æ„åœ–ç†è§£",
                weight=1.5
            ),
            BenchmarkTestCase(
                id="ctx_002",
                category=BenchmarkCategory.CONTEXT_UNDERSTANDING,
                difficulty=TestDifficulty.HARD,
                language=LanguageVariant.ZH_TW,
                query="æœ‰ä»€éº¼æ¨è–¦çš„å—ï¼Ÿ",
                context="ç”¨æˆ¶å‰›æ‰è©¢å•äº†é—œæ–¼è³‡æ–™ç§‘å­¸å®¶è·ä½çš„è¦æ±‚",
                expected_job_related=True,
                description="éš±å«æ±‚è·æ„åœ–çš„ä¸Šä¸‹æ–‡ç†è§£",
                weight=2.0
            )
        ]
        
        self.test_cases.extend(context_cases)
    
    def _add_semantic_similarity_test_cases(self) -> None:
        """æ·»åŠ èªç¾©ç›¸ä¼¼æ€§æ¸¬è©¦æ¡ˆä¾‹"""
        semantic_cases = [
            BenchmarkTestCase(
                id="sem_001",
                category=BenchmarkCategory.SEMANTIC_SIMILARITY,
                difficulty=TestDifficulty.MEDIUM,
                language=LanguageVariant.ZH_TW,
                query="æˆ‘æƒ³å°‹æ‰¾è·æ¥­æ©Ÿæœƒ",
                expected_job_related=True,
                description="èªç¾©ç›¸ä¼¼çš„æ±‚è·è¡¨é”",
                weight=1.5
            ),
            BenchmarkTestCase(
                id="sem_002",
                category=BenchmarkCategory.SEMANTIC_SIMILARITY,
                difficulty=TestDifficulty.MEDIUM,
                language=LanguageVariant.ZH_TW,
                query="å¸Œæœ›èƒ½æ‰¾åˆ°æ–°çš„å·¥ä½œæ©Ÿæœƒ",
                expected_job_related=True,
                description="å¦ä¸€ç¨®æ±‚è·è¡¨é”æ–¹å¼",
                weight=1.5
            ),
            BenchmarkTestCase(
                id="sem_003",
                category=BenchmarkCategory.SEMANTIC_SIMILARITY,
                difficulty=TestDifficulty.HARD,
                language=LanguageVariant.ZH_TW,
                query="æƒ³è¦è½‰æ›è·‘é“",
                expected_job_related=True,
                description="éš±å–»æ€§çš„æ±‚è·è¡¨é”",
                weight=2.0
            )
        ]
        
        self.test_cases.extend(semantic_cases)
    
    def _add_adversarial_test_cases(self) -> None:
        """æ·»åŠ å°æŠ—æ€§æ¸¬è©¦æ¡ˆä¾‹"""
        adversarial_cases = [
            BenchmarkTestCase(
                id="adv_001",
                category=BenchmarkCategory.ADVERSARIAL,
                difficulty=TestDifficulty.EXPERT,
                language=LanguageVariant.ZH_TW,
                query="æˆ‘ä¸æƒ³æ‰¾å·¥ä½œï¼Œä½†æ˜¯æƒ³äº†è§£è»Ÿé«”å·¥ç¨‹å¸«çš„è–ªè³‡",
                expected_job_related=False,  # æ˜ç¢ºèªªä¸æƒ³æ‰¾å·¥ä½œ
                description="å°æŠ—æ€§æ¸¬è©¦ - æ˜ç¢ºå¦å®šä½†åŒ…å«è·ä½é—œéµè©",
                weight=3.0
            ),
            BenchmarkTestCase(
                id="adv_002",
                category=BenchmarkCategory.ADVERSARIAL,
                difficulty=TestDifficulty.EXPERT,
                language=LanguageVariant.ZH_TW,
                query="å·¥ä½œçœŸç´¯ï¼Œæˆ‘æƒ³ä¼‘æ¯",
                expected_job_related=False,  # æŠ±æ€¨å·¥ä½œï¼Œä¸æ˜¯æ‰¾å·¥ä½œ
                description="å°æŠ—æ€§æ¸¬è©¦ - è² é¢å·¥ä½œæƒ…ç·’",
                weight=3.0
            ),
            BenchmarkTestCase(
                id="adv_003",
                category=BenchmarkCategory.ADVERSARIAL,
                difficulty=TestDifficulty.EXPERT,
                language=LanguageVariant.ZH_TW,
                query="æˆ‘æœ‹å‹æƒ³æ‰¾è»Ÿé«”å·¥ç¨‹å¸«çš„å·¥ä½œ",
                expected_job_related=False,  # æ˜¯æœ‹å‹è¦æ‰¾ï¼Œä¸æ˜¯è‡ªå·±
                description="å°æŠ—æ€§æ¸¬è©¦ - ç¬¬ä¸‰äººç¨±æ±‚è·",
                weight=3.0
            )
        ]
        
        self.test_cases.extend(adversarial_cases)
    
    def run_benchmark(self, providers: Optional[List[LLMProvider]] = None, 
                     categories: Optional[List[BenchmarkCategory]] = None,
                     max_workers: int = 3) -> Dict[str, Any]:
        """é‹è¡ŒåŸºæº–æ¸¬è©¦"""
        print("ğŸš€ é–‹å§‹LLMåŸºæº–æ¸¬è©¦")
        print("=" * 60)
        
        # ç¢ºå®šè¦æ¸¬è©¦çš„æä¾›å•†
        if providers is None:
            providers = self.config_manager.get_available_providers()
        
        # ç¢ºå®šè¦æ¸¬è©¦çš„é¡åˆ¥
        if categories is None:
            categories = list(BenchmarkCategory)
        
        # éæ¿¾æ¸¬è©¦æ¡ˆä¾‹
        filtered_test_cases = [
            tc for tc in self.test_cases 
            if tc.category in categories
        ]
        
        print(f"ğŸ“‹ æ¸¬è©¦æä¾›å•†: {[p.value for p in providers]}")
        print(f"ğŸ¯ æ¸¬è©¦é¡åˆ¥: {[c.value for c in categories]}")
        print(f"ğŸ“ æ¸¬è©¦æ¡ˆä¾‹æ•¸é‡: {len(filtered_test_cases)}")
        print(f"âš¡ ä¸¦è¡Œå·¥ä½œæ•¸: {max_workers}")
        
        # åŸ·è¡Œæ¸¬è©¦
        start_time = time.time()
        
        for provider in providers:
            print(f"\nğŸ”„ æ¸¬è©¦æä¾›å•†: {provider.value}")
            provider_results = self._run_provider_tests(
                provider, filtered_test_cases, max_workers
            )
            self.results.extend(provider_results)
        
        total_time = time.time() - start_time
        
        # è¨ˆç®—æŒ‡æ¨™
        self._calculate_benchmark_metrics()
        
        # ç”Ÿæˆå ±å‘Š
        report = self._generate_benchmark_report(total_time)
        
        # ä¿å­˜çµæœ
        self._save_benchmark_results(report)
        
        return report
    
    def _run_provider_tests(self, provider: LLMProvider, 
                           test_cases: List[BenchmarkTestCase],
                           max_workers: int) -> List[BenchmarkResult]:
        """é‹è¡Œç‰¹å®šæä¾›å•†çš„æ¸¬è©¦"""
        try:
            analyzer = LLMIntentAnalyzer(provider=provider)
            results = []
            
            # ä½¿ç”¨ç·šç¨‹æ± ä¸¦è¡ŒåŸ·è¡Œæ¸¬è©¦
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_test = {
                    executor.submit(self._execute_test_case, analyzer, test_case): test_case
                    for test_case in test_cases
                }
                
                completed = 0
                total = len(test_cases)
                
                for future in as_completed(future_to_test):
                    test_case = future_to_test[future]
                    completed += 1
                    
                    try:
                        result = future.result()
                        results.append(result)
                        
                        # é¡¯ç¤ºé€²åº¦
                        if completed % 10 == 0 or completed == total:
                            print(f"   é€²åº¦: {completed}/{total} ({completed/total*100:.1f}%)")
                            
                    except Exception as e:
                        print(f"   âŒ æ¸¬è©¦æ¡ˆä¾‹ {test_case.id} å¤±æ•—: {str(e)}")
                        # å‰µå»ºå¤±æ•—çµæœ
                        results.append(BenchmarkResult(
                            test_case_id=test_case.id,
                            provider=provider.value,
                            success=False,
                            accuracy_score=0.0,
                            response_time=0.0,
                            confidence=0.0,
                            predicted_job_related=False,
                            predicted_intent_type=None,
                            extracted_entities=None,
                            error_message=str(e),
                            reasoning="",
                            timestamp=datetime.now().isoformat()
                        ))
            
            print(f"   âœ… å®Œæˆ {len(results)} å€‹æ¸¬è©¦")
            return results
            
        except Exception as e:
            print(f"   âŒ æä¾›å•† {provider.value} æ¸¬è©¦å¤±æ•—: {str(e)}")
            return []
    
    def _execute_test_case(self, analyzer: LLMIntentAnalyzer, 
                          test_case: BenchmarkTestCase) -> BenchmarkResult:
        """åŸ·è¡Œå–®å€‹æ¸¬è©¦æ¡ˆä¾‹"""
        start_time = time.time()
        
        try:
            # è¨­ç½®è¶…æ™‚
            result = analyzer.analyze_intent(test_case.query)
            response_time = time.time() - start_time
            
            # æª¢æŸ¥æ˜¯å¦è¶…æ™‚
            if response_time > test_case.timeout:
                raise TimeoutError(f"æ¸¬è©¦è¶…æ™‚ ({response_time:.2f}s > {test_case.timeout}s)")
            
            # è¨ˆç®—æº–ç¢ºç‡åˆ†æ•¸
            accuracy_score = self._calculate_accuracy_score(test_case, result)
            
            # æå–å¯¦é«”
            extracted_entities = None
            if result.structured_intent and result.is_job_related:
                intent = result.structured_intent
                extracted_entities = {
                    "job_titles": getattr(intent, 'job_titles', []),
                    "skills": getattr(intent, 'skills', []),
                    "locations": getattr(intent, 'locations', []),
                    "salary_range": getattr(intent, 'salary_range', None)
                }
            
            return BenchmarkResult(
                test_case_id=test_case.id,
                provider=analyzer.provider.value,
                success=True,
                accuracy_score=accuracy_score,
                response_time=response_time,
                confidence=result.confidence,
                predicted_job_related=result.is_job_related,
                predicted_intent_type=result.intent_type.value if hasattr(result.intent_type, 'value') else str(result.intent_type),
                extracted_entities=extracted_entities,
                error_message=None,
                reasoning=getattr(result, 'llm_reasoning', ''),
                timestamp=datetime.now().isoformat(),
                metadata={
                    "test_category": test_case.category.value,
                    "test_difficulty": test_case.difficulty.value,
                    "test_language": test_case.language.value,
                    "test_weight": test_case.weight
                }
            )
            
        except Exception as e:
            response_time = time.time() - start_time
            return BenchmarkResult(
                test_case_id=test_case.id,
                provider=analyzer.provider.value,
                success=False,
                accuracy_score=0.0,
                response_time=response_time,
                confidence=0.0,
                predicted_job_related=False,
                predicted_intent_type=None,
                extracted_entities=None,
                error_message=str(e),
                reasoning="",
                timestamp=datetime.now().isoformat(),
                metadata={
                    "test_category": test_case.category.value,
                    "test_difficulty": test_case.difficulty.value,
                    "test_language": test_case.language.value,
                    "test_weight": test_case.weight
                }
            )
    
    def _calculate_accuracy_score(self, test_case: BenchmarkTestCase, 
                                 result: Any) -> float:
        """è¨ˆç®—æº–ç¢ºç‡åˆ†æ•¸"""
        score = 0.0
        
        # åŸºç¤æ±‚è·ç›¸é—œæ€§åˆ¤æ–· (æ¬Šé‡: 50%)
        if test_case.expected_job_related == result.is_job_related:
            score += 0.5
        
        # æ„åœ–é¡å‹åŒ¹é… (æ¬Šé‡: 20%)
        if test_case.expected_intent_type:
            predicted_intent = result.intent_type.value if hasattr(result.intent_type, 'value') else str(result.intent_type)
            if test_case.expected_intent_type == predicted_intent:
                score += 0.2
        else:
            score += 0.2  # å¦‚æœæ²’æœ‰æœŸæœ›çš„æ„åœ–é¡å‹ï¼Œçµ¦äºˆæ»¿åˆ†
        
        # å¯¦é«”æå–åŒ¹é… (æ¬Šé‡: 30%)
        if test_case.expected_entities and result.structured_intent and result.is_job_related:
            entity_score = self._calculate_entity_score(test_case.expected_entities, result.structured_intent)
            score += 0.3 * entity_score
        else:
            score += 0.3  # å¦‚æœæ²’æœ‰æœŸæœ›çš„å¯¦é«”ï¼Œçµ¦äºˆæ»¿åˆ†
        
        return min(score, 1.0)  # ç¢ºä¿åˆ†æ•¸ä¸è¶…é1.0
    
    def _calculate_entity_score(self, expected_entities: Dict[str, List[str]], 
                               structured_intent: Any) -> float:
        """è¨ˆç®—å¯¦é«”æå–åˆ†æ•¸"""
        if not expected_entities:
            return 1.0
        
        total_score = 0.0
        entity_count = 0
        
        for entity_type, expected_values in expected_entities.items():
            if not expected_values:
                continue
                
            entity_count += 1
            predicted_values = getattr(structured_intent, entity_type, [])
            
            if isinstance(predicted_values, str):
                predicted_values = [predicted_values]
            elif predicted_values is None:
                predicted_values = []
            
            # è¨ˆç®—äº¤é›†æ¯”ä¾‹
            if predicted_values:
                intersection = set(expected_values) & set(predicted_values)
                entity_score = len(intersection) / len(expected_values)
                total_score += entity_score
        
        return total_score / entity_count if entity_count > 0 else 1.0
    
    def _calculate_benchmark_metrics(self) -> None:
        """è¨ˆç®—åŸºæº–æ¸¬è©¦æŒ‡æ¨™"""
        print("\nğŸ“Š è¨ˆç®—åŸºæº–æ¸¬è©¦æŒ‡æ¨™...")
        
        # æŒ‰æä¾›å•†å’Œé¡åˆ¥åˆ†çµ„
        grouped_results = defaultdict(lambda: defaultdict(list))
        
        for result in self.results:
            category = BenchmarkCategory(result.metadata.get("test_category", "accuracy"))
            grouped_results[result.provider][category].append(result)
        
        # è¨ˆç®—æ¯å€‹æä¾›å•†æ¯å€‹é¡åˆ¥çš„æŒ‡æ¨™
        for provider, category_results in grouped_results.items():
            if provider not in self.metrics:
                self.metrics[provider] = {}
            
            for category, results in category_results.items():
                metrics = self._calculate_category_metrics(provider, category, results)
                self.metrics[provider][category] = metrics
        
        print(f"   âœ… è¨ˆç®—å®Œæˆï¼Œæ¶µè“‹ {len(self.metrics)} å€‹æä¾›å•†")
    
    def _calculate_category_metrics(self, provider: str, category: BenchmarkCategory, 
                                   results: List[BenchmarkResult]) -> BenchmarkMetrics:
        """è¨ˆç®—ç‰¹å®šé¡åˆ¥çš„æŒ‡æ¨™"""
        if not results:
            return BenchmarkMetrics(
                provider=provider,
                category=category,
                total_tests=0,
                passed_tests=0,
                failed_tests=0,
                accuracy=0.0,
                weighted_accuracy=0.0,
                avg_response_time=0.0,
                median_response_time=0.0,
                p95_response_time=0.0,
                avg_confidence=0.0,
                confidence_std=0.0,
                error_rate=1.0,
                consistency_score=0.0,
                throughput=0.0,
                difficulty_breakdown={},
                language_breakdown={}
            )
        
        # åŸºæœ¬çµ±è¨ˆ
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r.success)
        failed_tests = total_tests - passed_tests
        
        # æˆåŠŸçš„çµæœ
        successful_results = [r for r in results if r.success]
        
        if not successful_results:
            accuracy = 0.0
            weighted_accuracy = 0.0
            avg_response_time = 0.0
            median_response_time = 0.0
            p95_response_time = 0.0
            avg_confidence = 0.0
            confidence_std = 0.0
            consistency_score = 0.0
            throughput = 0.0
        else:
            # æº–ç¢ºç‡
            accuracy_scores = [r.accuracy_score for r in successful_results]
            accuracy = statistics.mean(accuracy_scores)
            
            # åŠ æ¬Šæº–ç¢ºç‡
            weights = [r.metadata.get("test_weight", 1.0) for r in successful_results]
            weighted_accuracy = sum(a * w for a, w in zip(accuracy_scores, weights)) / sum(weights)
            
            # éŸ¿æ‡‰æ™‚é–“
            response_times = [r.response_time for r in successful_results]
            avg_response_time = statistics.mean(response_times)
            median_response_time = statistics.median(response_times)
            p95_response_time = np.percentile(response_times, 95)
            
            # ç½®ä¿¡åº¦
            confidences = [r.confidence for r in successful_results]
            avg_confidence = statistics.mean(confidences)
            confidence_std = statistics.stdev(confidences) if len(confidences) > 1 else 0.0
            
            # ä¸€è‡´æ€§åˆ†æ•¸ï¼ˆé‡å°ä¸€è‡´æ€§æ¸¬è©¦ï¼‰
            consistency_score = self._calculate_consistency_score_for_category(successful_results)
            
            # ååé‡ï¼ˆæ¯ç§’è™•ç†çš„æŸ¥è©¢æ•¸ï¼‰
            total_time = sum(response_times)
            throughput = len(successful_results) / total_time if total_time > 0 else 0.0
        
        # éŒ¯èª¤ç‡
        error_rate = failed_tests / total_tests
        
        # é›£åº¦åˆ†è§£
        difficulty_breakdown = self._calculate_difficulty_breakdown(results)
        
        # èªè¨€åˆ†è§£
        language_breakdown = self._calculate_language_breakdown(results)
        
        return BenchmarkMetrics(
            provider=provider,
            category=category,
            total_tests=total_tests,
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            accuracy=accuracy,
            weighted_accuracy=weighted_accuracy,
            avg_response_time=avg_response_time,
            median_response_time=median_response_time,
            p95_response_time=p95_response_time,
            avg_confidence=avg_confidence,
            confidence_std=confidence_std,
            error_rate=error_rate,
            consistency_score=consistency_score,
            throughput=throughput,
            difficulty_breakdown=difficulty_breakdown,
            language_breakdown=language_breakdown
        )
    
    def _calculate_consistency_score_for_category(self, results: List[BenchmarkResult]) -> float:
        """è¨ˆç®—é¡åˆ¥çš„ä¸€è‡´æ€§åˆ†æ•¸"""
        # æ‰¾å‡ºä¸€è‡´æ€§æ¸¬è©¦çµ„
        consistency_groups = defaultdict(list)
        
        for result in results:
            group = result.metadata.get("consistency_group")
            if group:
                consistency_groups[group].append(result)
        
        if not consistency_groups:
            return 1.0  # æ²’æœ‰ä¸€è‡´æ€§æ¸¬è©¦ï¼Œå‡è¨­å®Œå…¨ä¸€è‡´
        
        group_scores = []
        
        for group, group_results in consistency_groups.items():
            if len(group_results) < 2:
                continue
            
            # è¨ˆç®—è©²çµ„çš„ä¸€è‡´æ€§
            predictions = [r.predicted_job_related for r in group_results]
            confidences = [r.confidence for r in group_results]
            
            # é æ¸¬ä¸€è‡´æ€§
            prediction_consistency = len(set(predictions)) == 1
            
            # ç½®ä¿¡åº¦ä¸€è‡´æ€§
            confidence_std = statistics.stdev(confidences) if len(confidences) > 1 else 0.0
            confidence_consistency = max(0.0, 1.0 - confidence_std)
            
            group_score = prediction_consistency * 0.7 + confidence_consistency * 0.3
            group_scores.append(group_score)
        
        return statistics.mean(group_scores) if group_scores else 1.0
    
    def _calculate_difficulty_breakdown(self, results: List[BenchmarkResult]) -> Dict[str, float]:
        """è¨ˆç®—é›£åº¦åˆ†è§£"""
        difficulty_groups = defaultdict(list)
        
        for result in results:
            difficulty = result.metadata.get("test_difficulty", "easy")
            if result.success:
                difficulty_groups[difficulty].append(result.accuracy_score)
        
        breakdown = {}
        for difficulty, scores in difficulty_groups.items():
            breakdown[difficulty] = statistics.mean(scores) if scores else 0.0
        
        return breakdown
    
    def _calculate_language_breakdown(self, results: List[BenchmarkResult]) -> Dict[str, float]:
        """è¨ˆç®—èªè¨€åˆ†è§£"""
        language_groups = defaultdict(list)
        
        for result in results:
            language = result.metadata.get("test_language", "zh_tw")
            if result.success:
                language_groups[language].append(result.accuracy_score)
        
        breakdown = {}
        for language, scores in language_groups.items():
            breakdown[language] = statistics.mean(scores) if scores else 0.0
        
        return breakdown
    
    def _generate_benchmark_report(self, total_time: float) -> Dict[str, Any]:
        """ç”ŸæˆåŸºæº–æ¸¬è©¦å ±å‘Š"""
        return {
            "benchmark_summary": {
                "timestamp": datetime.now().isoformat(),
                "total_time": total_time,
                "total_providers": len(set(r.provider for r in self.results)),
                "total_test_cases": len(set(r.test_case_id for r in self.results)),
                "total_executions": len(self.results),
                "overall_success_rate": sum(1 for r in self.results if r.success) / len(self.results) if self.results else 0.0
            },
            "provider_metrics": {
                provider: {
                    category.value: asdict(metrics) for category, metrics in category_metrics.items()
                } for provider, category_metrics in self.metrics.items()
            },
            "category_analysis": self._analyze_categories(),
            "provider_rankings": self._generate_provider_rankings(),
            "recommendations": self._generate_benchmark_recommendations(),
            "detailed_results": [asdict(result) for result in self.results]
        }
    
    def _analyze_categories(self) -> Dict[str, Any]:
        """åˆ†æå„é¡åˆ¥çš„æ•´é«”è¡¨ç¾"""
        category_analysis = {}
        
        for category in BenchmarkCategory:
            category_results = [r for r in self.results if r.metadata.get("test_category") == category.value]
            
            if not category_results:
                continue
            
            successful_results = [r for r in category_results if r.success]
            
            analysis = {
                "total_tests": len(category_results),
                "success_rate": len(successful_results) / len(category_results),
                "avg_accuracy": statistics.mean([r.accuracy_score for r in successful_results]) if successful_results else 0.0,
                "avg_response_time": statistics.mean([r.response_time for r in successful_results]) if successful_results else 0.0,
                "provider_performance": {}
            }
            
            # å„æä¾›å•†åœ¨è©²é¡åˆ¥çš„è¡¨ç¾
            provider_results = defaultdict(list)
            for result in successful_results:
                provider_results[result.provider].append(result)
            
            for provider, results in provider_results.items():
                analysis["provider_performance"][provider] = {
                    "accuracy": statistics.mean([r.accuracy_score for r in results]),
                    "response_time": statistics.mean([r.response_time for r in results]),
                    "test_count": len(results)
                }
            
            category_analysis[category.value] = analysis
        
        return category_analysis
    
    def _generate_provider_rankings(self) -> Dict[str, Any]:
        """ç”Ÿæˆæä¾›å•†æ’å"""
        if not self.metrics:
            return {}
        
        # è¨ˆç®—ç¶œåˆåˆ†æ•¸
        provider_scores = {}
        
        for provider, category_metrics in self.metrics.items():
            total_score = 0.0
            total_weight = 0.0
            
            for category, metrics in category_metrics.items():
                # é¡åˆ¥æ¬Šé‡
                category_weight = {
                    BenchmarkCategory.ACCURACY: 0.3,
                    BenchmarkCategory.SPEED: 0.2,
                    BenchmarkCategory.CONSISTENCY: 0.2,
                    BenchmarkCategory.ROBUSTNESS: 0.15,
                    BenchmarkCategory.MULTILINGUAL: 0.1,
                    BenchmarkCategory.EDGE_CASES: 0.05
                }.get(category, 0.1)
                
                # è¨ˆç®—é¡åˆ¥åˆ†æ•¸
                accuracy_score = metrics.weighted_accuracy
                speed_score = min(1.0, 5.0 / max(metrics.avg_response_time, 0.1))  # 5ç§’ç‚ºåŸºæº–
                consistency_score = metrics.consistency_score
                error_penalty = 1.0 - metrics.error_rate
                
                category_score = (accuracy_score * 0.4 + speed_score * 0.3 + 
                                consistency_score * 0.2 + error_penalty * 0.1)
                
                total_score += category_score * category_weight
                total_weight += category_weight
            
            provider_scores[provider] = total_score / total_weight if total_weight > 0 else 0.0
        
        # æ’å
        ranked_providers = sorted(provider_scores.items(), key=lambda x: x[1], reverse=True)
        
        return {
            "overall_ranking": [
                {"provider": provider, "score": score, "rank": i+1}
                for i, (provider, score) in enumerate(ranked_providers)
            ],
            "category_leaders": self._find_category_leaders(),
            "detailed_scores": provider_scores
        }
    
    def _find_category_leaders(self) -> Dict[str, str]:
        """æ‰¾å‡ºå„é¡åˆ¥çš„é ˜å…ˆè€…"""
        category_leaders = {}
        
        for category in BenchmarkCategory:
            best_provider = None
            best_score = -1.0
            
            for provider, category_metrics in self.metrics.items():
                if category in category_metrics:
                    metrics = category_metrics[category]
                    score = metrics.weighted_accuracy
                    
                    if score > best_score:
                        best_score = score
                        best_provider = provider
            
            if best_provider:
                category_leaders[category.value] = best_provider
        
        return category_leaders
    
    def _generate_benchmark_recommendations(self) -> Dict[str, Any]:
        """ç”ŸæˆåŸºæº–æ¸¬è©¦å»ºè­°"""
        if not self.metrics:
            return {}
        
        recommendations = {
            "best_overall": None,
            "best_for_accuracy": None,
            "best_for_speed": None,
            "best_for_consistency": None,
            "usage_recommendations": {},
            "improvement_suggestions": []
        }
        
        # æ‰¾å‡ºå„æ–¹é¢çš„æœ€ä½³æä¾›å•†
        provider_rankings = self._generate_provider_rankings()
        if provider_rankings.get("overall_ranking"):
            recommendations["best_overall"] = provider_rankings["overall_ranking"][0]["provider"]
        
        # å„é¡åˆ¥æœ€ä½³
        category_leaders = provider_rankings.get("category_leaders", {})
        recommendations["best_for_accuracy"] = category_leaders.get("accuracy")
        recommendations["best_for_speed"] = category_leaders.get("speed")
        recommendations["best_for_consistency"] = category_leaders.get("consistency")
        
        # ä½¿ç”¨å ´æ™¯å»ºè­°
        recommendations["usage_recommendations"] = {
            "ç”Ÿç”¢ç’°å¢ƒ": recommendations["best_overall"],
            "å³æ™‚æ‡‰ç”¨": recommendations["best_for_speed"],
            "é«˜æº–ç¢ºç‡éœ€æ±‚": recommendations["best_for_accuracy"],
            "ç©©å®šæ€§è¦æ±‚": recommendations["best_for_consistency"]
        }
        
        # æ”¹é€²å»ºè­°
        for provider, category_metrics in self.metrics.items():
            for category, metrics in category_metrics.items():
                if metrics.error_rate > 0.1:
                    recommendations["improvement_suggestions"].append(
                        f"{provider}åœ¨{category.value}é¡åˆ¥çš„éŒ¯èª¤ç‡è¼ƒé«˜({metrics.error_rate:.1%})ï¼Œå»ºè­°æª¢æŸ¥é…ç½®"
                    )
                
                if metrics.avg_response_time > 10.0:
                    recommendations["improvement_suggestions"].append(
                        f"{provider}åœ¨{category.value}é¡åˆ¥çš„éŸ¿æ‡‰æ™‚é–“è¼ƒæ…¢({metrics.avg_response_time:.1f}ç§’)ï¼Œå»ºè­°å„ªåŒ–"
                    )
                
                if metrics.consistency_score < 0.7:
                    recommendations["improvement_suggestions"].append(
                        f"{provider}åœ¨{category.value}é¡åˆ¥çš„ä¸€è‡´æ€§è¼ƒä½({metrics.consistency_score:.1%})ï¼Œå»ºè­°èª¿æ•´åƒæ•¸"
                    )
        
        return recommendations
    
    def _save_benchmark_results(self, report: Dict[str, Any]) -> None:
        """ä¿å­˜åŸºæº–æ¸¬è©¦çµæœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å®Œæ•´å ±å‘Š
        full_report_filename = f"llm_benchmark_full_report_{timestamp}.json"
        with open(full_report_filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ‘˜è¦å ±å‘Š
        summary_report = {
            "benchmark_summary": report["benchmark_summary"],
            "provider_rankings": report["provider_rankings"],
            "category_analysis": report["category_analysis"],
            "recommendations": report["recommendations"]
        }
        
        summary_filename = f"llm_benchmark_summary_{timestamp}.json"
        with open(summary_filename, 'w', encoding='utf-8') as f:
            json.dump(summary_report, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜CSVæ ¼å¼çš„çµæœ
        csv_filename = f"llm_benchmark_results_{timestamp}.csv"
        results_df = pd.DataFrame([asdict(result) for result in self.results])
        results_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
        
        print(f"\nğŸ’¾ åŸºæº–æ¸¬è©¦çµæœå·²ä¿å­˜:")
        print(f"   å®Œæ•´å ±å‘Š: {full_report_filename}")
        print(f"   æ‘˜è¦å ±å‘Š: {summary_filename}")
        print(f"   CSVçµæœ: {csv_filename}")
    
    def print_benchmark_summary(self) -> None:
        """æ‰“å°åŸºæº–æ¸¬è©¦æ‘˜è¦"""
        if not self.metrics:
            print("âŒ æ²’æœ‰åŸºæº–æ¸¬è©¦çµæœå¯é¡¯ç¤º")
            return
        
        print("\nğŸ“Š LLMåŸºæº–æ¸¬è©¦æ‘˜è¦")
        print("=" * 60)
        
        # åŸºæœ¬çµ±è¨ˆ
        total_tests = len(self.results)
        successful_tests = sum(1 for r in self.results if r.success)
        
        print(f"\nğŸ“‹ æ¸¬è©¦æ¦‚æ³:")
        print(f"   ç¸½æ¸¬è©¦æ¬¡æ•¸: {total_tests}")
        print(f"   æˆåŠŸæ¸¬è©¦: {successful_tests}")
        print(f"   æ•´é«”æˆåŠŸç‡: {successful_tests/total_tests*100:.1f}%")
        print(f"   æ¸¬è©¦æä¾›å•†: {len(set(r.provider for r in self.results))}")
        print(f"   æ¸¬è©¦é¡åˆ¥: {len(set(r.metadata.get('test_category') for r in self.results))}")
        
        # æä¾›å•†æ’å
        provider_rankings = self._generate_provider_rankings()
        if provider_rankings.get("overall_ranking"):
            print(f"\nğŸ† æä¾›å•†æ’å:")
            for rank_info in provider_rankings["overall_ranking"][:3]:  # é¡¯ç¤ºå‰3å
                print(f"   {rank_info['rank']}. {rank_info['provider']}: {rank_info['score']:.3f}")
        
        # é¡åˆ¥è¡¨ç¾
        print(f"\nğŸ“ˆ å„é¡åˆ¥è¡¨ç¾:")
        for provider, category_metrics in self.metrics.items():
            print(f"\n   {provider}:")
            for category, metrics in category_metrics.items():
                print(f"     {category.value}: æº–ç¢ºç‡={metrics.weighted_accuracy:.1%}, "
                      f"éŸ¿æ‡‰æ™‚é–“={metrics.avg_response_time:.2f}s, "
                      f"éŒ¯èª¤ç‡={metrics.error_rate:.1%}")
        
        # å»ºè­°
        recommendations = self._generate_benchmark_recommendations()
        if recommendations.get("best_overall"):
            print(f"\nğŸ’¡ å»ºè­°:")
            print(f"   æœ€ä½³æ•´é«”è¡¨ç¾: {recommendations['best_overall']}")
            print(f"   æœ€ä½³æº–ç¢ºç‡: {recommendations.get('best_for_accuracy', 'N/A')}")
            print(f"   æœ€ä½³é€Ÿåº¦: {recommendations.get('best_for_speed', 'N/A')}")
            print(f"   æœ€ä½³ä¸€è‡´æ€§: {recommendations.get('best_for_consistency', 'N/A')}")


def main():
    """ä¸»å‡½æ•¸ - åŸºæº–æ¸¬è©¦å·¥å…·å…¥å£é»"""
    print("ğŸš€ LLMåŸºæº–æ¸¬è©¦å¥—ä»¶")
    print("=" * 60)
    
    # å‰µå»ºæ¸¬è©¦å¥—ä»¶
    benchmark = LLMBenchmarkTestSuite()
    
    # é¸æ“‡æ¸¬è©¦æ¨¡å¼
    print("\nè«‹é¸æ“‡æ¸¬è©¦æ¨¡å¼:")
    print("1. å¿«é€Ÿæ¸¬è©¦ (æº–ç¢ºæ€§ + é€Ÿåº¦)")
    print("2. æ¨™æº–æ¸¬è©¦ (æº–ç¢ºæ€§ + é€Ÿåº¦ + ä¸€è‡´æ€§ + é­¯æ£’æ€§)")
    print("3. å…¨é¢æ¸¬è©¦ (æ‰€æœ‰é¡åˆ¥)")
    print("4. è‡ªå®šç¾©æ¸¬è©¦")
    
    try:
        choice = input("\nè«‹è¼¸å…¥é¸æ“‡ (1-4): ").strip()
        
        if choice == "1":
            categories = [BenchmarkCategory.ACCURACY, BenchmarkCategory.SPEED]
            providers = None  # ä½¿ç”¨æ‰€æœ‰å¯ç”¨æä¾›å•†
        elif choice == "2":
            categories = [
                BenchmarkCategory.ACCURACY,
                BenchmarkCategory.SPEED,
                BenchmarkCategory.CONSISTENCY,
                BenchmarkCategory.ROBUSTNESS
            ]
            providers = None
        elif choice == "3":
            categories = None  # ä½¿ç”¨æ‰€æœ‰é¡åˆ¥
            providers = None
        elif choice == "4":
            # è‡ªå®šç¾©é¸æ“‡
            print("\nå¯ç”¨çš„æ¸¬è©¦é¡åˆ¥:")
            for i, category in enumerate(BenchmarkCategory, 1):
                print(f"{i}. {category.value}")
            
            category_choices = input("è«‹è¼¸å…¥è¦æ¸¬è©¦çš„é¡åˆ¥ç·¨è™Ÿ (ç”¨é€—è™Ÿåˆ†éš”): ").strip()
            try:
                category_indices = [int(x.strip()) - 1 for x in category_choices.split(",")]
                categories = [list(BenchmarkCategory)[i] for i in category_indices]
            except (ValueError, IndexError):
                print("âŒ ç„¡æ•ˆçš„é¡åˆ¥é¸æ“‡ï¼Œä½¿ç”¨é»˜èªè¨­ç½®")
                categories = [BenchmarkCategory.ACCURACY, BenchmarkCategory.SPEED]
            
            providers = None  # å¯ä»¥é€²ä¸€æ­¥æ“´å±•æä¾›å•†é¸æ“‡
        else:
            print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œä½¿ç”¨æ¨™æº–æ¸¬è©¦")
            categories = [
                BenchmarkCategory.ACCURACY,
                BenchmarkCategory.SPEED,
                BenchmarkCategory.CONSISTENCY,
                BenchmarkCategory.ROBUSTNESS
            ]
            providers = None
        
        # é‹è¡ŒåŸºæº–æ¸¬è©¦
        report = benchmark.run_benchmark(providers=providers, categories=categories)
        
        # é¡¯ç¤ºæ‘˜è¦
        benchmark.print_benchmark_summary()
        
        print("\nâœ… åŸºæº–æ¸¬è©¦å®Œæˆï¼")
        print("è©³ç´°çµæœå·²ä¿å­˜åˆ°JSONå’ŒCSVæ–‡ä»¶ä¸­ã€‚")
        
    except KeyboardInterrupt:
        print("\nâŒ æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()