#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMæŒ‡ä»¤éµå¾ªèˆ‡çµæ§‹åŒ–è¼¸å‡ºæ¨™æº–åº« - æ¼”ç¤ºè…³æœ¬

æ­¤è…³æœ¬æ¼”ç¤ºäº†LLMæ¨™æº–åº«çš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. åŸºæœ¬çš„çµæ§‹åŒ–è¼¸å‡º
2. æƒ…æ„Ÿåˆ†æ
3. ä¿¡æ¯æå–
4. æ–‡æœ¬åˆ†é¡
5. æ‰¹é‡è™•ç†
6. å®¢æˆ¶ç«¯æ± è² è¼‰å‡è¡¡

ä½¿ç”¨æ–¹æ³•:
    python demo_llm_standard.py

ç’°å¢ƒè®Šé‡:
    OPENAI_API_KEY: OpenAI APIå¯†é‘°
    æˆ–ä½¿ç”¨LiteLLMä»£ç†: LITELLM_PROXY_URL=http://localhost:4000
"""

import os
import sys
import json
import time
from typing import Dict, List, Any

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# æ¨¡æ“¬LLMæ¨™æº–åº«ï¼ˆå¯¦éš›ä½¿ç”¨æ™‚æœƒå¾å·²å®‰è£çš„åŒ…å°å…¥ï¼‰
class MockStandardLLMClient:
    """æ¨¡æ“¬çš„LLMæ¨™æº–å®¢æˆ¶ç«¯ï¼ˆç”¨æ–¼æ¼”ç¤ºï¼‰"""
    
    def __init__(self, provider: str, api_key: str, model: str, **kwargs):
        self.provider = provider
        self.api_key = api_key
        self.model = model
        self.base_url = kwargs.get('base_url')
        self.timeout = kwargs.get('timeout', 30)
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'total_tokens': 0,
            'total_time': 0.0
        }
    
    def create_instruction(self, instruction_type: str, description: str, output_schema: Dict[str, Any], **kwargs) -> Dict[str, Any]:
        """å‰µå»ºæ¨™æº–æŒ‡ä»¤"""
        return {
            'instruction_type': instruction_type,
            'version': '1.0',
            'task': {
                'description': description,
                'context': kwargs.get('context', ''),
                'constraints': kwargs.get('constraints', [])
            },
            'output_schema': output_schema,
            'examples': kwargs.get('examples', []),
            'metadata': {
                'priority': kwargs.get('priority', 'medium'),
                'timeout': kwargs.get('timeout', self.timeout),
                'retry_count': kwargs.get('retry_count', 3)
            }
        }
    
    def execute(self, instruction: Dict[str, Any], user_input: str) -> Dict[str, Any]:
        """åŸ·è¡ŒæŒ‡ä»¤ï¼ˆæ¨¡æ“¬å¯¦ç¾ï¼‰"""
        start_time = time.time()
        self.stats['total_requests'] += 1
        
        try:
            # æ¨¡æ“¬APIèª¿ç”¨å»¶é²
            time.sleep(0.5 + (self.stats['total_requests'] % 3) * 0.2)
            
            # æ ¹æ“šæŒ‡ä»¤é¡å‹ç”Ÿæˆæ¨¡æ“¬éŸ¿æ‡‰
            mock_data = self._generate_mock_response(instruction, user_input)
            
            end_time = time.time()
            duration = end_time - start_time
            
            self.stats['successful_requests'] += 1
            self.stats['total_time'] += duration
            self.stats['total_tokens'] += len(user_input.split()) * 2  # æ¨¡æ“¬tokenè¨ˆç®—
            
            return {
                'status': 'success',
                'data': mock_data,
                'metadata': {
                    'provider': self.provider,
                    'model': self.model,
                    'response_time': duration,
                    'tokens_used': len(user_input.split()) * 2,
                    'timestamp': time.time()
                },
                'errors': []
            }
            
        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time
            
            return {
                'status': 'error',
                'data': None,
                'metadata': {
                    'provider': self.provider,
                    'model': self.model,
                    'response_time': duration,
                    'timestamp': time.time()
                },
                'errors': [str(e)]
            }
    
    def _generate_mock_response(self, instruction: Dict[str, Any], user_input: str) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨¡æ“¬éŸ¿æ‡‰æ•¸æ“š"""
        instruction_type = instruction['instruction_type']
        description = instruction['task']['description']
        
        # æƒ…æ„Ÿåˆ†æ
        if 'æƒ…æ„Ÿ' in description or 'sentiment' in description.lower():
            positive_words = ['å¥½', 'æ£’', 'æ»¿æ„', 'æ¨è–¦', 'å„ªç§€', 'å–œæ­¡', 'ä¸éŒ¯']
            negative_words = ['å·®', 'ç³Ÿ', 'å¤±æœ›', 'è²´', 'çˆ›', 'è¨å­', 'å•é¡Œ']
            
            positive_count = sum(1 for word in positive_words if word in user_input)
            negative_count = sum(1 for word in negative_words if word in user_input)
            
            if positive_count > negative_count:
                sentiment = 'positive'
                confidence = 0.8 + (positive_count * 0.05)
            elif negative_count > positive_count:
                sentiment = 'negative'
                confidence = 0.8 + (negative_count * 0.05)
            else:
                sentiment = 'neutral'
                confidence = 0.6
            
            confidence = min(confidence, 1.0)
            
            return {
                'sentiment': sentiment,
                'confidence': round(confidence, 2),
                'keywords': [word for word in positive_words + negative_words if word in user_input]
            }
        
        # ä¿¡æ¯æå–
        elif 'æå–' in description or 'extraction' in description.lower():
            import re
            
            # æå–å§“åï¼ˆä¸­æ–‡å§“åæ¨¡å¼ï¼‰
            name_pattern = r'[æˆ‘å«|å§“åæ˜¯|åå­—æ˜¯]?([\u4e00-\u9fa5]{2,4})'
            name_match = re.search(name_pattern, user_input)
            name = name_match.group(1) if name_match else 'æœªçŸ¥'
            
            # æå–å¹´é½¡
            age_pattern = r'(\d{1,2})æ­²'
            age_match = re.search(age_pattern, user_input)
            age = int(age_match.group(1)) if age_match else None
            
            # æå–éƒµç®±
            email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
            email_match = re.search(email_pattern, user_input)
            email = email_match.group(0) if email_match else None
            
            # æå–é›»è©±
            phone_pattern = r'[+]?[0-9-]{10,15}'
            phone_match = re.search(phone_pattern, user_input)
            phone = phone_match.group(0) if phone_match else None
            
            # æå–æŠ€èƒ½
            skill_keywords = ['Python', 'Java', 'JavaScript', 'C++', 'Go', 'Rust', 'æ©Ÿå™¨å­¸ç¿’', 'æ·±åº¦å­¸ç¿’', 
                            'Spring', 'æ•¸æ“šåº«', 'MySQL', 'PostgreSQL', 'Redis', 'Docker', 'Kubernetes']
            skills = [skill for skill in skill_keywords if skill in user_input]
            
            result = {'name': name}
            if age is not None:
                result['age'] = age
            if email:
                result['email'] = email
            if phone:
                result['phone'] = phone
            if skills:
                result['skills'] = skills
            
            return result
        
        # æ–‡æœ¬åˆ†é¡
        elif 'åˆ†é¡' in description or 'classification' in description.lower():
            tech_keywords = ['æŠ€è¡“', 'äººå·¥æ™ºèƒ½', 'AI', 'æ©Ÿå™¨å­¸ç¿’', 'æ·±åº¦å­¸ç¿’', 'ç®—æ³•', 'ç·¨ç¨‹', 'è»Ÿä»¶', 'ç¡¬ä»¶']
            business_keywords = ['å•†æ¥­', 'è‚¡å¸‚', 'æŠ•è³‡', 'ç¶“æ¿Ÿ', 'é‡‘è', 'å¸‚å ´', 'éŠ·å”®', 'ç®¡ç†']
            entertainment_keywords = ['å¨›æ¨‚', 'é›»å½±', 'éŸ³æ¨‚', 'éŠæˆ²', 'æ˜æ˜Ÿ', 'ç¶œè—', 'å°èªª']
            sports_keywords = ['é«”è‚²', 'é‹å‹•', 'è¶³çƒ', 'ç±ƒçƒ', 'ç¶²çƒ', 'æ¸¸æ³³', 'å¥èº«', 'æ¯”è³½']
            health_keywords = ['å¥åº·', 'é†«ç™‚', 'ç–¾ç—…', 'æ²»ç™‚', 'è—¥ç‰©', 'é†«é™¢', 'é†«ç”Ÿ', 'é¤Šç”Ÿ']
            
            categories = {
                'æŠ€è¡“': tech_keywords,
                'å•†æ¥­': business_keywords,
                'å¨›æ¨‚': entertainment_keywords,
                'é«”è‚²': sports_keywords,
                'å¥åº·': health_keywords
            }
            
            max_score = 0
            best_category = 'å…¶ä»–'
            
            for category, keywords in categories.items():
                score = sum(1 for keyword in keywords if keyword in user_input)
                if score > max_score:
                    max_score = score
                    best_category = category
            
            confidence = min(0.6 + (max_score * 0.1), 0.95) if max_score > 0 else 0.3
            
            return {
                'category': best_category,
                'confidence': round(confidence, 2),
                'subcategory': f'{best_category}ç›¸é—œ' if best_category != 'å…¶ä»–' else 'æœªåˆ†é¡'
            }
        
        # é»˜èªéŸ¿æ‡‰
        else:
            return {
                'message': f'å·²è™•ç†è¼¸å…¥: {user_input[:50]}...',
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'provider': self.provider
            }
    
    def batch_execute(self, instruction: Dict[str, Any], inputs: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡åŸ·è¡ŒæŒ‡ä»¤"""
        results = []
        for user_input in inputs:
            result = self.execute(instruction, user_input)
            results.append(result)
        return results
    
    def health_check(self) -> Dict[str, Any]:
        """å¥åº·æª¢æŸ¥"""
        return {
            'status': 'healthy',
            'provider': self.provider,
            'model': self.model,
            'timestamp': time.time()
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆä¿¡æ¯"""
        if self.stats['total_requests'] > 0:
            success_rate = self.stats['successful_requests'] / self.stats['total_requests']
            avg_response_time = self.stats['total_time'] / self.stats['total_requests']
        else:
            success_rate = 0.0
            avg_response_time = 0.0
        
        return {
            'total_requests': self.stats['total_requests'],
            'successful_requests': self.stats['successful_requests'],
            'success_rate': success_rate,
            'avg_response_time': avg_response_time,
            'total_tokens': self.stats['total_tokens']
        }


class MockLLMClientPool:
    """æ¨¡æ“¬çš„å®¢æˆ¶ç«¯æ± """
    
    def __init__(self, clients: List[MockStandardLLMClient]):
        self.clients = clients
        self.current_index = 0
        self.pool_stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'total_time': 0.0
        }
    
    def execute(self, instruction: Dict[str, Any], user_input: str) -> Dict[str, Any]:
        """ä½¿ç”¨è² è¼‰å‡è¡¡åŸ·è¡ŒæŒ‡ä»¤"""
        start_time = time.time()
        self.pool_stats['total_requests'] += 1
        
        # è¼ªè©¢é¸æ“‡å®¢æˆ¶ç«¯
        client = self.clients[self.current_index]
        self.current_index = (self.current_index + 1) % len(self.clients)
        
        try:
            result = client.execute(instruction, user_input)
            
            if result['status'] == 'success':
                self.pool_stats['successful_requests'] += 1
            
            end_time = time.time()
            self.pool_stats['total_time'] += (end_time - start_time)
            
            return result
            
        except Exception as e:
            # æ•…éšœè½‰ç§»åˆ°ä¸‹ä¸€å€‹å®¢æˆ¶ç«¯
            if len(self.clients) > 1:
                fallback_client = self.clients[self.current_index]
                self.current_index = (self.current_index + 1) % len(self.clients)
                return fallback_client.execute(instruction, user_input)
            else:
                raise e
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–æ± çµ±è¨ˆä¿¡æ¯"""
        if self.pool_stats['total_requests'] > 0:
            success_rate = self.pool_stats['successful_requests'] / self.pool_stats['total_requests']
            avg_response_time = self.pool_stats['total_time'] / self.pool_stats['total_requests']
        else:
            success_rate = 0.0
            avg_response_time = 0.0
        
        return {
            'total_requests': self.pool_stats['total_requests'],
            'successful_requests': self.pool_stats['successful_requests'],
            'success_rate': success_rate,
            'avg_response_time': avg_response_time,
            'active_clients': len(self.clients)
        }


def demo_basic_usage():
    """æ¼”ç¤ºåŸºæœ¬ä½¿ç”¨"""
    print("\nğŸš€ æ¼”ç¤º1: åŸºæœ¬çµæ§‹åŒ–è¼¸å‡º")
    print("-" * 50)
    
    # å‰µå»ºå®¢æˆ¶ç«¯
    client = MockStandardLLMClient(
        provider="openai",
        api_key="demo-key",
        model="gpt-3.5-turbo"
    )
    
    # å‰µå»ºç°¡å–®æŒ‡ä»¤
    instruction = client.create_instruction(
        instruction_type="structured_output",
        description="ç”Ÿæˆä¸€å€‹å‹å¥½çš„å•å€™æ¶ˆæ¯",
        output_schema={
            "type": "object",
            "properties": {
                "message": {"type": "string"},
                "timestamp": {"type": "string"},
                "provider": {"type": "string"}
            },
            "required": ["message"]
        }
    )
    
    # åŸ·è¡ŒæŒ‡ä»¤
    response = client.execute(instruction, "è«‹å‘æˆ‘å•å¥½")
    
    if response['status'] == 'success':
        data = response['data']
        print(f"âœ… åŸ·è¡ŒæˆåŠŸ!")
        print(f"   æ¶ˆæ¯: {data['message']}")
        print(f"   æ™‚é–“æˆ³: {data.get('timestamp', 'N/A')}")
        print(f"   æä¾›å•†: {data.get('provider', 'N/A')}")
        print(f"   éŸ¿æ‡‰æ™‚é–“: {response['metadata']['response_time']:.2f}ç§’")
    else:
        print(f"âŒ åŸ·è¡Œå¤±æ•—: {response['errors']}")


def demo_sentiment_analysis():
    """æ¼”ç¤ºæƒ…æ„Ÿåˆ†æ"""
    print("\nğŸ’­ æ¼”ç¤º2: æƒ…æ„Ÿåˆ†æ")
    print("-" * 50)
    
    client = MockStandardLLMClient(
        provider="anthropic",
        api_key="demo-key",
        model="claude-3-haiku"
    )
    
    # å‰µå»ºæƒ…æ„Ÿåˆ†ææŒ‡ä»¤
    instruction = client.create_instruction(
        instruction_type="text_analysis",
        description="åˆ†ææ–‡æœ¬çš„æƒ…æ„Ÿå‚¾å‘",
        output_schema={
            "type": "object",
            "properties": {
                "sentiment": {
                    "type": "string",
                    "enum": ["positive", "negative", "neutral"]
                },
                "confidence": {
                    "type": "number",
                    "minimum": 0,
                    "maximum": 1
                },
                "keywords": {
                    "type": "array",
                    "items": {"type": "string"}
                }
            },
            "required": ["sentiment", "confidence"]
        }
    )
    
    # æ¸¬è©¦ä¸åŒæƒ…æ„Ÿçš„æ–‡æœ¬
    test_texts = [
        "é€™å€‹ç”¢å“çœŸçš„å¾ˆæ£’ï¼Œæˆ‘éå¸¸æ»¿æ„ï¼",
        "æœå‹™æ…‹åº¦ä¸€èˆ¬ï¼Œé‚„æœ‰æ”¹é€²ç©ºé–“ã€‚",
        "åƒ¹æ ¼å¤ªè²´äº†ï¼Œæ€§åƒ¹æ¯”ä¸é«˜ã€‚",
        "è³ªé‡ä¸éŒ¯ï¼Œå€¼å¾—æ¨è–¦çµ¦æœ‹å‹ã€‚"
    ]
    
    for i, text in enumerate(test_texts, 1):
        print(f"\nğŸ“ æ¸¬è©¦æ–‡æœ¬ {i}: {text}")
        response = client.execute(instruction, text)
        
        if response['status'] == 'success':
            data = response['data']
            sentiment_emoji = {
                'positive': 'ğŸ˜Š',
                'negative': 'ğŸ˜',
                'neutral': 'ğŸ˜'
            }
            
            print(f"   æƒ…æ„Ÿ: {data['sentiment']} {sentiment_emoji.get(data['sentiment'], '')}")
            print(f"   ç½®ä¿¡åº¦: {data['confidence']:.2f}")
            if data.get('keywords'):
                print(f"   é—œéµè©: {', '.join(data['keywords'])}")
        else:
            print(f"   âŒ åˆ†æå¤±æ•—: {response['errors']}")


def demo_information_extraction():
    """æ¼”ç¤ºä¿¡æ¯æå–"""
    print("\nğŸ” æ¼”ç¤º3: ä¿¡æ¯æå–")
    print("-" * 50)
    
    client = MockStandardLLMClient(
        provider="google",
        api_key="demo-key",
        model="gemini-pro"
    )
    
    # å‰µå»ºä¿¡æ¯æå–æŒ‡ä»¤
    instruction = client.create_instruction(
        instruction_type="extraction",
        description="å¾æ–‡æœ¬ä¸­æå–å€‹äººä¿¡æ¯",
        output_schema={
            "type": "object",
            "properties": {
                "name": {"type": "string"},
                "age": {"type": "integer", "minimum": 0, "maximum": 150},
                "email": {"type": "string"},
                "phone": {"type": "string"},
                "skills": {
                    "type": "array",
                    "items": {"type": "string"}
                }
            },
            "required": ["name"]
        }
    )
    
    # æ¸¬è©¦æ–‡æœ¬
    test_texts = [
        "æˆ‘å«å¼µä¸‰ï¼Œä»Šå¹´28æ­²ï¼Œéƒµç®±æ˜¯zhangsan@example.comï¼Œé›»è©±æ˜¯+86-13812345678ï¼Œæ“…é•·Pythonå’Œæ©Ÿå™¨å­¸ç¿’ã€‚",
        "æå››ï¼Œ25æ­²ï¼Œè¯ç¹«æ–¹å¼ï¼šlisi@company.comï¼Œæ‰‹æ©Ÿï¼š+86-15987654321ï¼ŒæŠ€èƒ½ï¼šJavaã€Springã€æ•¸æ“šåº«è¨­è¨ˆã€‚"
    ]
    
    for i, text in enumerate(test_texts, 1):
        print(f"\nğŸ“„ æ¸¬è©¦æ–‡æœ¬ {i}: {text}")
        response = client.execute(instruction, text)
        
        if response['status'] == 'success':
            data = response['data']
            print(f"   å§“å: {data.get('name', 'N/A')}")
            print(f"   å¹´é½¡: {data.get('age', 'N/A')}")
            print(f"   éƒµç®±: {data.get('email', 'N/A')}")
            print(f"   é›»è©±: {data.get('phone', 'N/A')}")
            if data.get('skills'):
                print(f"   æŠ€èƒ½: {', '.join(data['skills'])}")
        else:
            print(f"   âŒ æå–å¤±æ•—: {response['errors']}")


def demo_text_classification():
    """æ¼”ç¤ºæ–‡æœ¬åˆ†é¡"""
    print("\nğŸ“Š æ¼”ç¤º4: æ–‡æœ¬åˆ†é¡")
    print("-" * 50)
    
    client = MockStandardLLMClient(
        provider="deepseek",
        api_key="demo-key",
        model="deepseek-chat"
    )
    
    # å‰µå»ºåˆ†é¡æŒ‡ä»¤
    instruction = client.create_instruction(
        instruction_type="classification",
        description="å°æ–‡æœ¬é€²è¡Œä¸»é¡Œåˆ†é¡",
        output_schema={
            "type": "object",
            "properties": {
                "category": {
                    "type": "string",
                    "enum": ["æŠ€è¡“", "å•†æ¥­", "å¨›æ¨‚", "é«”è‚²", "å¥åº·", "å…¶ä»–"]
                },
                "subcategory": {"type": "string"},
                "confidence": {
                    "type": "number",
                    "minimum": 0,
                    "maximum": 1
                }
            },
            "required": ["category", "confidence"]
        }
    )
    
    # æ¸¬è©¦æ–‡æœ¬
    test_texts = [
        "äººå·¥æ™ºèƒ½æŠ€è¡“åœ¨é†«ç™‚è¨ºæ–·ä¸­çš„æ‡‰ç”¨è¶Šä¾†è¶Šå»£æ³›ã€‚",
        "ä»Šå¹´çš„è‚¡å¸‚è¡¨ç¾è¶…å‡ºäº†å¤§å¤šæ•¸æŠ•è³‡è€…çš„é æœŸã€‚",
        "æ–°ä¸Šæ˜ çš„ç§‘å¹»é›»å½±ç²å¾—äº†è§€çœ¾çš„ä¸€è‡´å¥½è©•ã€‚",
        "è·æ¥­ç±ƒçƒè¯è³½çš„å­£å¾Œè³½å³å°‡é–‹å§‹ã€‚",
        "ç ”ç©¶è¡¨æ˜ï¼Œå®šæœŸé‹å‹•æœ‰åŠ©æ–¼æé«˜å…ç–«åŠ›ã€‚"
    ]
    
    for i, text in enumerate(test_texts, 1):
        print(f"\nğŸ“° æ¸¬è©¦æ–‡æœ¬ {i}: {text}")
        response = client.execute(instruction, text)
        
        if response['status'] == 'success':
            data = response['data']
            category_emoji = {
                'æŠ€è¡“': 'ğŸ’»',
                'å•†æ¥­': 'ğŸ’¼',
                'å¨›æ¨‚': 'ğŸ¬',
                'é«”è‚²': 'âš½',
                'å¥åº·': 'ğŸ¥',
                'å…¶ä»–': 'ğŸ“'
            }
            
            print(f"   åˆ†é¡: {data['category']} {category_emoji.get(data['category'], '')}")
            print(f"   å­åˆ†é¡: {data.get('subcategory', 'N/A')}")
            print(f"   ç½®ä¿¡åº¦: {data['confidence']:.2f}")
        else:
            print(f"   âŒ åˆ†é¡å¤±æ•—: {response['errors']}")


def demo_batch_processing():
    """æ¼”ç¤ºæ‰¹é‡è™•ç†"""
    print("\nğŸ“¦ æ¼”ç¤º5: æ‰¹é‡è™•ç†")
    print("-" * 50)
    
    client = MockStandardLLMClient(
        provider="openai",
        api_key="demo-key",
        model="gpt-4"
    )
    
    # å‰µå»ºæ‰¹é‡æƒ…æ„Ÿåˆ†ææŒ‡ä»¤
    instruction = client.create_instruction(
        instruction_type="text_analysis",
        description="æ‰¹é‡åˆ†æç”¨æˆ¶è©•è«–çš„æƒ…æ„Ÿ",
        output_schema={
            "type": "object",
            "properties": {
                "sentiment": {
                    "type": "string",
                    "enum": ["positive", "negative", "neutral"]
                },
                "confidence": {
                    "type": "number",
                    "minimum": 0,
                    "maximum": 1
                }
            },
            "required": ["sentiment", "confidence"]
        }
    )
    
    # æ‰¹é‡è¼¸å…¥
    batch_inputs = [
        "ç”¢å“è³ªé‡å¾ˆå¥½ï¼Œå€¼å¾—è³¼è²·ï¼",
        "é…é€é€Ÿåº¦å¤ªæ…¢äº†ï¼Œä¸æ»¿æ„ã€‚",
        "åƒ¹æ ¼åˆç†ï¼Œæœå‹™ä¸€èˆ¬ã€‚",
        "åŒ…è£ç²¾ç¾ï¼Œè¶…å‡ºé æœŸï¼",
        "å®¢æœæ…‹åº¦å¾ˆå·®ï¼Œé«”é©—ä¸å¥½ã€‚"
    ]
    
    print(f"ğŸ“ è™•ç† {len(batch_inputs)} æ¢ç”¨æˆ¶è©•è«–...")
    
    start_time = time.time()
    responses = client.batch_execute(instruction, batch_inputs)
    end_time = time.time()
    
    batch_duration = end_time - start_time
    
    print(f"\nâ±ï¸ æ‰¹é‡è™•ç†å®Œæˆï¼Œè€—æ™‚: {batch_duration:.2f}ç§’")
    print(f"ğŸ“Š å¹³å‡æ¯æ¢: {batch_duration/len(batch_inputs):.2f}ç§’")
    
    # çµ±è¨ˆçµæœ
    sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}
    
    for i, (input_text, response) in enumerate(zip(batch_inputs, responses), 1):
        print(f"\n{i}. {input_text}")
        if response['status'] == 'success':
            data = response['data']
            sentiment = data['sentiment']
            confidence = data['confidence']
            sentiment_counts[sentiment] += 1
            
            emoji = {'positive': 'ğŸ˜Š', 'negative': 'ğŸ˜', 'neutral': 'ğŸ˜'}
            print(f"   â†’ {sentiment} {emoji[sentiment]} (ç½®ä¿¡åº¦: {confidence:.2f})")
        else:
            print(f"   â†’ âŒ è™•ç†å¤±æ•—: {response['errors']}")
    
    print(f"\nğŸ“ˆ æƒ…æ„Ÿåˆ†å¸ƒçµ±è¨ˆ:")
    for sentiment, count in sentiment_counts.items():
        percentage = (count / len(batch_inputs)) * 100
        print(f"   {sentiment}: {count} æ¢ ({percentage:.1f}%)")


def demo_client_pool():
    """æ¼”ç¤ºå®¢æˆ¶ç«¯æ± è² è¼‰å‡è¡¡"""
    print("\nâš–ï¸ æ¼”ç¤º6: å®¢æˆ¶ç«¯æ± è² è¼‰å‡è¡¡")
    print("-" * 50)
    
    # å‰µå»ºå¤šå€‹å®¢æˆ¶ç«¯
    clients = [
        MockStandardLLMClient(provider="openai", api_key="key1", model="gpt-4"),
        MockStandardLLMClient(provider="anthropic", api_key="key2", model="claude-3-haiku"),
        MockStandardLLMClient(provider="google", api_key="key3", model="gemini-pro")
    ]
    
    # å‰µå»ºå®¢æˆ¶ç«¯æ± 
    pool = MockLLMClientPool(clients)
    
    print(f"ğŸ”§ å‰µå»ºäº†åŒ…å« {len(clients)} å€‹å®¢æˆ¶ç«¯çš„æ± ")
    
    # å‰µå»ºæ¸¬è©¦æŒ‡ä»¤
    instruction = clients[0].create_instruction(
        instruction_type="structured_output",
        description="æ¸¬è©¦è² è¼‰å‡è¡¡",
        output_schema={
            "type": "object",
            "properties": {
                "message": {"type": "string"},
                "provider": {"type": "string"}
            },
            "required": ["message"]
        }
    )
    
    # åŸ·è¡Œå¤šæ¬¡è«‹æ±‚æ¸¬è©¦è² è¼‰å‡è¡¡
    test_requests = [f"æ¸¬è©¦è«‹æ±‚ {i+1}" for i in range(6)]
    
    print(f"\nğŸ”„ åŸ·è¡Œ {len(test_requests)} å€‹è«‹æ±‚æ¸¬è©¦è² è¼‰å‡è¡¡...")
    
    start_time = time.time()
    
    for i, request in enumerate(test_requests, 1):
        response = pool.execute(instruction, request)
        
        if response['status'] == 'success':
            provider = response['metadata']['provider']
            response_time = response['metadata']['response_time']
            print(f"   {i}. {request} â†’ {provider} (è€—æ™‚: {response_time:.2f}ç§’)")
        else:
            print(f"   {i}. {request} â†’ âŒ å¤±æ•—")
    
    end_time = time.time()
    total_time = end_time - start_time
    
    # ç²å–æ± çµ±è¨ˆä¿¡æ¯
    pool_stats = pool.get_stats()
    
    print(f"\nğŸ“Š å®¢æˆ¶ç«¯æ± çµ±è¨ˆ:")
    print(f"   ç¸½è«‹æ±‚æ•¸: {pool_stats['total_requests']}")
    print(f"   æˆåŠŸè«‹æ±‚æ•¸: {pool_stats['successful_requests']}")
    print(f"   æˆåŠŸç‡: {pool_stats['success_rate']:.2%}")
    print(f"   å¹³å‡éŸ¿æ‡‰æ™‚é–“: {pool_stats['avg_response_time']:.2f}ç§’")
    print(f"   ç¸½è€—æ™‚: {total_time:.2f}ç§’")
    
    # é¡¯ç¤ºå„å®¢æˆ¶ç«¯çš„ä½¿ç”¨æƒ…æ³
    print(f"\nğŸ” å„å®¢æˆ¶ç«¯ä½¿ç”¨æƒ…æ³:")
    for i, client in enumerate(clients, 1):
        stats = client.get_stats()
        print(f"   å®¢æˆ¶ç«¯ {i} ({client.provider}): {stats['total_requests']} è«‹æ±‚")


def demo_performance_monitoring():
    """æ¼”ç¤ºæ€§èƒ½ç›£æ§"""
    print("\nğŸ“ˆ æ¼”ç¤º7: æ€§èƒ½ç›£æ§")
    print("-" * 50)
    
    client = MockStandardLLMClient(
        provider="openai",
        api_key="demo-key",
        model="gpt-3.5-turbo"
    )
    
    # å‰µå»ºæ¸¬è©¦æŒ‡ä»¤
    instruction = client.create_instruction(
        instruction_type="structured_output",
        description="æ€§èƒ½æ¸¬è©¦æŒ‡ä»¤",
        output_schema={
            "type": "object",
            "properties": {
                "result": {"type": "string"}
            },
            "required": ["result"]
        }
    )
    
    # åŸ·è¡Œå¤šæ¬¡è«‹æ±‚æ”¶é›†æ€§èƒ½æ•¸æ“š
    print("ğŸ”„ åŸ·è¡Œæ€§èƒ½æ¸¬è©¦...")
    
    test_count = 10
    response_times = []
    
    for i in range(test_count):
        start_time = time.time()
        response = client.execute(instruction, f"æ€§èƒ½æ¸¬è©¦ {i+1}")
        end_time = time.time()
        
        if response['status'] == 'success':
            response_time = response['metadata']['response_time']
            response_times.append(response_time)
            print(f"   è«‹æ±‚ {i+1}: {response_time:.3f}ç§’")
        else:
            print(f"   è«‹æ±‚ {i+1}: å¤±æ•—")
    
    # è¨ˆç®—æ€§èƒ½çµ±è¨ˆ
    if response_times:
        avg_time = sum(response_times) / len(response_times)
        min_time = min(response_times)
        max_time = max(response_times)
        
        print(f"\nğŸ“Š æ€§èƒ½çµ±è¨ˆ:")
        print(f"   ç¸½è«‹æ±‚æ•¸: {test_count}")
        print(f"   æˆåŠŸè«‹æ±‚æ•¸: {len(response_times)}")
        print(f"   æˆåŠŸç‡: {len(response_times)/test_count:.2%}")
        print(f"   å¹³å‡éŸ¿æ‡‰æ™‚é–“: {avg_time:.3f}ç§’")
        print(f"   æœ€å¿«éŸ¿æ‡‰æ™‚é–“: {min_time:.3f}ç§’")
        print(f"   æœ€æ…¢éŸ¿æ‡‰æ™‚é–“: {max_time:.3f}ç§’")
    
    # å¥åº·æª¢æŸ¥
    print(f"\nğŸ¥ å¥åº·æª¢æŸ¥:")
    health = client.health_check()
    print(f"   ç‹€æ…‹: {health['status']}")
    print(f"   æä¾›å•†: {health['provider']}")
    print(f"   æ¨¡å‹: {health['model']}")
    
    # å®¢æˆ¶ç«¯çµ±è¨ˆ
    stats = client.get_stats()
    print(f"\nğŸ“‹ å®¢æˆ¶ç«¯çµ±è¨ˆ:")
    print(f"   ç¸½è«‹æ±‚æ•¸: {stats['total_requests']}")
    print(f"   æˆåŠŸè«‹æ±‚æ•¸: {stats['successful_requests']}")
    print(f"   æˆåŠŸç‡: {stats['success_rate']:.2%}")
    print(f"   å¹³å‡éŸ¿æ‡‰æ™‚é–“: {stats['avg_response_time']:.3f}ç§’")
    print(f"   ç¸½Tokenä½¿ç”¨: {stats['total_tokens']}")


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ¯ LLMæŒ‡ä»¤éµå¾ªèˆ‡çµæ§‹åŒ–è¼¸å‡ºæ¨™æº–åº« - åŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    print("ç‰ˆæœ¬: 1.0.0")
    print("ä½œè€…: JobSpy Team")
    print("\næœ¬æ¼”ç¤ºä½¿ç”¨æ¨¡æ“¬å®¢æˆ¶ç«¯å±•ç¤ºæ¨™æº–åº«çš„æ ¸å¿ƒåŠŸèƒ½")
    print("å¯¦éš›ä½¿ç”¨æ™‚ï¼Œè«‹é…ç½®çœŸå¯¦çš„APIå¯†é‘°")
    print("=" * 60)
    
    # é‹è¡Œæ‰€æœ‰æ¼”ç¤º
    demos = [
        demo_basic_usage,
        demo_sentiment_analysis,
        demo_information_extraction,
        demo_text_classification,
        demo_batch_processing,
        demo_client_pool,
        demo_performance_monitoring
    ]
    
    for demo in demos:
        try:
            demo()
            time.sleep(1)  # æ¼”ç¤ºé–“éš”
        except KeyboardInterrupt:
            print("\n\nâ¹ï¸ æ¼”ç¤ºè¢«ç”¨æˆ¶ä¸­æ–·")
            break
        except Exception as e:
            print(f"\nâŒ æ¼”ç¤ºç•°å¸¸: {e}")
            continue
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ“š æ›´å¤šä¿¡æ¯:")
    print("   - æŸ¥çœ‹ README_LLM_STANDARD.md äº†è§£è©³ç´°æ–‡æª”")
    print("   - é‹è¡Œ test_llm_standard_integration.py é€²è¡Œå®Œæ•´æ¸¬è©¦")
    print("   - æŸ¥çœ‹ examples/ ç›®éŒ„äº†è§£æ›´å¤šä½¿ç”¨ç¤ºä¾‹")
    print("=" * 60)


if __name__ == '__main__':
    main()