#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMæ€§èƒ½å°æ¯”æ¸¬è©¦å¥—ä»¶
æ•´åˆæ¸¬è©¦æ¡ˆä¾‹ç”Ÿæˆã€åŸ·è¡Œã€åˆ†æå’Œå ±å‘ŠåŠŸèƒ½ï¼Œæä¾›å®Œæ•´çš„LLMæ¨¡å‹å°æ¯”æ¸¬è©¦è§£æ±ºæ–¹æ¡ˆ

Author: JobSpy Team
Date: 2025-01-27
"""

import json
import asyncio
import time
import statistics
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from collections import defaultdict
import uuid
import concurrent.futures
import threading
from contextlib import contextmanager

# å°å…¥å…¶ä»–æ¨¡çµ„ï¼ˆå‡è¨­å®ƒå€‘åœ¨åŒä¸€ç›®éŒ„ï¼‰
try:
    from llm_test_case_expander import LLMTestCaseExpander, ExpansionConfig, ExpandedTestCase
    from llm_test_execution_engine import LLMTestExecutionEngine, ExecutionConfig, TestExecution
    from llm_model_comparison_analyzer import LLMModelComparisonAnalyzer, ComparisonResult
except ImportError:
    print("âš ï¸ è­¦å‘Š: ç„¡æ³•å°å…¥ç›¸é—œæ¨¡çµ„ï¼Œè«‹ç¢ºä¿æ‰€æœ‰ä¾è³´æ–‡ä»¶éƒ½åœ¨åŒä¸€ç›®éŒ„")


class TestPhase(Enum):
    """æ¸¬è©¦éšæ®µ"""
    PREPARATION = "preparation"  # æº–å‚™éšæ®µ
    GENERATION = "generation"  # ç”Ÿæˆéšæ®µ
    EXECUTION = "execution"  # åŸ·è¡Œéšæ®µ
    ANALYSIS = "analysis"  # åˆ†æéšæ®µ
    REPORTING = "reporting"  # å ±å‘Šéšæ®µ
    COMPLETED = "completed"  # å®Œæˆéšæ®µ


class ComparisonScope(Enum):
    """å°æ¯”ç¯„åœ"""
    BASIC = "basic"  # åŸºç¤å°æ¯”
    COMPREHENSIVE = "comprehensive"  # å…¨é¢å°æ¯”
    PERFORMANCE_FOCUSED = "performance_focused"  # æ€§èƒ½å°å‘
    ACCURACY_FOCUSED = "accuracy_focused"  # æº–ç¢ºæ€§å°å‘
    MULTILINGUAL = "multilingual"  # å¤šèªè¨€å°æ¯”
    EDGE_CASE = "edge_case"  # é‚Šç•Œæ¡ˆä¾‹å°æ¯”
    CUSTOM = "custom"  # è‡ªå®šç¾©å°æ¯”


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    model_id: str
    model_name: str
    provider: str
    api_endpoint: Optional[str] = None
    api_key: Optional[str] = None
    max_tokens: int = 1000
    temperature: float = 0.7
    timeout: int = 30
    rate_limit: int = 10  # æ¯ç§’è«‹æ±‚æ•¸
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ComparisonConfig:
    """å°æ¯”é…ç½®"""
    comparison_id: str
    scope: ComparisonScope
    models: List[ModelConfig]
    test_case_count: int = 1000
    parallel_execution: bool = True
    max_workers: int = 5
    include_performance_metrics: bool = True
    include_accuracy_analysis: bool = True
    include_cost_analysis: bool = True
    generate_visualizations: bool = True
    output_directory: str = "comparison_results"
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ComparisonProgress:
    """å°æ¯”é€²åº¦"""
    current_phase: TestPhase
    total_phases: int = 6
    current_step: int = 0
    total_steps: int = 0
    start_time: datetime = field(default_factory=datetime.now)
    phase_start_time: datetime = field(default_factory=datetime.now)
    estimated_completion: Optional[datetime] = None
    status_message: str = ""
    errors: List[str] = field(default_factory=list)


@dataclass
class ModelComparisonResult:
    """æ¨¡å‹å°æ¯”çµæœ"""
    comparison_id: str
    model_results: Dict[str, Dict[str, Any]]
    performance_comparison: Dict[str, Any]
    accuracy_comparison: Dict[str, Any]
    cost_comparison: Dict[str, Any]
    statistical_analysis: Dict[str, Any]
    recommendations: List[str]
    execution_summary: Dict[str, Any]
    metadata: Dict[str, Any]
    generation_time: datetime = field(default_factory=datetime.now)


class LLMPerformanceComparisonSuite:
    """LLMæ€§èƒ½å°æ¯”æ¸¬è©¦å¥—ä»¶"""
    
    def __init__(self):
        """åˆå§‹åŒ–å°æ¯”å¥—ä»¶"""
        self.test_case_expander = None
        self.execution_engine = None
        self.comparison_analyzer = None
        self.current_comparison = None
        self.progress = None
        self.results_cache = {}
        self.lock = threading.Lock()
        
        # åˆå§‹åŒ–çµ„ä»¶
        self._initialize_components()
    
    def _initialize_components(self) -> None:
        """åˆå§‹åŒ–çµ„ä»¶"""
        try:
            self.test_case_expander = LLMTestCaseExpander()
            self.execution_engine = LLMTestExecutionEngine()
            self.comparison_analyzer = LLMModelComparisonAnalyzer()
            print("âœ… æ‰€æœ‰çµ„ä»¶åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ çµ„ä»¶åˆå§‹åŒ–å¤±æ•—: {e}")
            print("   è«‹ç¢ºä¿æ‰€æœ‰ä¾è³´æ¨¡çµ„éƒ½å·²æ­£ç¢ºå®‰è£")
    
    def create_comparison_config(self, scope: ComparisonScope, models: List[Dict[str, Any]]) -> ComparisonConfig:
        """å‰µå»ºå°æ¯”é…ç½®"""
        comparison_id = f"comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
        
        # è½‰æ›æ¨¡å‹é…ç½®
        model_configs = []
        for model_data in models:
            config = ModelConfig(
                model_id=model_data.get('model_id', ''),
                model_name=model_data.get('model_name', ''),
                provider=model_data.get('provider', ''),
                api_endpoint=model_data.get('api_endpoint'),
                api_key=model_data.get('api_key'),
                max_tokens=model_data.get('max_tokens', 1000),
                temperature=model_data.get('temperature', 0.7),
                timeout=model_data.get('timeout', 30),
                rate_limit=model_data.get('rate_limit', 10),
                metadata=model_data.get('metadata', {})
            )
            model_configs.append(config)
        
        # æ ¹æ“šç¯„åœè¨­ç½®é»˜èªåƒæ•¸
        scope_configs = {
            ComparisonScope.BASIC: {
                'test_case_count': 500,
                'max_workers': 3,
                'include_performance_metrics': True,
                'include_accuracy_analysis': True,
                'include_cost_analysis': False
            },
            ComparisonScope.COMPREHENSIVE: {
                'test_case_count': 2000,
                'max_workers': 5,
                'include_performance_metrics': True,
                'include_accuracy_analysis': True,
                'include_cost_analysis': True
            },
            ComparisonScope.PERFORMANCE_FOCUSED: {
                'test_case_count': 1000,
                'max_workers': 8,
                'include_performance_metrics': True,
                'include_accuracy_analysis': False,
                'include_cost_analysis': True
            },
            ComparisonScope.ACCURACY_FOCUSED: {
                'test_case_count': 1500,
                'max_workers': 3,
                'include_performance_metrics': False,
                'include_accuracy_analysis': True,
                'include_cost_analysis': False
            },
            ComparisonScope.MULTILINGUAL: {
                'test_case_count': 800,
                'max_workers': 4,
                'include_performance_metrics': True,
                'include_accuracy_analysis': True,
                'include_cost_analysis': True
            },
            ComparisonScope.EDGE_CASE: {
                'test_case_count': 300,
                'max_workers': 2,
                'include_performance_metrics': True,
                'include_accuracy_analysis': True,
                'include_cost_analysis': False
            }
        }
        
        default_config = scope_configs.get(scope, scope_configs[ComparisonScope.BASIC])
        
        config = ComparisonConfig(
            comparison_id=comparison_id,
            scope=scope,
            models=model_configs,
            **default_config
        )
        
        return config
    
    def run_comparison(self, config: ComparisonConfig) -> ModelComparisonResult:
        """é‹è¡Œå®Œæ•´å°æ¯”æ¸¬è©¦"""
        print(f"ğŸš€ é–‹å§‹LLMæ¨¡å‹æ€§èƒ½å°æ¯”æ¸¬è©¦")
        print(f"   å°æ¯”ID: {config.comparison_id}")
        print(f"   å°æ¯”ç¯„åœ: {config.scope.value}")
        print(f"   æ¨¡å‹æ•¸é‡: {len(config.models)}")
        print(f"   æ¸¬è©¦æ¡ˆä¾‹æ•¸é‡: {config.test_case_count}")
        
        # åˆå§‹åŒ–é€²åº¦è¿½è¹¤
        self.progress = ComparisonProgress(
            current_phase=TestPhase.PREPARATION,
            total_steps=self._calculate_total_steps(config)
        )
        
        self.current_comparison = config
        
        try:
            # éšæ®µ1: æº–å‚™éšæ®µ
            self._update_progress(TestPhase.PREPARATION, "æº–å‚™æ¸¬è©¦ç’°å¢ƒ...")
            self._prepare_environment(config)
            
            # éšæ®µ2: ç”Ÿæˆæ¸¬è©¦æ¡ˆä¾‹
            self._update_progress(TestPhase.GENERATION, "ç”Ÿæˆæ¸¬è©¦æ¡ˆä¾‹...")
            test_cases = self._generate_test_cases(config)
            
            # éšæ®µ3: åŸ·è¡Œæ¸¬è©¦
            self._update_progress(TestPhase.EXECUTION, "åŸ·è¡Œæ¨¡å‹æ¸¬è©¦...")
            execution_results = self._execute_tests(config, test_cases)
            
            # éšæ®µ4: åˆ†æçµæœ
            self._update_progress(TestPhase.ANALYSIS, "åˆ†æå°æ¯”çµæœ...")
            analysis_results = self._analyze_results(config, execution_results)
            
            # éšæ®µ5: ç”Ÿæˆå ±å‘Š
            self._update_progress(TestPhase.REPORTING, "ç”Ÿæˆå°æ¯”å ±å‘Š...")
            comparison_result = self._generate_comparison_report(config, analysis_results)
            
            # éšæ®µ6: å®Œæˆ
            self._update_progress(TestPhase.COMPLETED, "å°æ¯”æ¸¬è©¦å®Œæˆ")
            
            print(f"\nâœ… å°æ¯”æ¸¬è©¦å®Œæˆ!")
            print(f"   ç¸½è€—æ™‚: {datetime.now() - self.progress.start_time}")
            print(f"   çµæœä¿å­˜åœ¨: {config.output_directory}")
            
            return comparison_result
            
        except Exception as e:
            self.progress.errors.append(str(e))
            print(f"âŒ å°æ¯”æ¸¬è©¦å¤±æ•—: {e}")
            raise
    
    def _calculate_total_steps(self, config: ComparisonConfig) -> int:
        """è¨ˆç®—ç¸½æ­¥é©Ÿæ•¸"""
        steps = 0
        steps += 2  # æº–å‚™éšæ®µ
        steps += 3  # ç”Ÿæˆéšæ®µ
        steps += len(config.models) * 2  # åŸ·è¡Œéšæ®µ
        steps += 4  # åˆ†æéšæ®µ
        steps += 3  # å ±å‘Šéšæ®µ
        return steps
    
    def _update_progress(self, phase: TestPhase, message: str) -> None:
        """æ›´æ–°é€²åº¦"""
        with self.lock:
            if self.progress.current_phase != phase:
                self.progress.current_phase = phase
                self.progress.phase_start_time = datetime.now()
            
            self.progress.current_step += 1
            self.progress.status_message = message
            
            # ä¼°ç®—å®Œæˆæ™‚é–“
            if self.progress.current_step > 0:
                elapsed = datetime.now() - self.progress.start_time
                estimated_total = elapsed * (self.progress.total_steps / self.progress.current_step)
                self.progress.estimated_completion = self.progress.start_time + estimated_total
        
        # æ‰“å°é€²åº¦
        percentage = (self.progress.current_step / self.progress.total_steps) * 100
        print(f"   [{percentage:5.1f}%] {phase.value}: {message}")
    
    def _prepare_environment(self, config: ComparisonConfig) -> None:
        """æº–å‚™æ¸¬è©¦ç’°å¢ƒ"""
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        output_path = Path(config.output_directory)
        output_path.mkdir(exist_ok=True)
        
        # å‰µå»ºå­ç›®éŒ„
        (output_path / "test_cases").mkdir(exist_ok=True)
        (output_path / "execution_results").mkdir(exist_ok=True)
        (output_path / "analysis_results").mkdir(exist_ok=True)
        (output_path / "reports").mkdir(exist_ok=True)
        (output_path / "visualizations").mkdir(exist_ok=True)
        
        self._update_progress(TestPhase.PREPARATION, "é©—è­‰æ¨¡å‹é…ç½®...")
        
        # é©—è­‰æ¨¡å‹é…ç½®
        for model in config.models:
            if not model.model_id or not model.provider:
                raise ValueError(f"æ¨¡å‹é…ç½®ä¸å®Œæ•´: {model.model_name}")
    
    def _generate_test_cases(self, config: ComparisonConfig) -> List[ExpandedTestCase]:
        """ç”Ÿæˆæ¸¬è©¦æ¡ˆä¾‹"""
        if not self.test_case_expander:
            raise RuntimeError("æ¸¬è©¦æ¡ˆä¾‹æ“´å……å™¨æœªåˆå§‹åŒ–")
        
        # è¼‰å…¥åŸºç¤æ¨¡æ¿
        self.test_case_expander.load_base_templates()
        
        self._update_progress(TestPhase.GENERATION, "é…ç½®æ“´å……ç­–ç•¥...")
        
        # æ ¹æ“šå°æ¯”ç¯„åœé…ç½®æ“´å……ç­–ç•¥
        expansion_config = self._create_expansion_config(config)
        
        self._update_progress(TestPhase.GENERATION, "åŸ·è¡Œæ¡ˆä¾‹æ“´å……...")
        
        # ç”Ÿæˆæ¸¬è©¦æ¡ˆä¾‹
        test_cases = self.test_case_expander.expand_test_cases(expansion_config)
        
        # ä¿å­˜æ¸¬è©¦æ¡ˆä¾‹
        test_cases_file = Path(config.output_directory) / "test_cases" / "generated_test_cases.json"
        self.test_case_expander.export_test_cases(str(test_cases_file), "json")
        
        print(f"   ğŸ“ ç”Ÿæˆäº† {len(test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹")
        
        return test_cases
    
    def _create_expansion_config(self, config: ComparisonConfig) -> 'ExpansionConfig':
        """å‰µå»ºæ“´å……é…ç½®"""
        from llm_test_case_expander import ExpansionStrategy
        
        # æ ¹æ“šå°æ¯”ç¯„åœé¸æ“‡ç­–ç•¥
        scope_strategies = {
            ComparisonScope.BASIC: [
                ExpansionStrategy.TEMPLATE_VARIATION,
                ExpansionStrategy.PARAMETER_COMBINATION
            ],
            ComparisonScope.COMPREHENSIVE: list(ExpansionStrategy),
            ComparisonScope.PERFORMANCE_FOCUSED: [
                ExpansionStrategy.TEMPLATE_VARIATION,
                ExpansionStrategy.COMPLEXITY_SCALING,
                ExpansionStrategy.NOISE_INJECTION
            ],
            ComparisonScope.ACCURACY_FOCUSED: [
                ExpansionStrategy.SEMANTIC_EXPANSION,
                ExpansionStrategy.SYNTACTIC_VARIATION,
                ExpansionStrategy.DOMAIN_TRANSFER
            ],
            ComparisonScope.MULTILINGUAL: [
                ExpansionStrategy.MULTILINGUAL_TRANSLATION,
                ExpansionStrategy.TEMPLATE_VARIATION
            ],
            ComparisonScope.EDGE_CASE: [
                ExpansionStrategy.NOISE_INJECTION,
                ExpansionStrategy.COMPLEXITY_SCALING
            ]
        }
        
        strategies = scope_strategies.get(config.scope, scope_strategies[ComparisonScope.BASIC])
        
        # èªè¨€åˆ†ä½ˆé…ç½®
        if config.scope == ComparisonScope.MULTILINGUAL:
            language_distribution = {
                "zh-TW": 0.3, "en-US": 0.3, "zh-CN": 0.2, "ja-JP": 0.1, "ko-KR": 0.1
            }
        else:
            language_distribution = {
                "zh-TW": 0.7, "en-US": 0.2, "zh-CN": 0.1
            }
        
        return ExpansionConfig(
            target_count=config.test_case_count,
            expansion_strategies=strategies,
            language_distribution=language_distribution,
            ensure_diversity=True,
            max_similarity_threshold=0.8
        )
    
    def _execute_tests(self, config: ComparisonConfig, test_cases: List[ExpandedTestCase]) -> Dict[str, List[TestExecution]]:
        """åŸ·è¡Œæ¸¬è©¦"""
        if not self.execution_engine:
            raise RuntimeError("æ¸¬è©¦åŸ·è¡Œå¼•æ“æœªåˆå§‹åŒ–")
        
        execution_results = {}
        
        for i, model in enumerate(config.models):
            self._update_progress(TestPhase.EXECUTION, f"æ¸¬è©¦æ¨¡å‹ {model.model_name} ({i+1}/{len(config.models)})...")
            
            # é…ç½®åŸ·è¡Œå¼•æ“
            execution_config = ExecutionConfig(
                execution_mode="parallel" if config.parallel_execution else "sequential",
                max_workers=config.max_workers,
                timeout=model.timeout,
                rate_limit=model.rate_limit,
                retry_attempts=3,
                save_results=True
            )
            
            # åŸ·è¡Œæ¸¬è©¦
            try:
                model_results = self._execute_model_tests(model, test_cases, execution_config)
                execution_results[model.model_id] = model_results
                
                # ä¿å­˜å–®å€‹æ¨¡å‹çµæœ
                results_file = Path(config.output_directory) / "execution_results" / f"{model.model_id}_results.json"
                self._save_execution_results(model_results, str(results_file))
                
                print(f"     âœ… {model.model_name} æ¸¬è©¦å®Œæˆ: {len(model_results)} å€‹çµæœ")
                
            except Exception as e:
                print(f"     âŒ {model.model_name} æ¸¬è©¦å¤±æ•—: {e}")
                execution_results[model.model_id] = []
                self.progress.errors.append(f"æ¨¡å‹ {model.model_name} æ¸¬è©¦å¤±æ•—: {e}")
        
        return execution_results
    
    def _execute_model_tests(self, model: ModelConfig, test_cases: List[ExpandedTestCase], 
                           execution_config: 'ExecutionConfig') -> List['TestExecution']:
        """åŸ·è¡Œå–®å€‹æ¨¡å‹çš„æ¸¬è©¦"""
        # è½‰æ›æ¸¬è©¦æ¡ˆä¾‹æ ¼å¼
        formatted_cases = []
        for case in test_cases:
            formatted_case = {
                'test_case_id': case.test_case_id,
                'query': case.query,
                'expected_intent': case.expected_intent,
                'expected_entities': case.expected_entities,
                'category': case.category.value,
                'difficulty': case.difficulty.value,
                'language': case.language,
                'metadata': case.metadata
            }
            formatted_cases.append(formatted_case)
        
        # æ¨¡æ“¬åŸ·è¡Œï¼ˆå¯¦éš›å¯¦ç¾éœ€è¦èª¿ç”¨çœŸå¯¦çš„LLM APIï¼‰
        results = []
        for case in formatted_cases:
            # é€™è£¡æ‡‰è©²èª¿ç”¨å¯¦éš›çš„LLM API
            # ç¾åœ¨ä½¿ç”¨æ¨¡æ“¬çµæœ
            execution = self._simulate_model_execution(model, case)
            results.append(execution)
        
        return results
    
    def _simulate_model_execution(self, model: ModelConfig, test_case: Dict[str, Any]) -> 'TestExecution':
        """æ¨¡æ“¬æ¨¡å‹åŸ·è¡Œï¼ˆç”¨æ–¼æ¼”ç¤ºï¼‰"""
        import random
        
        # æ¨¡æ“¬åŸ·è¡Œæ™‚é–“
        execution_time = random.uniform(0.5, 3.0)
        time.sleep(0.01)  # æ¨¡æ“¬ç¶²è·¯å»¶é²
        
        # æ¨¡æ“¬éŸ¿æ‡‰
        response = {
            'intent': test_case['expected_intent'],
            'entities': test_case.get('expected_entities', {}),
            'confidence': random.uniform(0.7, 0.95),
            'response_text': f"æ¨¡æ“¬éŸ¿æ‡‰ for {test_case['query']}"
        }
        
        # æ¨¡æ“¬æˆåŠŸç‡
        success = random.random() > 0.05  # 95% æˆåŠŸç‡
        
        from llm_test_execution_engine import ExecutionStatus
        
        execution = TestExecution(
            execution_id=str(uuid.uuid4()),
            test_case_id=test_case['test_case_id'],
            model_id=model.model_id,
            query=test_case['query'],
            response=response if success else None,
            status=ExecutionStatus.SUCCESS if success else ExecutionStatus.FAILED,
            execution_time=execution_time,
            timestamp=datetime.now(),
            error_message=None if success else "æ¨¡æ“¬éŒ¯èª¤",
            metadata={
                'model_name': model.model_name,
                'provider': model.provider,
                'temperature': model.temperature,
                'max_tokens': model.max_tokens
            }
        )
        
        return execution
    
    def _save_execution_results(self, results: List['TestExecution'], file_path: str) -> None:
        """ä¿å­˜åŸ·è¡Œçµæœ"""
        data = []
        for result in results:
            data.append({
                'execution_id': result.execution_id,
                'test_case_id': result.test_case_id,
                'model_id': result.model_id,
                'query': result.query,
                'response': result.response,
                'status': result.status.value,
                'execution_time': result.execution_time,
                'timestamp': result.timestamp.isoformat(),
                'error_message': result.error_message,
                'metadata': result.metadata
            })
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def _analyze_results(self, config: ComparisonConfig, 
                        execution_results: Dict[str, List['TestExecution']]) -> Dict[str, Any]:
        """åˆ†æçµæœ"""
        if not self.comparison_analyzer:
            raise RuntimeError("å°æ¯”åˆ†æå™¨æœªåˆå§‹åŒ–")
        
        self._update_progress(TestPhase.ANALYSIS, "è¨ˆç®—æ€§èƒ½æŒ‡æ¨™...")
        
        analysis_results = {
            'model_performance': {},
            'comparison_matrix': {},
            'statistical_analysis': {},
            'insights': []
        }
        
        # åˆ†ææ¯å€‹æ¨¡å‹çš„æ€§èƒ½
        for model_id, executions in execution_results.items():
            model_name = next((m.model_name for m in config.models if m.model_id == model_id), model_id)
            
            performance = self._analyze_model_performance(executions)
            analysis_results['model_performance'][model_id] = {
                'model_name': model_name,
                'performance': performance
            }
        
        self._update_progress(TestPhase.ANALYSIS, "åŸ·è¡Œçµ±è¨ˆåˆ†æ...")
        
        # åŸ·è¡Œå°æ¯”åˆ†æ
        if len(execution_results) > 1:
            comparison_analysis = self._perform_comparison_analysis(execution_results, config)
            analysis_results['comparison_matrix'] = comparison_analysis
        
        self._update_progress(TestPhase.ANALYSIS, "ç”Ÿæˆåˆ†ææ´å¯Ÿ...")
        
        # ç”Ÿæˆæ´å¯Ÿ
        insights = self._generate_analysis_insights(analysis_results, config)
        analysis_results['insights'] = insights
        
        # ä¿å­˜åˆ†æçµæœ
        analysis_file = Path(config.output_directory) / "analysis_results" / "comparison_analysis.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2)
        
        return analysis_results
    
    def _analyze_model_performance(self, executions: List['TestExecution']) -> Dict[str, Any]:
        """åˆ†æå–®å€‹æ¨¡å‹æ€§èƒ½"""
        if not executions:
            return {'error': 'ç„¡åŸ·è¡Œçµæœ'}
        
        # åŸºæœ¬çµ±è¨ˆ
        total_executions = len(executions)
        successful_executions = [e for e in executions if e.status.value == 'success']
        failed_executions = [e for e in executions if e.status.value == 'failed']
        
        success_rate = len(successful_executions) / total_executions if total_executions > 0 else 0
        
        # æ€§èƒ½çµ±è¨ˆ
        execution_times = [e.execution_time for e in successful_executions]
        
        performance = {
            'basic_stats': {
                'total_executions': total_executions,
                'successful_executions': len(successful_executions),
                'failed_executions': len(failed_executions),
                'success_rate': success_rate
            },
            'performance_stats': {
                'avg_execution_time': statistics.mean(execution_times) if execution_times else 0,
                'median_execution_time': statistics.median(execution_times) if execution_times else 0,
                'min_execution_time': min(execution_times) if execution_times else 0,
                'max_execution_time': max(execution_times) if execution_times else 0,
                'std_execution_time': statistics.stdev(execution_times) if len(execution_times) > 1 else 0
            },
            'accuracy_stats': self._calculate_accuracy_stats(successful_executions),
            'error_analysis': self._analyze_errors(failed_executions)
        }
        
        return performance
    
    def _calculate_accuracy_stats(self, executions: List['TestExecution']) -> Dict[str, Any]:
        """è¨ˆç®—æº–ç¢ºæ€§çµ±è¨ˆ"""
        if not executions:
            return {}
        
        # æ¨¡æ“¬æº–ç¢ºæ€§è¨ˆç®—ï¼ˆå¯¦éš›å¯¦ç¾éœ€è¦æ›´è¤‡é›œçš„é‚è¼¯ï¼‰
        confidence_scores = []
        intent_accuracy = 0
        entity_accuracy = 0
        
        for execution in executions:
            if execution.response and 'confidence' in execution.response:
                confidence_scores.append(execution.response['confidence'])
            
            # ç°¡åŒ–çš„æº–ç¢ºæ€§è¨ˆç®—
            if execution.response:
                intent_accuracy += 1  # å‡è¨­æ„åœ–éƒ½æ­£ç¢º
                entity_accuracy += 1  # å‡è¨­å¯¦é«”éƒ½æ­£ç¢º
        
        total = len(executions)
        
        return {
            'intent_accuracy': intent_accuracy / total if total > 0 else 0,
            'entity_accuracy': entity_accuracy / total if total > 0 else 0,
            'avg_confidence': statistics.mean(confidence_scores) if confidence_scores else 0,
            'confidence_std': statistics.stdev(confidence_scores) if len(confidence_scores) > 1 else 0
        }
    
    def _analyze_errors(self, failed_executions: List['TestExecution']) -> Dict[str, Any]:
        """åˆ†æéŒ¯èª¤"""
        if not failed_executions:
            return {'error_count': 0, 'error_types': {}}
        
        error_types = defaultdict(int)
        for execution in failed_executions:
            error_msg = execution.error_message or 'Unknown error'
            error_types[error_msg] += 1
        
        return {
            'error_count': len(failed_executions),
            'error_types': dict(error_types),
            'error_rate': len(failed_executions) / len(failed_executions) if failed_executions else 0
        }
    
    def _perform_comparison_analysis(self, execution_results: Dict[str, List['TestExecution']], 
                                   config: ComparisonConfig) -> Dict[str, Any]:
        """åŸ·è¡Œå°æ¯”åˆ†æ"""
        comparison_matrix = {}
        
        model_ids = list(execution_results.keys())
        
        # å…©å…©å°æ¯”
        for i, model_a in enumerate(model_ids):
            for j, model_b in enumerate(model_ids):
                if i < j:  # é¿å…é‡è¤‡å°æ¯”
                    comparison_key = f"{model_a}_vs_{model_b}"
                    
                    comparison = self._compare_two_models(
                        execution_results[model_a],
                        execution_results[model_b],
                        model_a,
                        model_b
                    )
                    
                    comparison_matrix[comparison_key] = comparison
        
        return comparison_matrix
    
    def _compare_two_models(self, executions_a: List['TestExecution'], 
                          executions_b: List['TestExecution'],
                          model_a_id: str, model_b_id: str) -> Dict[str, Any]:
        """å°æ¯”å…©å€‹æ¨¡å‹"""
        # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
        perf_a = self._analyze_model_performance(executions_a)
        perf_b = self._analyze_model_performance(executions_b)
        
        # å°æ¯”çµæœ
        comparison = {
            'model_a': model_a_id,
            'model_b': model_b_id,
            'performance_comparison': {
                'success_rate_diff': perf_a['basic_stats']['success_rate'] - perf_b['basic_stats']['success_rate'],
                'avg_time_diff': perf_a['performance_stats']['avg_execution_time'] - perf_b['performance_stats']['avg_execution_time'],
                'accuracy_diff': perf_a['accuracy_stats'].get('intent_accuracy', 0) - perf_b['accuracy_stats'].get('intent_accuracy', 0)
            },
            'statistical_significance': self._calculate_statistical_significance(executions_a, executions_b),
            'winner': self._determine_winner(perf_a, perf_b)
        }
        
        return comparison
    
    def _calculate_statistical_significance(self, executions_a: List['TestExecution'], 
                                          executions_b: List['TestExecution']) -> Dict[str, Any]:
        """è¨ˆç®—çµ±è¨ˆé¡¯è‘—æ€§"""
        # ç°¡åŒ–çš„çµ±è¨ˆåˆ†æ
        times_a = [e.execution_time for e in executions_a if e.status.value == 'success']
        times_b = [e.execution_time for e in executions_b if e.status.value == 'success']
        
        if len(times_a) < 2 or len(times_b) < 2:
            return {'p_value': None, 'significant': False}
        
        # æ¨¡æ“¬tæª¢é©—çµæœ
        import random
        p_value = random.uniform(0.01, 0.1)
        significant = p_value < 0.05
        
        return {
            'p_value': p_value,
            'significant': significant,
            'test_type': 't_test',
            'sample_size_a': len(times_a),
            'sample_size_b': len(times_b)
        }
    
    def _determine_winner(self, perf_a: Dict[str, Any], perf_b: Dict[str, Any]) -> str:
        """ç¢ºå®šå‹è€…"""
        score_a = 0
        score_b = 0
        
        # æˆåŠŸç‡å°æ¯”
        if perf_a['basic_stats']['success_rate'] > perf_b['basic_stats']['success_rate']:
            score_a += 1
        elif perf_b['basic_stats']['success_rate'] > perf_a['basic_stats']['success_rate']:
            score_b += 1
        
        # åŸ·è¡Œæ™‚é–“å°æ¯”ï¼ˆè¶ŠçŸ­è¶Šå¥½ï¼‰
        if perf_a['performance_stats']['avg_execution_time'] < perf_b['performance_stats']['avg_execution_time']:
            score_a += 1
        elif perf_b['performance_stats']['avg_execution_time'] < perf_a['performance_stats']['avg_execution_time']:
            score_b += 1
        
        # æº–ç¢ºæ€§å°æ¯”
        acc_a = perf_a['accuracy_stats'].get('intent_accuracy', 0)
        acc_b = perf_b['accuracy_stats'].get('intent_accuracy', 0)
        
        if acc_a > acc_b:
            score_a += 1
        elif acc_b > acc_a:
            score_b += 1
        
        if score_a > score_b:
            return 'model_a'
        elif score_b > score_a:
            return 'model_b'
        else:
            return 'tie'
    
    def _generate_analysis_insights(self, analysis_results: Dict[str, Any], 
                                  config: ComparisonConfig) -> List[str]:
        """ç”Ÿæˆåˆ†ææ´å¯Ÿ"""
        insights = []
        
        # æ€§èƒ½æ´å¯Ÿ
        model_performances = analysis_results['model_performance']
        
        if len(model_performances) > 1:
            # æ‰¾å‡ºæœ€ä½³æ€§èƒ½æ¨¡å‹
            best_success_rate = max(perf['performance']['basic_stats']['success_rate'] 
                                  for perf in model_performances.values())
            best_speed = min(perf['performance']['performance_stats']['avg_execution_time'] 
                           for perf in model_performances.values() 
                           if perf['performance']['performance_stats']['avg_execution_time'] > 0)
            
            for model_id, data in model_performances.items():
                perf = data['performance']
                model_name = data['model_name']
                
                if perf['basic_stats']['success_rate'] == best_success_rate:
                    insights.append(f"ğŸ† {model_name} å…·æœ‰æœ€é«˜çš„æˆåŠŸç‡ ({best_success_rate:.1%})")
                
                if perf['performance_stats']['avg_execution_time'] == best_speed:
                    insights.append(f"âš¡ {model_name} å…·æœ‰æœ€å¿«çš„å¹³å‡éŸ¿æ‡‰æ™‚é–“ ({best_speed:.2f}ç§’)")
        
        # å°æ¯”æ´å¯Ÿ
        if 'comparison_matrix' in analysis_results:
            significant_comparisons = []
            for comp_key, comp_data in analysis_results['comparison_matrix'].items():
                if comp_data['statistical_significance']['significant']:
                    significant_comparisons.append(comp_key)
            
            if significant_comparisons:
                insights.append(f"ğŸ“Š ç™¼ç¾ {len(significant_comparisons)} å€‹çµ±è¨ˆé¡¯è‘—çš„æ€§èƒ½å·®ç•°")
        
        # ç¯„åœç‰¹å®šæ´å¯Ÿ
        if config.scope == ComparisonScope.MULTILINGUAL:
            insights.append("ğŸŒ å¤šèªè¨€æ¸¬è©¦é¡¯ç¤ºä¸åŒæ¨¡å‹åœ¨è·¨èªè¨€ç†è§£èƒ½åŠ›ä¸Šå­˜åœ¨å·®ç•°")
        elif config.scope == ComparisonScope.EDGE_CASE:
            insights.append("âš ï¸ é‚Šç•Œæ¡ˆä¾‹æ¸¬è©¦æ­ç¤ºäº†æ¨¡å‹åœ¨è™•ç†ç•°å¸¸è¼¸å…¥æ™‚çš„é­¯æ£’æ€§å·®ç•°")
        
        return insights
    
    def _generate_comparison_report(self, config: ComparisonConfig, 
                                  analysis_results: Dict[str, Any]) -> ModelComparisonResult:
        """ç”Ÿæˆå°æ¯”å ±å‘Š"""
        self._update_progress(TestPhase.REPORTING, "ç·¨è­¯å°æ¯”çµæœ...")
        
        # æå–æ¨¡å‹çµæœ
        model_results = {}
        for model_id, data in analysis_results['model_performance'].items():
            model_results[model_id] = data
        
        # ç”Ÿæˆå»ºè­°
        recommendations = self._generate_recommendations(analysis_results, config)
        
        self._update_progress(TestPhase.REPORTING, "ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨...")
        
        # ç”Ÿæˆå¯è¦–åŒ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if config.generate_visualizations:
            self._generate_visualizations(analysis_results, config)
        
        # å‰µå»ºæœ€çµ‚çµæœ
        comparison_result = ModelComparisonResult(
            comparison_id=config.comparison_id,
            model_results=model_results,
            performance_comparison=analysis_results.get('comparison_matrix', {}),
            accuracy_comparison=self._extract_accuracy_comparison(analysis_results),
            cost_comparison=self._calculate_cost_comparison(analysis_results, config),
            statistical_analysis=self._extract_statistical_analysis(analysis_results),
            recommendations=recommendations,
            execution_summary=self._create_execution_summary(analysis_results, config),
            metadata={
                'scope': config.scope.value,
                'test_case_count': config.test_case_count,
                'models_tested': [m.model_name for m in config.models],
                'generation_duration': str(datetime.now() - self.progress.start_time),
                'errors': self.progress.errors
            }
        )
        
        # ä¿å­˜æœ€çµ‚å ±å‘Š
        report_file = Path(config.output_directory) / "reports" / "comparison_report.json"
        self._save_comparison_report(comparison_result, str(report_file))
        
        return comparison_result
    
    def _generate_recommendations(self, analysis_results: Dict[str, Any], 
                                config: ComparisonConfig) -> List[str]:
        """ç”Ÿæˆå»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼åˆ†æçµæœç”Ÿæˆå»ºè­°
        model_performances = analysis_results['model_performance']
        
        if len(model_performances) > 1:
            # æ€§èƒ½å»ºè­°
            best_model = max(model_performances.items(), 
                           key=lambda x: x[1]['performance']['basic_stats']['success_rate'])
            
            recommendations.append(
                f"ğŸ¯ æ¨è–¦ä½¿ç”¨ {best_model[1]['model_name']} ä»¥ç²å¾—æœ€ä½³æ•´é«”æ€§èƒ½"
            )
            
            # æˆæœ¬æ•ˆç›Šå»ºè­°
            if config.include_cost_analysis:
                recommendations.append(
                    "ğŸ’° å»ºè­°æ ¹æ“šå¯¦éš›ä½¿ç”¨é‡å’Œé ç®—é¸æ“‡æœ€å…·æˆæœ¬æ•ˆç›Šçš„æ¨¡å‹"
                )
            
            # ä½¿ç”¨å ´æ™¯å»ºè­°
            if config.scope == ComparisonScope.PERFORMANCE_FOCUSED:
                recommendations.append(
                    "âš¡ å°æ–¼é«˜ä¸¦ç™¼å ´æ™¯ï¼Œå„ªå…ˆè€ƒæ…®éŸ¿æ‡‰æ™‚é–“æœ€å¿«çš„æ¨¡å‹"
                )
            elif config.scope == ComparisonScope.ACCURACY_FOCUSED:
                recommendations.append(
                    "ğŸ¯ å°æ–¼æº–ç¢ºæ€§è¦æ±‚é«˜çš„å ´æ™¯ï¼Œé¸æ“‡æº–ç¢ºç‡æœ€é«˜çš„æ¨¡å‹"
                )
        
        # é€šç”¨å»ºè­°
        recommendations.extend([
            "ğŸ“Š å»ºè­°å®šæœŸé‡æ–°è©•ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå› ç‚ºæ¨¡å‹æœƒæŒçºŒæ›´æ–°",
            "ğŸ”„ è€ƒæ…®ä½¿ç”¨A/Bæ¸¬è©¦åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­é©—è­‰æ¨¡å‹é¸æ“‡",
            "ğŸ“ˆ ç›£æ§å¯¦éš›ä½¿ç”¨ä¸­çš„æ€§èƒ½æŒ‡æ¨™ï¼Œç¢ºä¿èˆ‡æ¸¬è©¦çµæœä¸€è‡´"
        ])
        
        return recommendations
    
    def _generate_visualizations(self, analysis_results: Dict[str, Any], 
                               config: ComparisonConfig) -> None:
        """ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨"""
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
            
            # è¨­ç½®ä¸­æ–‡å­—é«”
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
            plt.rcParams['axes.unicode_minus'] = False
            
            viz_dir = Path(config.output_directory) / "visualizations"
            
            # ç”Ÿæˆæ€§èƒ½å°æ¯”åœ–
            self._create_performance_comparison_chart(analysis_results, viz_dir)
            
            # ç”ŸæˆæˆåŠŸç‡å°æ¯”åœ–
            self._create_success_rate_chart(analysis_results, viz_dir)
            
            # ç”ŸæˆéŸ¿æ‡‰æ™‚é–“åˆ†ä½ˆåœ–
            self._create_response_time_chart(analysis_results, viz_dir)
            
            print("   ğŸ“Š å¯è¦–åŒ–åœ–è¡¨å·²ç”Ÿæˆ")
            
        except ImportError:
            print("   âš ï¸ ç„¡æ³•ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨ï¼Œè«‹å®‰è£ matplotlib å’Œ seaborn")
        except Exception as e:
            print(f"   âŒ ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨å¤±æ•—: {e}")
    
    def _create_performance_comparison_chart(self, analysis_results: Dict[str, Any], 
                                           viz_dir: Path) -> None:
        """å‰µå»ºæ€§èƒ½å°æ¯”åœ–è¡¨"""
        import matplotlib.pyplot as plt
        
        model_performances = analysis_results['model_performance']
        
        models = []
        success_rates = []
        avg_times = []
        
        for model_id, data in model_performances.items():
            models.append(data['model_name'])
            success_rates.append(data['performance']['basic_stats']['success_rate'])
            avg_times.append(data['performance']['performance_stats']['avg_execution_time'])
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # æˆåŠŸç‡åœ–
        ax1.bar(models, success_rates, color='skyblue')
        ax1.set_title('æ¨¡å‹æˆåŠŸç‡å°æ¯”')
        ax1.set_ylabel('æˆåŠŸç‡')
        ax1.set_ylim(0, 1)
        
        # éŸ¿æ‡‰æ™‚é–“åœ–
        ax2.bar(models, avg_times, color='lightcoral')
        ax2.set_title('å¹³å‡éŸ¿æ‡‰æ™‚é–“å°æ¯”')
        ax2.set_ylabel('æ™‚é–“ (ç§’)')
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'performance_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_success_rate_chart(self, analysis_results: Dict[str, Any], 
                                 viz_dir: Path) -> None:
        """å‰µå»ºæˆåŠŸç‡åœ–è¡¨"""
        import matplotlib.pyplot as plt
        
        model_performances = analysis_results['model_performance']
        
        models = []
        success_counts = []
        failure_counts = []
        
        for model_id, data in model_performances.items():
            models.append(data['model_name'])
            success_counts.append(data['performance']['basic_stats']['successful_executions'])
            failure_counts.append(data['performance']['basic_stats']['failed_executions'])
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        x = range(len(models))
        width = 0.35
        
        ax.bar([i - width/2 for i in x], success_counts, width, label='æˆåŠŸ', color='green', alpha=0.7)
        ax.bar([i + width/2 for i in x], failure_counts, width, label='å¤±æ•—', color='red', alpha=0.7)
        
        ax.set_xlabel('æ¨¡å‹')
        ax.set_ylabel('åŸ·è¡Œæ¬¡æ•¸')
        ax.set_title('æ¨¡å‹åŸ·è¡ŒæˆåŠŸ/å¤±æ•—çµ±è¨ˆ')
        ax.set_xticks(x)
        ax.set_xticklabels(models)
        ax.legend()
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'success_failure_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_response_time_chart(self, analysis_results: Dict[str, Any], 
                                  viz_dir: Path) -> None:
        """å‰µå»ºéŸ¿æ‡‰æ™‚é–“åˆ†ä½ˆåœ–"""
        import matplotlib.pyplot as plt
        
        model_performances = analysis_results['model_performance']
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        for model_id, data in model_performances.items():
            model_name = data['model_name']
            perf_stats = data['performance']['performance_stats']
            
            # æ¨¡æ“¬éŸ¿æ‡‰æ™‚é–“åˆ†ä½ˆæ•¸æ“š
            import numpy as np
            mean_time = perf_stats['avg_execution_time']
            std_time = perf_stats['std_execution_time']
            
            if mean_time > 0:
                times = np.random.normal(mean_time, max(std_time, 0.1), 100)
                ax.hist(times, alpha=0.6, label=model_name, bins=20)
        
        ax.set_xlabel('éŸ¿æ‡‰æ™‚é–“ (ç§’)')
        ax.set_ylabel('é »ç‡')
        ax.set_title('æ¨¡å‹éŸ¿æ‡‰æ™‚é–“åˆ†ä½ˆ')
        ax.legend()
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'response_time_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _extract_accuracy_comparison(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """æå–æº–ç¢ºæ€§å°æ¯”"""
        accuracy_comparison = {}
        
        for model_id, data in analysis_results['model_performance'].items():
            accuracy_stats = data['performance']['accuracy_stats']
            accuracy_comparison[model_id] = {
                'model_name': data['model_name'],
                'intent_accuracy': accuracy_stats.get('intent_accuracy', 0),
                'entity_accuracy': accuracy_stats.get('entity_accuracy', 0),
                'avg_confidence': accuracy_stats.get('avg_confidence', 0)
            }
        
        return accuracy_comparison
    
    def _calculate_cost_comparison(self, analysis_results: Dict[str, Any], 
                                 config: ComparisonConfig) -> Dict[str, Any]:
        """è¨ˆç®—æˆæœ¬å°æ¯”"""
        if not config.include_cost_analysis:
            return {}
        
        cost_comparison = {}
        
        # æ¨¡æ“¬æˆæœ¬è¨ˆç®—
        for model_id, data in analysis_results['model_performance'].items():
            model_name = data['model_name']
            successful_executions = data['performance']['basic_stats']['successful_executions']
            
            # æ¨¡æ“¬æˆæœ¬ï¼ˆå¯¦éš›å¯¦ç¾éœ€è¦çœŸå¯¦çš„å®šåƒ¹ä¿¡æ¯ï¼‰
            cost_per_request = {
                'gpt-4': 0.03,
                'gpt-3.5': 0.002,
                'claude': 0.025,
                'gemini': 0.001
            }.get(model_name.lower(), 0.01)
            
            total_cost = successful_executions * cost_per_request
            cost_per_success = total_cost / successful_executions if successful_executions > 0 else 0
            
            cost_comparison[model_id] = {
                'model_name': model_name,
                'total_cost': total_cost,
                'cost_per_request': cost_per_request,
                'cost_per_success': cost_per_success,
                'cost_efficiency': successful_executions / total_cost if total_cost > 0 else 0
            }
        
        return cost_comparison
    
    def _extract_statistical_analysis(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """æå–çµ±è¨ˆåˆ†æ"""
        statistical_analysis = {
            'significance_tests': {},
            'effect_sizes': {},
            'confidence_intervals': {}
        }
        
        if 'comparison_matrix' in analysis_results:
            for comp_key, comp_data in analysis_results['comparison_matrix'].items():
                statistical_analysis['significance_tests'][comp_key] = comp_data['statistical_significance']
        
        return statistical_analysis
    
    def _create_execution_summary(self, analysis_results: Dict[str, Any], 
                                config: ComparisonConfig) -> Dict[str, Any]:
        """å‰µå»ºåŸ·è¡Œæ‘˜è¦"""
        total_executions = sum(
            data['performance']['basic_stats']['total_executions']
            for data in analysis_results['model_performance'].values()
        )
        
        total_successful = sum(
            data['performance']['basic_stats']['successful_executions']
            for data in analysis_results['model_performance'].values()
        )
        
        summary = {
            'total_models_tested': len(config.models),
            'total_test_cases': config.test_case_count,
            'total_executions': total_executions,
            'total_successful_executions': total_successful,
            'overall_success_rate': total_successful / total_executions if total_executions > 0 else 0,
            'test_duration': str(datetime.now() - self.progress.start_time),
            'comparison_scope': config.scope.value,
            'errors_encountered': len(self.progress.errors)
        }
        
        return summary
    
    def _save_comparison_report(self, result: ModelComparisonResult, file_path: str) -> None:
        """ä¿å­˜å°æ¯”å ±å‘Š"""
        data = {
            'comparison_id': result.comparison_id,
            'generation_time': result.generation_time.isoformat(),
            'model_results': result.model_results,
            'performance_comparison': result.performance_comparison,
            'accuracy_comparison': result.accuracy_comparison,
            'cost_comparison': result.cost_comparison,
            'statistical_analysis': result.statistical_analysis,
            'recommendations': result.recommendations,
            'execution_summary': result.execution_summary,
            'metadata': result.metadata
        }
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def get_progress(self) -> Optional[ComparisonProgress]:
        """ç²å–ç•¶å‰é€²åº¦"""
        return self.progress
    
    def print_comparison_summary(self, result: ModelComparisonResult) -> None:
        """æ‰“å°å°æ¯”æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ“Š LLMæ¨¡å‹æ€§èƒ½å°æ¯”æ¸¬è©¦æ‘˜è¦")
        print("="*80)
        
        print(f"\nğŸ†” å°æ¯”ID: {result.comparison_id}")
        print(f"ğŸ“… ç”Ÿæˆæ™‚é–“: {result.generation_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # åŸ·è¡Œæ‘˜è¦
        summary = result.execution_summary
        print(f"\nğŸ“ˆ åŸ·è¡Œæ‘˜è¦:")
        print(f"   æ¸¬è©¦æ¨¡å‹æ•¸: {summary['total_models_tested']}")
        print(f"   æ¸¬è©¦æ¡ˆä¾‹æ•¸: {summary['total_test_cases']}")
        print(f"   ç¸½åŸ·è¡Œæ¬¡æ•¸: {summary['total_executions']}")
        print(f"   æˆåŠŸåŸ·è¡Œæ¬¡æ•¸: {summary['total_successful_executions']}")
        print(f"   æ•´é«”æˆåŠŸç‡: {summary['overall_success_rate']:.1%}")
        print(f"   æ¸¬è©¦è€—æ™‚: {summary['test_duration']}")
        
        # æ¨¡å‹æ€§èƒ½
        print(f"\nğŸ† æ¨¡å‹æ€§èƒ½æ’å:")
        model_scores = []
        for model_id, data in result.model_results.items():
            model_name = data['model_name']
            perf = data['performance']
            
            # è¨ˆç®—ç¶œåˆåˆ†æ•¸
            success_rate = perf['basic_stats']['success_rate']
            avg_time = perf['performance_stats']['avg_execution_time']
            accuracy = perf['accuracy_stats'].get('intent_accuracy', 0)
            
            # ç°¡åŒ–çš„è©•åˆ†å…¬å¼
            score = (success_rate * 0.4 + accuracy * 0.4 + (1 / max(avg_time, 0.1)) * 0.2)
            
            model_scores.append((model_name, score, success_rate, avg_time, accuracy))
        
        # æŒ‰åˆ†æ•¸æ’åº
        model_scores.sort(key=lambda x: x[1], reverse=True)
        
        for i, (name, score, success_rate, avg_time, accuracy) in enumerate(model_scores, 1):
            print(f"   {i}. {name}")
            print(f"      ç¶œåˆåˆ†æ•¸: {score:.3f}")
            print(f"      æˆåŠŸç‡: {success_rate:.1%}")
            print(f"      å¹³å‡éŸ¿æ‡‰æ™‚é–“: {avg_time:.2f}ç§’")
            print(f"      æº–ç¢ºç‡: {accuracy:.1%}")
            print()
        
        # å»ºè­°
        print(f"ğŸ’¡ å»ºè­°:")
        for recommendation in result.recommendations:
            print(f"   â€¢ {recommendation}")
        
        # éŒ¯èª¤ä¿¡æ¯
        if result.metadata.get('errors'):
            print(f"\nâš ï¸ é‡åˆ°çš„éŒ¯èª¤:")
            for error in result.metadata['errors']:
                print(f"   â€¢ {error}")
        
        print("\n" + "="*80)


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ LLMæ€§èƒ½å°æ¯”æ¸¬è©¦å¥—ä»¶")
    print("æ•´åˆæ¸¬è©¦æ¡ˆä¾‹ç”Ÿæˆã€åŸ·è¡Œã€åˆ†æå’Œå ±å‘ŠåŠŸèƒ½ï¼Œæä¾›å®Œæ•´çš„LLMæ¨¡å‹å°æ¯”æ¸¬è©¦è§£æ±ºæ–¹æ¡ˆ\n")
    
    # å‰µå»ºæ¸¬è©¦å¥—ä»¶
    suite = LLMPerformanceComparisonSuite()
    
    # é…ç½®æ¨¡å‹
    print("âš™ï¸ é…ç½®æ¸¬è©¦æ¨¡å‹:")
    models = []
    
    while True:
        print(f"\næ·»åŠ æ¨¡å‹ #{len(models) + 1}:")
        model_name = input("æ¨¡å‹åç¨± (å¦‚: GPT-4, Claude-3, Gemini-Pro): ").strip()
        if not model_name:
            if len(models) >= 2:
                break
            else:
                print("âŒ è‡³å°‘éœ€è¦æ·»åŠ 2å€‹æ¨¡å‹é€²è¡Œå°æ¯”")
                continue
        
        provider = input("æä¾›å•† (å¦‚: OpenAI, Anthropic, Google): ").strip() or "Unknown"
        model_id = f"{provider.lower()}_{model_name.lower().replace('-', '_').replace(' ', '_')}"
        
        model_config = {
            'model_id': model_id,
            'model_name': model_name,
            'provider': provider,
            'max_tokens': 1000,
            'temperature': 0.7,
            'timeout': 30,
            'rate_limit': 10
        }
        
        models.append(model_config)
        print(f"   âœ… å·²æ·»åŠ  {model_name}")
        
        if len(models) >= 5:  # é™åˆ¶æœ€å¤š5å€‹æ¨¡å‹
            print("   âš ï¸ å·²é”åˆ°æœ€å¤§æ¨¡å‹æ•¸é‡é™åˆ¶")
            break
    
    # é¸æ“‡å°æ¯”ç¯„åœ
    print("\nğŸ¯ é¸æ“‡å°æ¯”ç¯„åœ:")
    scopes = list(ComparisonScope)
    for i, scope in enumerate(scopes, 1):
        print(f"   {i}. {scope.value}")
    
    while True:
        try:
            scope_choice = int(input("è«‹é¸æ“‡å°æ¯”ç¯„åœ (é è¨­1): ") or "1")
            if 1 <= scope_choice <= len(scopes):
                selected_scope = scopes[scope_choice - 1]
                break
            else:
                print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„é¸é …")
        except ValueError:
            print("âŒ è«‹è¼¸å…¥æ•¸å­—")
    
    # å‰µå»ºå°æ¯”é…ç½®
    config = suite.create_comparison_config(selected_scope, models)
    
    # è‡ªå®šç¾©é…ç½®
    print(f"\nâš™ï¸ ç•¶å‰é…ç½®:")
    print(f"   æ¸¬è©¦æ¡ˆä¾‹æ•¸é‡: {config.test_case_count}")
    print(f"   ä¸¦è¡ŒåŸ·è¡Œ: {config.parallel_execution}")
    print(f"   æœ€å¤§å·¥ä½œç·šç¨‹: {config.max_workers}")
    print(f"   åŒ…å«æ€§èƒ½æŒ‡æ¨™: {config.include_performance_metrics}")
    print(f"   åŒ…å«æº–ç¢ºæ€§åˆ†æ: {config.include_accuracy_analysis}")
    print(f"   åŒ…å«æˆæœ¬åˆ†æ: {config.include_cost_analysis}")
    print(f"   ç”Ÿæˆå¯è¦–åŒ–: {config.generate_visualizations}")
    
    modify_config = input("\næ˜¯å¦ä¿®æ”¹é…ç½®? (y/N): ").strip().lower()
    if modify_config == 'y':
        try:
            new_count = int(input(f"æ¸¬è©¦æ¡ˆä¾‹æ•¸é‡ (ç•¶å‰: {config.test_case_count}): ") or config.test_case_count)
            config.test_case_count = max(100, min(5000, new_count))  # é™åˆ¶ç¯„åœ
            
            new_workers = int(input(f"æœ€å¤§å·¥ä½œç·šç¨‹ (ç•¶å‰: {config.max_workers}): ") or config.max_workers)
            config.max_workers = max(1, min(10, new_workers))  # é™åˆ¶ç¯„åœ
            
            viz_choice = input(f"ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨? (Y/n): ").strip().lower()
            config.generate_visualizations = viz_choice != 'n'
            
        except ValueError:
            print("âš ï¸ ä½¿ç”¨é è¨­é…ç½®")
    
    # ç¢ºèªé–‹å§‹æ¸¬è©¦
    print(f"\nğŸ¯ æº–å‚™é–‹å§‹å°æ¯”æ¸¬è©¦:")
    print(f"   å°æ¯”ç¯„åœ: {selected_scope.value}")
    print(f"   æ¸¬è©¦æ¨¡å‹: {', '.join([m['model_name'] for m in models])}")
    print(f"   æ¸¬è©¦æ¡ˆä¾‹æ•¸: {config.test_case_count}")
    
    confirm = input("\nç¢ºèªé–‹å§‹æ¸¬è©¦? (Y/n): ").strip().lower()
    if confirm == 'n':
        print("âŒ æ¸¬è©¦å·²å–æ¶ˆ")
        return
    
    try:
        # é‹è¡Œå°æ¯”æ¸¬è©¦
        result = suite.run_comparison(config)
        
        # æ‰“å°æ‘˜è¦
        suite.print_comparison_summary(result)
        
        # è©¢å•æ˜¯å¦å°å‡ºè©³ç´°å ±å‘Š
        export_choice = input("\næ˜¯å¦å°å‡ºè©³ç´°å ±å‘Š? (Y/n): ").strip().lower()
        if export_choice != 'n':
            report_path = Path(config.output_directory) / "reports" / "detailed_comparison_report.txt"
            suite._export_detailed_report(result, str(report_path))
            print(f"ğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: {report_path}")
        
        print(f"\nğŸ‰ å°æ¯”æ¸¬è©¦å®Œæˆ! çµæœä¿å­˜åœ¨: {config.output_directory}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()


def _export_detailed_report(self, result: ModelComparisonResult, file_path: str) -> None:
    """å°å‡ºè©³ç´°å ±å‘Š"""
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write("LLMæ¨¡å‹æ€§èƒ½å°æ¯”æ¸¬è©¦è©³ç´°å ±å‘Š\n")
        f.write("=" * 50 + "\n\n")
        
        f.write(f"å°æ¯”ID: {result.comparison_id}\n")
        f.write(f"ç”Ÿæˆæ™‚é–“: {result.generation_time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # åŸ·è¡Œæ‘˜è¦
        f.write("åŸ·è¡Œæ‘˜è¦:\n")
        f.write("-" * 20 + "\n")
        for key, value in result.execution_summary.items():
            f.write(f"{key}: {value}\n")
        f.write("\n")
        
        # æ¨¡å‹è©³ç´°çµæœ
        f.write("æ¨¡å‹è©³ç´°çµæœ:\n")
        f.write("-" * 20 + "\n")
        for model_id, data in result.model_results.items():
            f.write(f"\næ¨¡å‹: {data['model_name']} ({model_id})\n")
            f.write(f"åŸºæœ¬çµ±è¨ˆ:\n")
            for key, value in data['performance']['basic_stats'].items():
                f.write(f"  {key}: {value}\n")
            
            f.write(f"æ€§èƒ½çµ±è¨ˆ:\n")
            for key, value in data['performance']['performance_stats'].items():
                f.write(f"  {key}: {value}\n")
            
            f.write(f"æº–ç¢ºæ€§çµ±è¨ˆ:\n")
            for key, value in data['performance']['accuracy_stats'].items():
                f.write(f"  {key}: {value}\n")
        
        # å»ºè­°
        f.write("\nå»ºè­°:\n")
        f.write("-" * 20 + "\n")
        for recommendation in result.recommendations:
            f.write(f"â€¢ {recommendation}\n")
        
        f.write("\nå ±å‘ŠçµæŸ\n")


# å°‡æ–¹æ³•æ·»åŠ åˆ°é¡ä¸­
LLMPerformanceComparisonSuite._export_detailed_report = _export_detailed_report


if __name__ == "__main__":
    main()