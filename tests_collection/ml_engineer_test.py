#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ML Engineer è·ä½æ¸¬è©¦è…³æœ¬
æ¸¬è©¦æ¯å€‹æ±‚è·ç¶²ç«™æœå°‹ML Engineerè·ä½ï¼Œæ¯å€‹ç¶²ç«™15å€‹ï¼Œé™åˆ¶åœ¨è¿‘7æ—¥å…§çš„æ–°å¢å·¥ä½œå´—ä½
"""

import os
import json
import csv
from datetime import datetime, timedelta
from typing import List, Dict, Any
import pandas as pd
from jobspy import scrape_jobs
from jobspy.model import Site


def create_output_directory() -> str:
    """
    å‰µå»ºè¼¸å‡ºç›®éŒ„
    
    Returns:
        str: è¼¸å‡ºç›®éŒ„è·¯å¾‘
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"ml_engineer_test_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    return output_dir


def scrape_site_jobs(site: Site, output_dir: str) -> Dict[str, Any]:
    """
    çˆ¬å–æŒ‡å®šç¶²ç«™çš„ML Engineerè·ä½
    
    Args:
        site: æ±‚è·ç¶²ç«™
        output_dir: è¼¸å‡ºç›®éŒ„
        
    Returns:
        Dict: åŒ…å«ç¶²ç«™åç¨±ã€è·ä½æ•¸é‡ã€åŸ·è¡Œæ™‚é–“ç­‰è³‡è¨Šçš„å­—å…¸
    """
    site_name = site.value
    print(f"\né–‹å§‹æ¸¬è©¦ {site_name}...")
    
    start_time = datetime.now()
    
    try:
        # è¨ˆç®—7å¤©å‰çš„æ—¥æœŸ
        seven_days_ago = datetime.now() - timedelta(days=7)
        
        # çˆ¬å–è·ä½è³‡æ–™
        jobs_df = scrape_jobs(
            site_name=site,
            search_term="ML Engineer",
            location="Australia",  # å¯ä»¥æ ¹æ“šéœ€è¦èª¿æ•´åœ°å€
            results_wanted=15,
            hours_old=168,  # 7å¤© = 168å°æ™‚
            country_indeed="Australia"  # é‡å°Indeedçš„ç‰¹æ®Šåƒæ•¸
        )
        
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        if jobs_df is not None and not jobs_df.empty:
            # ä¿å­˜ç‚ºCSV
            csv_filename = os.path.join(output_dir, f"{site_name}_ml_engineer_jobs.csv")
            jobs_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            
            # ä¿å­˜åŸå§‹JSONæ•¸æ“š
            json_filename = os.path.join(output_dir, f"{site_name}_ml_engineer_raw_data.json")
            jobs_data = jobs_df.to_dict('records')
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump(jobs_data, f, ensure_ascii=False, indent=2, default=str)
            
            job_count = len(jobs_df)
            print(f"âœ… {site_name}: æˆåŠŸç²å– {job_count} å€‹è·ä½")
            
            return {
                'site': site_name,
                'status': 'success',
                'job_count': job_count,
                'execution_time': execution_time,
                'avg_time_per_job': execution_time / job_count if job_count > 0 else 0,
                'csv_file': csv_filename,
                'json_file': json_filename,
                'error': None
            }
        else:
            print(f"âŒ {site_name}: æœªæ‰¾åˆ°è·ä½")
            return {
                'site': site_name,
                'status': 'no_results',
                'job_count': 0,
                'execution_time': execution_time,
                'avg_time_per_job': 0,
                'csv_file': None,
                'json_file': None,
                'error': 'No jobs found'
            }
            
    except Exception as e:
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        error_msg = str(e)
        print(f"âŒ {site_name}: éŒ¯èª¤ - {error_msg}")
        
        return {
            'site': site_name,
            'status': 'error',
            'job_count': 0,
            'execution_time': execution_time,
            'avg_time_per_job': 0,
            'csv_file': None,
            'json_file': None,
            'error': error_msg
        }


def combine_all_results(results: List[Dict], output_dir: str) -> None:
    """
    åˆä½µæ‰€æœ‰ç¶²ç«™çš„çµæœ
    
    Args:
        results: æ‰€æœ‰ç¶²ç«™çš„æ¸¬è©¦çµæœ
        output_dir: è¼¸å‡ºç›®éŒ„
    """
    # åˆä½µæ‰€æœ‰æˆåŠŸçš„CSVæ–‡ä»¶
    all_jobs = []
    
    for result in results:
        if result['status'] == 'success' and result['csv_file']:
            try:
                df = pd.read_csv(result['csv_file'])
                df['source_site'] = result['site']
                all_jobs.append(df)
            except Exception as e:
                print(f"è®€å– {result['site']} CSVæ–‡ä»¶æ™‚å‡ºéŒ¯: {e}")
    
    if all_jobs:
        combined_df = pd.concat(all_jobs, ignore_index=True)
        combined_csv = os.path.join(output_dir, "all_sites_ml_engineer_combined.csv")
        combined_df.to_csv(combined_csv, index=False, encoding='utf-8-sig')
        print(f"\nâœ… åˆä½µæ–‡ä»¶å·²ä¿å­˜: {combined_csv}")
        print(f"ç¸½è¨ˆè·ä½æ•¸é‡: {len(combined_df)}")


def generate_test_report(results: List[Dict], output_dir: str) -> None:
    """
    ç”Ÿæˆæ¸¬è©¦å ±å‘Š
    
    Args:
        results: æ‰€æœ‰ç¶²ç«™çš„æ¸¬è©¦çµæœ
        output_dir: è¼¸å‡ºç›®éŒ„
    """
    report_file = os.path.join(output_dir, "ml_engineer_test_report.txt")
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("ML Engineer è·ä½æ¸¬è©¦å ±å‘Š\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"æ¸¬è©¦æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æœå°‹é—œéµå­—: ML Engineer\n")
        f.write(f"æ¯å€‹ç¶²ç«™ç›®æ¨™è·ä½æ•¸: 15\n")
        f.write(f"æ™‚é–“ç¯„åœ: è¿‘7å¤©å…§\n\n")
        
        # çµ±è¨ˆè³‡è¨Š
        successful_sites = [r for r in results if r['status'] == 'success']
        total_jobs = sum(r['job_count'] for r in successful_sites)
        total_time = sum(r['execution_time'] for r in results)
        
        f.write("æ¸¬è©¦çµ±è¨ˆ:\n")
        f.write("-" * 30 + "\n")
        f.write(f"æ¸¬è©¦ç¶²ç«™ç¸½æ•¸: {len(results)}\n")
        f.write(f"æˆåŠŸç¶²ç«™æ•¸: {len(successful_sites)}\n")
        f.write(f"æˆåŠŸç‡: {len(successful_sites)/len(results)*100:.1f}%\n")
        f.write(f"ç¸½è·ä½æ•¸: {total_jobs}\n")
        f.write(f"ç¸½åŸ·è¡Œæ™‚é–“: {total_time:.2f} ç§’\n")
        f.write(f"å¹³å‡æ¯å€‹ç¶²ç«™è€—æ™‚: {total_time/len(results):.2f} ç§’\n\n")
        
        # å„ç¶²ç«™è©³ç´°çµæœ
        f.write("å„ç¶²ç«™è©³ç´°çµæœ:\n")
        f.write("-" * 30 + "\n")
        
        for result in results:
            f.write(f"\nç¶²ç«™: {result['site']}\n")
            f.write(f"  ç‹€æ…‹: {result['status']}\n")
            f.write(f"  è·ä½æ•¸é‡: {result['job_count']}\n")
            f.write(f"  åŸ·è¡Œæ™‚é–“: {result['execution_time']:.2f} ç§’\n")
            
            if result['job_count'] > 0:
                f.write(f"  å¹³å‡æ¯å€‹è·ä½è€—æ™‚: {result['avg_time_per_job']:.2f} ç§’\n")
            
            if result['error']:
                f.write(f"  éŒ¯èª¤ä¿¡æ¯: {result['error']}\n")
            
            if result['csv_file']:
                f.write(f"  CSVæ–‡ä»¶: {os.path.basename(result['csv_file'])}\n")
            if result['json_file']:
                f.write(f"  JSONæ–‡ä»¶: {os.path.basename(result['json_file'])}\n")
        
        # ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨
        f.write("\n\nç”Ÿæˆçš„æ–‡ä»¶:\n")
        f.write("-" * 30 + "\n")
        for result in results:
            if result['csv_file']:
                f.write(f"- {os.path.basename(result['csv_file'])}\n")
            if result['json_file']:
                f.write(f"- {os.path.basename(result['json_file'])}\n")
        f.write("- all_sites_ml_engineer_combined.csv\n")
        f.write("- ml_engineer_test_report.txt\n")
    
    print(f"\nğŸ“Š æ¸¬è©¦å ±å‘Šå·²ç”Ÿæˆ: {report_file}")


def main():
    """
    ä¸»å‡½æ•¸ï¼šåŸ·è¡ŒML Engineerè·ä½æ¸¬è©¦
    """
    print("ğŸš€ é–‹å§‹ML Engineerè·ä½æ¸¬è©¦")
    print("æœå°‹æ¢ä»¶: ML Engineer, æ¯å€‹ç¶²ç«™15å€‹è·ä½, è¿‘7å¤©å…§")
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    output_dir = create_output_directory()
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {output_dir}")
    
    # æ”¯æ´çš„ç¶²ç«™åˆ—è¡¨
    supported_sites = [
        Site.LINKEDIN,
        Site.INDEED, 
        Site.ZIP_RECRUITER,
        Site.GLASSDOOR,
        Site.GOOGLE,
        Site.BAYT,
        Site.NAUKRI,
        Site.BDJOBS,
        Site.SEEK
    ]
    
    results = []
    
    # æ¸¬è©¦æ¯å€‹ç¶²ç«™
    for site in supported_sites:
        result = scrape_site_jobs(site, output_dir)
        results.append(result)
    
    # åˆä½µçµæœ
    combine_all_results(results, output_dir)
    
    # ç”Ÿæˆå ±å‘Š
    generate_test_report(results, output_dir)
    
    print("\nğŸ‰ ML Engineerè·ä½æ¸¬è©¦å®Œæˆï¼")
    print(f"ğŸ“ æ‰€æœ‰çµæœå·²ä¿å­˜åˆ°: {output_dir}")


if __name__ == "__main__":
    main()