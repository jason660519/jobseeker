#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ML Engineer è·ä½æ¸¬è©¦è…³æœ¬ (ä¿®æ­£ç‰ˆ)
ä¿®æ­£äº†å¤±æ•—ç¶²ç«™çš„å•é¡Œï¼š
- ZipRecruiter: æ·»åŠ å»¶é²é¿å…429éŒ¯èª¤
- Google: èª¿æ•´æœå°‹åƒæ•¸å’Œåœ°å€è¨­å®š
- Bayt: æ·»åŠ é‡è©¦æ©Ÿåˆ¶å’ŒéŒ¯èª¤è™•ç†
- BDJobs: ä¿®æ­£user_agentåƒæ•¸å•é¡Œ
"""

import os
import json
import csv
import time
import random
from datetime import datetime, timedelta
from typing import List, Dict, Any
import pandas as pd
from jobspy import scrape_jobs
from jobspy.model import Site


def create_output_directory() -> str:
    """
    å‰µå»ºè¼¸å‡ºç›®éŒ„
    
    Returns:
        str: è¼¸å‡ºç›®éŒ„è·¯å¾‘
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"ml_engineer_test_fixed_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    return output_dir


def add_random_delay(min_seconds: float = 2.0, max_seconds: float = 5.0) -> None:
    """
    æ·»åŠ éš¨æ©Ÿå»¶é²ä»¥é¿å…è¢«ç¶²ç«™é˜»æ“‹
    
    Args:
        min_seconds: æœ€å°å»¶é²ç§’æ•¸
        max_seconds: æœ€å¤§å»¶é²ç§’æ•¸
    """
    delay = random.uniform(min_seconds, max_seconds)
    print(f"â³ ç­‰å¾… {delay:.1f} ç§’ä»¥é¿å…è¢«é˜»æ“‹...")
    time.sleep(delay)


def scrape_site_jobs_with_retry(site: Site, output_dir: str, max_retries: int = 3) -> Dict[str, Any]:
    """
    å¸¶é‡è©¦æ©Ÿåˆ¶çš„è·ä½çˆ¬å–å‡½æ•¸
    
    Args:
        site: æ±‚è·ç¶²ç«™
        output_dir: è¼¸å‡ºç›®éŒ„
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
        
    Returns:
        Dict: åŒ…å«ç¶²ç«™åç¨±ã€è·ä½æ•¸é‡ã€åŸ·è¡Œæ™‚é–“ç­‰è³‡è¨Šçš„å­—å…¸
    """
    site_name = site.value
    print(f"\né–‹å§‹æ¸¬è©¦ {site_name}...")
    
    # é‡å°ä¸åŒç¶²ç«™æ·»åŠ å»¶é²
    if site in [Site.ZIP_RECRUITER, Site.BAYT]:
        add_random_delay(3.0, 6.0)  # è¼ƒé•·å»¶é²
    else:
        add_random_delay(1.0, 3.0)  # æ¨™æº–å»¶é²
    
    start_time = datetime.now()
    
    for attempt in range(max_retries):
        try:
            # æ ¹æ“šä¸åŒç¶²ç«™èª¿æ•´åƒæ•¸
            scrape_params = {
                'site_name': site,
                'search_term': "ML Engineer",
                'results_wanted': 15,
                'hours_old': 168,  # 7å¤© = 168å°æ™‚
            }
            
            # é‡å°ç‰¹å®šç¶²ç«™çš„åƒæ•¸èª¿æ•´
            if site == Site.GOOGLE:
                # Google éœ€è¦æ›´å…·é«”çš„æœå°‹è©å’Œåœ°å€
                scrape_params.update({
                    'search_term': "Machine Learning Engineer",
                    'location': "Sydney, Australia",
                    'country_indeed': "Australia"
                })
            elif site == Site.BAYT:
                # Bayt ä¸»è¦æœå‹™ä¸­æ±åœ°å€
                scrape_params.update({
                    'location': "Dubai, UAE",
                    'search_term': "Machine Learning Engineer"
                })
            elif site == Site.BDJOBS:
                # BDJobs æœå‹™å­ŸåŠ æ‹‰ï¼Œç§»é™¤å¯èƒ½å°è‡´éŒ¯èª¤çš„åƒæ•¸
                scrape_params.update({
                    'location': "Dhaka, Bangladesh",
                    'search_term': "ML Engineer"
                })
                # ä¸å‚³é user_agent åƒæ•¸çµ¦ BDJobs
            elif site == Site.ZIP_RECRUITER:
                # ZipRecruiter éœ€è¦ç¾åœ‹åœ°å€
                scrape_params.update({
                    'location': "United States",
                    'country_indeed': "USA"
                })
            else:
                # å…¶ä»–ç¶²ç«™ä½¿ç”¨æ¾³æ´²ä½œç‚ºé è¨­åœ°å€
                scrape_params.update({
                    'location': "Australia",
                    'country_indeed': "Australia"
                })
            
            print(f"ğŸ”„ å˜—è©¦ {attempt + 1}/{max_retries}: {site_name}")
            
            # çˆ¬å–è·ä½è³‡æ–™
            jobs_df = scrape_jobs(**scrape_params)
            
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            
            if jobs_df is not None and not jobs_df.empty:
                # ä¿å­˜ç‚ºCSV
                csv_filename = os.path.join(output_dir, f"{site_name}_ml_engineer_jobs.csv")
                jobs_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
                
                # ä¿å­˜åŸå§‹JSONæ•¸æ“š
                json_filename = os.path.join(output_dir, f"{site_name}_ml_engineer_raw_data.json")
                jobs_data = jobs_df.to_dict('records')
                with open(json_filename, 'w', encoding='utf-8') as f:
                    json.dump(jobs_data, f, ensure_ascii=False, indent=2, default=str)
                
                job_count = len(jobs_df)
                print(f"âœ… {site_name}: æˆåŠŸç²å– {job_count} å€‹è·ä½")
                
                return {
                    'site': site_name,
                    'status': 'success',
                    'job_count': job_count,
                    'execution_time': execution_time,
                    'avg_time_per_job': execution_time / job_count if job_count > 0 else 0,
                    'csv_file': csv_filename,
                    'json_file': json_filename,
                    'error': None,
                    'attempts': attempt + 1
                }
            else:
                if attempt < max_retries - 1:
                    print(f"âš ï¸ {site_name}: ç¬¬ {attempt + 1} æ¬¡å˜—è©¦æœªæ‰¾åˆ°è·ä½ï¼Œæº–å‚™é‡è©¦...")
                    add_random_delay(5.0, 10.0)  # é‡è©¦å‰è¼ƒé•·å»¶é²
                    continue
                else:
                    print(f"âŒ {site_name}: æ‰€æœ‰å˜—è©¦å¾Œä»æœªæ‰¾åˆ°è·ä½")
                    end_time = datetime.now()
                    execution_time = (end_time - start_time).total_seconds()
                    return {
                        'site': site_name,
                        'status': 'no_results',
                        'job_count': 0,
                        'execution_time': execution_time,
                        'avg_time_per_job': 0,
                        'csv_file': None,
                        'json_file': None,
                        'error': 'No jobs found after all attempts',
                        'attempts': max_retries
                    }
                    
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ {site_name}: ç¬¬ {attempt + 1} æ¬¡å˜—è©¦éŒ¯èª¤ - {error_msg}")
            
            if attempt < max_retries - 1:
                print(f"ğŸ”„ æº–å‚™é‡è©¦ {site_name}...")
                add_random_delay(5.0, 10.0)  # éŒ¯èª¤å¾Œè¼ƒé•·å»¶é²
                continue
            else:
                end_time = datetime.now()
                execution_time = (end_time - start_time).total_seconds()
                return {
                    'site': site_name,
                    'status': 'error',
                    'job_count': 0,
                    'execution_time': execution_time,
                    'avg_time_per_job': 0,
                    'csv_file': None,
                    'json_file': None,
                    'error': error_msg,
                    'attempts': max_retries
                }


def combine_all_results(results: List[Dict], output_dir: str) -> None:
    """
    åˆä½µæ‰€æœ‰ç¶²ç«™çš„çµæœ
    
    Args:
        results: æ‰€æœ‰ç¶²ç«™çš„æ¸¬è©¦çµæœ
        output_dir: è¼¸å‡ºç›®éŒ„
    """
    # åˆä½µæ‰€æœ‰æˆåŠŸçš„CSVæ–‡ä»¶
    all_jobs = []
    
    for result in results:
        if result['status'] == 'success' and result['csv_file']:
            try:
                df = pd.read_csv(result['csv_file'])
                df['source_site'] = result['site']
                all_jobs.append(df)
            except Exception as e:
                print(f"è®€å– {result['site']} CSVæ–‡ä»¶æ™‚å‡ºéŒ¯: {e}")
    
    if all_jobs:
        combined_df = pd.concat(all_jobs, ignore_index=True)
        combined_csv = os.path.join(output_dir, "all_sites_ml_engineer_combined.csv")
        combined_df.to_csv(combined_csv, index=False, encoding='utf-8-sig')
        print(f"\nâœ… åˆä½µæ–‡ä»¶å·²ä¿å­˜: {combined_csv}")
        print(f"ç¸½è¨ˆè·ä½æ•¸é‡: {len(combined_df)}")


def generate_test_report(results: List[Dict], output_dir: str) -> None:
    """
    ç”Ÿæˆæ¸¬è©¦å ±å‘Š
    
    Args:
        results: æ‰€æœ‰ç¶²ç«™çš„æ¸¬è©¦çµæœ
        output_dir: è¼¸å‡ºç›®éŒ„
    """
    report_file = os.path.join(output_dir, "ml_engineer_test_report_fixed.txt")
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("ML Engineer è·ä½æ¸¬è©¦å ±å‘Š (ä¿®æ­£ç‰ˆ)\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"æ¸¬è©¦æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æœå°‹é—œéµå­—: ML Engineer\n")
        f.write(f"æ¯å€‹ç¶²ç«™ç›®æ¨™è·ä½æ•¸: 15\n")
        f.write(f"æ™‚é–“ç¯„åœ: è¿‘7å¤©å…§\n")
        f.write(f"ä¿®æ­£å…§å®¹: æ·»åŠ å»¶é²ã€é‡è©¦æ©Ÿåˆ¶ã€åƒæ•¸èª¿æ•´\n\n")
        
        # çµ±è¨ˆè³‡è¨Š
        successful_sites = [r for r in results if r['status'] == 'success']
        total_jobs = sum(r['job_count'] for r in successful_sites)
        total_time = sum(r['execution_time'] for r in results)
        
        f.write("æ¸¬è©¦çµ±è¨ˆ:\n")
        f.write("-" * 30 + "\n")
        f.write(f"æ¸¬è©¦ç¶²ç«™ç¸½æ•¸: {len(results)}\n")
        f.write(f"æˆåŠŸç¶²ç«™æ•¸: {len(successful_sites)}\n")
        f.write(f"æˆåŠŸç‡: {len(successful_sites)/len(results)*100:.1f}%\n")
        f.write(f"ç¸½è·ä½æ•¸: {total_jobs}\n")
        f.write(f"ç¸½åŸ·è¡Œæ™‚é–“: {total_time:.2f} ç§’\n")
        f.write(f"å¹³å‡æ¯å€‹ç¶²ç«™è€—æ™‚: {total_time/len(results):.2f} ç§’\n\n")
        
        # å„ç¶²ç«™è©³ç´°çµæœ
        f.write("å„ç¶²ç«™è©³ç´°çµæœ:\n")
        f.write("-" * 30 + "\n")
        
        for result in results:
            f.write(f"\nç¶²ç«™: {result['site']}\n")
            f.write(f"  ç‹€æ…‹: {result['status']}\n")
            f.write(f"  è·ä½æ•¸é‡: {result['job_count']}\n")
            f.write(f"  åŸ·è¡Œæ™‚é–“: {result['execution_time']:.2f} ç§’\n")
            f.write(f"  å˜—è©¦æ¬¡æ•¸: {result.get('attempts', 1)}\n")
            
            if result['job_count'] > 0:
                f.write(f"  å¹³å‡æ¯å€‹è·ä½è€—æ™‚: {result['avg_time_per_job']:.2f} ç§’\n")
            
            if result['error']:
                f.write(f"  éŒ¯èª¤ä¿¡æ¯: {result['error']}\n")
            
            if result['csv_file']:
                f.write(f"  CSVæ–‡ä»¶: {os.path.basename(result['csv_file'])}\n")
            if result['json_file']:
                f.write(f"  JSONæ–‡ä»¶: {os.path.basename(result['json_file'])}\n")
        
        # ä¿®æ­£èªªæ˜
        f.write("\n\nä¿®æ­£æªæ–½èªªæ˜:\n")
        f.write("-" * 30 + "\n")
        f.write("- ZipRecruiter: æ·»åŠ 3-6ç§’éš¨æ©Ÿå»¶é²ï¼Œä½¿ç”¨ç¾åœ‹åœ°å€\n")
        f.write("- Google: èª¿æ•´æœå°‹è©ç‚º'Machine Learning Engineer'ï¼ŒæŒ‡å®šæ‚‰å°¼åœ°å€\n")
        f.write("- Bayt: æ·»åŠ é‡è©¦æ©Ÿåˆ¶ï¼Œä½¿ç”¨æœæ‹œåœ°å€ï¼Œå¢åŠ å»¶é²\n")
        f.write("- BDJobs: ç§»é™¤user_agentåƒæ•¸ï¼Œä½¿ç”¨é”å¡åœ°å€\n")
        f.write("- æ‰€æœ‰ç¶²ç«™: æ·»åŠ éš¨æ©Ÿå»¶é²å’Œé‡è©¦æ©Ÿåˆ¶\n")
        
        # ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨
        f.write("\n\nç”Ÿæˆçš„æ–‡ä»¶:\n")
        f.write("-" * 30 + "\n")
        for result in results:
            if result['csv_file']:
                f.write(f"- {os.path.basename(result['csv_file'])}\n")
            if result['json_file']:
                f.write(f"- {os.path.basename(result['json_file'])}\n")
        f.write("- all_sites_ml_engineer_combined.csv\n")
        f.write("- ml_engineer_test_report_fixed.txt\n")
    
    print(f"\nğŸ“Š æ¸¬è©¦å ±å‘Šå·²ç”Ÿæˆ: {report_file}")


def main():
    """
    ä¸»å‡½æ•¸ï¼šåŸ·è¡ŒML Engineerè·ä½æ¸¬è©¦ (ä¿®æ­£ç‰ˆ)
    """
    print("ğŸš€ é–‹å§‹ML Engineerè·ä½æ¸¬è©¦ (ä¿®æ­£ç‰ˆ)")
    print("æœå°‹æ¢ä»¶: ML Engineer, æ¯å€‹ç¶²ç«™15å€‹è·ä½, è¿‘7å¤©å…§")
    print("ä¿®æ­£å…§å®¹: æ·»åŠ å»¶é²ã€é‡è©¦æ©Ÿåˆ¶ã€åƒæ•¸èª¿æ•´")
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    output_dir = create_output_directory()
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {output_dir}")
    
    # æ”¯æ´çš„ç¶²ç«™åˆ—è¡¨ (é‡é»æ¸¬è©¦ä¹‹å‰å¤±æ•—çš„ç¶²ç«™)
    supported_sites = [
        Site.LINKEDIN,      # åƒè€ƒåŸºæº–
        Site.INDEED,        # åƒè€ƒåŸºæº–
        Site.ZIP_RECRUITER, # ä¿®æ­£: æ·»åŠ å»¶é²
        Site.GLASSDOOR,     # åƒè€ƒåŸºæº–
        Site.GOOGLE,        # ä¿®æ­£: èª¿æ•´æœå°‹åƒæ•¸
        Site.BAYT,          # ä¿®æ­£: é‡è©¦æ©Ÿåˆ¶
        Site.NAUKRI,        # åƒè€ƒåŸºæº–
        Site.BDJOBS,        # ä¿®æ­£: ç§»é™¤user_agent
        Site.SEEK           # åƒè€ƒåŸºæº–
    ]
    
    results = []
    
    # æ¸¬è©¦æ¯å€‹ç¶²ç«™
    for i, site in enumerate(supported_sites):
        print(f"\né€²åº¦: {i+1}/{len(supported_sites)}")
        result = scrape_site_jobs_with_retry(site, output_dir)
        results.append(result)
        
        # åœ¨ç¶²ç«™ä¹‹é–“æ·»åŠ å»¶é²
        if i < len(supported_sites) - 1:
            add_random_delay(2.0, 4.0)
    
    # åˆä½µçµæœ
    combine_all_results(results, output_dir)
    
    # ç”Ÿæˆå ±å‘Š
    generate_test_report(results, output_dir)
    
    # é¡¯ç¤ºä¿®æ­£çµæœæ‘˜è¦
    successful_sites = [r for r in results if r['status'] == 'success']
    failed_sites = [r for r in results if r['status'] != 'success']
    
    print(f"\nğŸ‰ ML Engineerè·ä½æ¸¬è©¦å®Œæˆ (ä¿®æ­£ç‰ˆ)ï¼")
    print(f"ğŸ“ æ‰€æœ‰çµæœå·²ä¿å­˜åˆ°: {output_dir}")
    print(f"\nğŸ“Š ä¿®æ­£çµæœæ‘˜è¦:")
    print(f"âœ… æˆåŠŸç¶²ç«™: {len(successful_sites)}/{len(supported_sites)}")
    if failed_sites:
        print(f"âŒ ä»ç„¶å¤±æ•—çš„ç¶²ç«™:")
        for site in failed_sites:
            print(f"   - {site['site']}: {site['error']}")


if __name__ == "__main__":
    main()