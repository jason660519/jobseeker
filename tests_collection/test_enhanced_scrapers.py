#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼·ç‰ˆçˆ¬èŸ²æ¸¬è©¦è…³æœ¬
å°ˆé–€æ¸¬è©¦ ZipRecruiterã€Google Jobs å’Œ Bayt çš„ä¿®å¾©æ•ˆæœ
"""

import os
import sys
import json
import time
from datetime import datetime
from typing import Dict, Any, List

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from jobspy import scrape_jobs
from jobspy.model import Site


def setup_enhanced_environment():
    """
    è¨­ç½®å¢å¼·ç‰ˆçˆ¬èŸ²ç’°å¢ƒè®Šæ•¸
    """
    os.environ['ENABLE_ALL_ENHANCED'] = 'true'
    os.environ['ENABLE_ENHANCED_ZIPRECRUITER'] = 'true'
    os.environ['ENABLE_ENHANCED_GOOGLE'] = 'true'
    os.environ['ENABLE_ENHANCED_BAYT'] = 'true'
    print("âœ… å·²å•Ÿç”¨æ‰€æœ‰å¢å¼·ç‰ˆçˆ¬èŸ²")


def add_random_delay(min_seconds: float = 2.0, max_seconds: float = 5.0):
    """
    æ·»åŠ éš¨æ©Ÿå»¶é²
    """
    import random
    delay = random.uniform(min_seconds, max_seconds)
    print(f"â³ å»¶é² {delay:.1f} ç§’...")
    time.sleep(delay)


def test_enhanced_scraper(site: Site, site_name: str, output_dir: str, max_retries: int = 2) -> Dict[str, Any]:
    """
    æ¸¬è©¦å–®å€‹å¢å¼·ç‰ˆçˆ¬èŸ²
    
    Args:
        site: æ±‚è·ç¶²ç«™æšèˆ‰
        site_name: ç¶²ç«™åç¨±
        output_dir: è¼¸å‡ºç›®éŒ„
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
        
    Returns:
        Dict: æ¸¬è©¦çµæœ
    """
    print(f"\nğŸš€ é–‹å§‹æ¸¬è©¦å¢å¼·ç‰ˆ {site_name}...")
    
    # é‡å°ä¸åŒç¶²ç«™æ·»åŠ å»¶é²
    if site in [Site.ZIP_RECRUITER, Site.BAYT]:
        add_random_delay(3.0, 6.0)  # è¼ƒé•·å»¶é²
    else:
        add_random_delay(1.0, 3.0)  # æ¨™æº–å»¶é²
    
    start_time = datetime.now()
    
    for attempt in range(max_retries):
        try:
            print(f"ğŸ”„ å˜—è©¦ {attempt + 1}/{max_retries}: {site_name}")
            
            # æ ¹æ“šä¸åŒç¶²ç«™èª¿æ•´åƒæ•¸
            scrape_params = {
                'site_name': site,
                'search_term': "ML Engineer",
                'results_wanted': 10,  # æ¸›å°‘æ•¸é‡ä»¥åŠ å¿«æ¸¬è©¦
                'hours_old': 168,  # 7å¤©
                'verbose': 1
            }
            
            # é‡å°ç‰¹å®šç¶²ç«™çš„åƒæ•¸èª¿æ•´
            if site == Site.GOOGLE:
                scrape_params.update({
                    'search_term': "Machine Learning Engineer",
                    'location': "Sydney, Australia",
                    'country_indeed': "Australia"
                })
            elif site == Site.BAYT:
                scrape_params.update({
                    'location': "Dubai, UAE",
                    'search_term': "Machine Learning Engineer"
                })
            elif site == Site.ZIP_RECRUITER:
                scrape_params.update({
                    'location': "United States",
                    'country_indeed': "USA"
                })
            
            # çˆ¬å–è·ä½è³‡æ–™
            jobs_df = scrape_jobs(**scrape_params)
            
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            
            if jobs_df is not None and not jobs_df.empty:
                # ä¿å­˜ç‚º CSV
                csv_filename = os.path.join(output_dir, f"enhanced_{site_name}_ml_engineer_jobs.csv")
                jobs_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
                
                # ä¿å­˜ç‚º JSON
                json_filename = os.path.join(output_dir, f"enhanced_{site_name}_ml_engineer_raw_data.json")
                jobs_df.to_json(json_filename, orient='records', indent=2, force_ascii=False)
                
                print(f"âœ… {site_name} æˆåŠŸæ‰¾åˆ° {len(jobs_df)} å€‹è·ä½")
                print(f"ğŸ“ CSV æª”æ¡ˆ: {csv_filename}")
                print(f"ğŸ“ JSON æª”æ¡ˆ: {json_filename}")
                
                return {
                    'site': site_name,
                    'status': 'success',
                    'job_count': len(jobs_df),
                    'execution_time': execution_time,
                    'attempts': attempt + 1,
                    'avg_time_per_job': execution_time / len(jobs_df),
                    'csv_file': csv_filename,
                    'json_file': json_filename,
                    'sample_jobs': jobs_df.head(3).to_dict('records') if len(jobs_df) > 0 else []
                }
            else:
                print(f"âš ï¸ {site_name} ç¬¬ {attempt + 1} æ¬¡å˜—è©¦æœªæ‰¾åˆ°è·ä½")
                if attempt < max_retries - 1:
                    add_random_delay(5.0, 10.0)  # é‡è©¦å‰ç­‰å¾…æ›´é•·æ™‚é–“
                    
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ {site_name} ç¬¬ {attempt + 1} æ¬¡å˜—è©¦å¤±æ•—: {error_msg}")
            
            if attempt < max_retries - 1:
                add_random_delay(5.0, 10.0)  # é‡è©¦å‰ç­‰å¾…æ›´é•·æ™‚é–“
            else:
                end_time = datetime.now()
                execution_time = (end_time - start_time).total_seconds()
                
                return {
                    'site': site_name,
                    'status': 'error',
                    'job_count': 0,
                    'execution_time': execution_time,
                    'attempts': max_retries,
                    'error_message': error_msg
                }
    
    # æ‰€æœ‰å˜—è©¦éƒ½å¤±æ•—
    end_time = datetime.now()
    execution_time = (end_time - start_time).total_seconds()
    
    return {
        'site': site_name,
        'status': 'no_results',
        'job_count': 0,
        'execution_time': execution_time,
        'attempts': max_retries,
        'error_message': 'No jobs found after all attempts'
    }


def generate_enhanced_test_report(results: List[Dict[str, Any]], output_dir: str):
    """
    ç”Ÿæˆå¢å¼·ç‰ˆæ¸¬è©¦å ±å‘Š
    """
    report_filename = os.path.join(output_dir, "enhanced_scrapers_test_report.txt")
    
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write("å¢å¼·ç‰ˆçˆ¬èŸ²æ¸¬è©¦å ±å‘Š\n")
        f.write("=" * 50 + "\n")
        f.write(f"æ¸¬è©¦æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ¸¬è©¦ç¶²ç«™: ZipRecruiter, Google Jobs, Bayt\n")
        f.write("\n")
        
        # çµ±è¨ˆæ‘˜è¦
        successful_sites = [r for r in results if r['status'] == 'success']
        failed_sites = [r for r in results if r['status'] in ['error', 'no_results']]
        total_jobs = sum(r['job_count'] for r in successful_sites)
        
        f.write("æ¸¬è©¦æ‘˜è¦:\n")
        f.write("-" * 30 + "\n")
        f.write(f"æˆåŠŸç¶²ç«™æ•¸: {len(successful_sites)}/3\n")
        f.write(f"å¤±æ•—ç¶²ç«™æ•¸: {len(failed_sites)}/3\n")
        f.write(f"ç¸½è·ä½æ•¸: {total_jobs}\n")
        f.write(f"å¹³å‡æˆåŠŸç‡: {len(successful_sites)/3*100:.1f}%\n")
        f.write("\n")
        
        # è©³ç´°çµæœ
        f.write("è©³ç´°æ¸¬è©¦çµæœ:\n")
        f.write("-" * 30 + "\n")
        
        for result in results:
            f.write(f"\nç¶²ç«™: {result['site']}\n")
            f.write(f"  ç‹€æ…‹: {result['status']}\n")
            f.write(f"  è·ä½æ•¸é‡: {result['job_count']}\n")
            f.write(f"  åŸ·è¡Œæ™‚é–“: {result['execution_time']:.2f} ç§’\n")
            f.write(f"  å˜—è©¦æ¬¡æ•¸: {result['attempts']}\n")
            
            if result['status'] == 'success':
                f.write(f"  å¹³å‡æ¯å€‹è·ä½è€—æ™‚: {result['avg_time_per_job']:.2f} ç§’\n")
                f.write(f"  CSVæ–‡ä»¶: {os.path.basename(result['csv_file'])}\n")
                f.write(f"  JSONæ–‡ä»¶: {os.path.basename(result['json_file'])}\n")
                
                # é¡¯ç¤ºæ¨£æœ¬è·ä½
                if result['sample_jobs']:
                    f.write("  æ¨£æœ¬è·ä½:\n")
                    for i, job in enumerate(result['sample_jobs'][:2], 1):
                        f.write(f"    {i}. {job.get('title', 'N/A')} - {job.get('company_name', 'N/A')}\n")
            else:
                f.write(f"  éŒ¯èª¤ä¿¡æ¯: {result.get('error_message', 'Unknown error')}\n")
        
        # ä¿®å¾©æ•ˆæœåˆ†æ
        f.write("\n\nä¿®å¾©æ•ˆæœåˆ†æ:\n")
        f.write("-" * 30 + "\n")
        
        for result in results:
            site_name = result['site']
            if result['status'] == 'success':
                f.write(f"âœ… {site_name}: ä¿®å¾©æˆåŠŸï¼Œæ‰¾åˆ° {result['job_count']} å€‹è·ä½\n")
            elif result['status'] == 'no_results':
                f.write(f"âš ï¸ {site_name}: ä¿®å¾©éƒ¨åˆ†æˆåŠŸï¼Œç„¡éŒ¯èª¤ä½†æœªæ‰¾åˆ°è·ä½\n")
            else:
                f.write(f"âŒ {site_name}: ä¿®å¾©å¤±æ•—ï¼ŒéŒ¯èª¤: {result.get('error_message', 'Unknown')}\n")
        
        # ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨
        f.write("\n\nç”Ÿæˆçš„æ–‡ä»¶:\n")
        f.write("-" * 30 + "\n")
        for result in results:
            if result['status'] == 'success':
                f.write(f"- {os.path.basename(result['csv_file'])}\n")
                f.write(f"- {os.path.basename(result['json_file'])}\n")
    
    print(f"\nğŸ“Š æ¸¬è©¦å ±å‘Šå·²ç”Ÿæˆ: {report_filename}")
    return report_filename


def main():
    """
    ä¸»æ¸¬è©¦å‡½æ•¸
    """
    print("ğŸ”§ å¢å¼·ç‰ˆçˆ¬èŸ²æ¸¬è©¦é–‹å§‹")
    print("=" * 50)
    
    # è¨­ç½®å¢å¼·ç‰ˆç’°å¢ƒ
    setup_enhanced_environment()
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"enhanced_test_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {output_dir}")
    
    # è¦æ¸¬è©¦çš„ç¶²ç«™
    test_sites = [
        (Site.ZIP_RECRUITER, "ziprecruiter"),
        (Site.GOOGLE, "google"),
        (Site.BAYT, "bayt")
    ]
    
    results = []
    
    # é€ä¸€æ¸¬è©¦æ¯å€‹ç¶²ç«™
    for site, site_name in test_sites:
        try:
            result = test_enhanced_scraper(site, site_name, output_dir)
            results.append(result)
        except Exception as e:
            print(f"âŒ æ¸¬è©¦ {site_name} æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {str(e)}")
            results.append({
                'site': site_name,
                'status': 'error',
                'job_count': 0,
                'execution_time': 0,
                'attempts': 0,
                'error_message': f'Unexpected error: {str(e)}'
            })
        
        # ç¶²ç«™é–“å»¶é²
        if site != test_sites[-1][0]:  # ä¸æ˜¯æœ€å¾Œä¸€å€‹ç¶²ç«™
            add_random_delay(3.0, 8.0)
    
    # ç”Ÿæˆæ¸¬è©¦å ±å‘Š
    report_file = generate_enhanced_test_report(results, output_dir)
    
    # é¡¯ç¤ºæ¸¬è©¦æ‘˜è¦
    print("\n" + "=" * 50)
    print("ğŸ¯ å¢å¼·ç‰ˆçˆ¬èŸ²æ¸¬è©¦å®Œæˆ")
    print("=" * 50)
    
    successful_sites = [r for r in results if r['status'] == 'success']
    total_jobs = sum(r['job_count'] for r in successful_sites)
    
    print(f"âœ… æˆåŠŸç¶²ç«™: {len(successful_sites)}/3")
    print(f"ğŸ“Š ç¸½è·ä½æ•¸: {total_jobs}")
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {output_dir}")
    print(f"ğŸ“‹ æ¸¬è©¦å ±å‘Š: {os.path.basename(report_file)}")
    
    # é¡¯ç¤ºå„ç¶²ç«™çµæœ
    for result in results:
        status_emoji = "âœ…" if result['status'] == 'success' else "âŒ"
        print(f"{status_emoji} {result['site']}: {result['job_count']} å€‹è·ä½")
    
    return results


if __name__ == "__main__":
    try:
        results = main()
    except KeyboardInterrupt:
        print("\nâš ï¸ æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()