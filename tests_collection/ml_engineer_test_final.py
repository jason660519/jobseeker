#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ML Engineer è·ä½æ¸¬è©¦è…³æœ¬ - æœ€çµ‚ä¿®æ­£ç‰ˆ
ä¿®æ­£æ‰€æœ‰å¤±æ•—ç¶²ç«™çš„å•é¡Œ
"""

import os
import time
import random
from datetime import datetime
from jobspy import scrape_jobs
from jobspy.model import Site
import pandas as pd

def create_output_directory():
    """å‰µå»ºè¼¸å‡ºç›®éŒ„"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"ml_engineer_test_final_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    return output_dir

def scrape_site_jobs_with_retry(site, site_name, output_dir, max_retries=3):
    """å¸¶é‡è©¦æ©Ÿåˆ¶çš„è·ä½çˆ¬å–å‡½æ•¸"""
    for attempt in range(max_retries):
        try:
            start_time = datetime.now()
            
            # åŸºæœ¬åƒæ•¸
            scrape_params = {
                'site_name': site,
                'search_term': "ML Engineer",
                'results_wanted': 15,
                'country_indeed': "Australia",
                'description_format': "markdown",
                'verbose': 1
            }
            
            # é‡å°ä¸åŒç¶²ç«™çš„ç‰¹æ®Šé…ç½®
            if site == Site.GOOGLE:
                # Google éœ€è¦æ›´å…·é«”çš„æœå°‹è©å’Œåœ°å€
                scrape_params.update({
                    'search_term': "Machine Learning Engineer",
                    'location': "Sydney, Australia",
                    'google_search_term': "Machine Learning Engineer jobs Sydney"
                })
            elif site == Site.BAYT:
                # Bayt æœå‹™ä¸­æ±åœ°å€
                scrape_params.update({
                    'location': "Dubai, UAE",
                    'search_term': "Machine Learning Engineer"
                })
            elif site == Site.BDJOBS:
                # BDJobs æœå‹™å­ŸåŠ æ‹‰ï¼Œä¸å‚³é user_agent åƒæ•¸
                scrape_params.update({
                    'location': "Dhaka, Bangladesh",
                    'search_term': "ML Engineer"
                })
                # é‡è¦ï¼šä¸ç‚º BDJobs è¨­ç½® user_agent
            elif site == Site.ZIP_RECRUITER:
                # ZipRecruiter éœ€è¦ç¾åœ‹åœ°å€
                scrape_params.update({
                    'location': "United States",
                    'country_indeed': "USA"
                })
            else:
                # å…¶ä»–ç¶²ç«™ä½¿ç”¨æ¾³æ´²ä½œç‚ºé è¨­åœ°å€
                scrape_params.update({
                    'location': "Australia",
                    'country_indeed': "Australia"
                })
            
            # åªç‚ºæ”¯æ´ user_agent çš„ç¶²ç«™æ·»åŠ è©²åƒæ•¸
            if site != Site.BDJOBS:
                scrape_params['user_agent'] = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            
            print(f"ğŸ”„ å˜—è©¦ {attempt + 1}/{max_retries}: {site_name}")
            
            # æ·»åŠ éš¨æ©Ÿå»¶é²ä»¥é¿å…è¢«é˜»æ“‹
            if site in [Site.ZIP_RECRUITER, Site.BAYT]:
                delay = random.uniform(3, 6)
                print(f"   â³ å»¶é² {delay:.1f} ç§’...")
                time.sleep(delay)
            else:
                time.sleep(2)  # åŸºæœ¬å»¶é²
            
            # çˆ¬å–è·ä½è³‡æ–™
            jobs_df = scrape_jobs(**scrape_params)
            
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            
            if jobs_df is not None and not jobs_df.empty:
                # ä¿å­˜ç‚ºCSV
                csv_filename = os.path.join(output_dir, f"{site_name}_ml_engineer_jobs.csv")
                jobs_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
                
                # ä¿å­˜åŸå§‹JSONæ•¸æ“š
                json_filename = os.path.join(output_dir, f"{site_name}_ml_engineer_raw_data.json")
                jobs_df.to_json(json_filename, orient='records', indent=2, force_ascii=False)
                
                print(f"âœ… {site_name}: æ‰¾åˆ° {len(jobs_df)} å€‹è·ä½ (è€—æ™‚: {execution_time:.2f}ç§’)")
                return {
                    'site': site_name,
                    'status': 'success',
                    'jobs_count': len(jobs_df),
                    'execution_time': execution_time,
                    'attempts': attempt + 1,
                    'csv_file': csv_filename,
                    'json_file': json_filename,
                    'jobs_df': jobs_df
                }
            else:
                print(f"âš ï¸ {site_name}: æœªæ‰¾åˆ°è·ä½ (å˜—è©¦ {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 5
                    print(f"   â³ ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                    time.sleep(wait_time)
                    
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ {site_name}: éŒ¯èª¤ - {error_msg} (å˜—è©¦ {attempt + 1}/{max_retries})")
            
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 5
                print(f"   â³ ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                time.sleep(wait_time)
            else:
                end_time = datetime.now()
                execution_time = (end_time - start_time).total_seconds()
                return {
                    'site': site_name,
                    'status': 'error',
                    'jobs_count': 0,
                    'execution_time': execution_time,
                    'attempts': max_retries,
                    'error': error_msg
                }
    
    # æ‰€æœ‰å˜—è©¦éƒ½å¤±æ•—
    return {
        'site': site_name,
        'status': 'no_results',
        'jobs_count': 0,
        'execution_time': 0,
        'attempts': max_retries,
        'error': 'No jobs found after all attempts'
    }

def combine_all_results(results, output_dir):
    """åˆä½µæ‰€æœ‰ç¶²ç«™çš„çµæœ"""
    all_jobs = []
    
    for result in results:
        if result['status'] == 'success' and 'jobs_df' in result:
            jobs_df = result['jobs_df']
            all_jobs.append(jobs_df)
    
    if all_jobs:
        combined_df = pd.concat(all_jobs, ignore_index=True)
        combined_filename = os.path.join(output_dir, "all_sites_ml_engineer_combined.csv")
        combined_df.to_csv(combined_filename, index=False, encoding='utf-8-sig')
        print(f"\nğŸ“Š åˆä½µæ–‡ä»¶å·²ä¿å­˜: {combined_filename}")
        print(f"ğŸ“Š ç¸½è¨ˆè·ä½æ•¸é‡: {len(combined_df)}")
        return combined_filename, len(combined_df)
    
    return None, 0

def generate_test_report(results, output_dir, total_jobs, total_time):
    """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
    report_filename = os.path.join(output_dir, "ml_engineer_test_report_final.txt")
    
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write("ML Engineer è·ä½æ¸¬è©¦å ±å‘Š - æœ€çµ‚ä¿®æ­£ç‰ˆ\n")
        f.write("=" * 50 + "\n")
        f.write(f"æ¸¬è©¦æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ç¸½åŸ·è¡Œæ™‚é–“: {total_time:.2f} ç§’\n")
        f.write(f"æ¸¬è©¦ç¶²ç«™æ•¸é‡: {len(results)}\n")
        
        successful_sites = [r for r in results if r['status'] == 'success']
        f.write(f"æˆåŠŸç¶²ç«™æ•¸é‡: {len(successful_sites)}\n")
        f.write(f"ç¸½è·ä½æ•¸é‡: {total_jobs}\n\n")
        
        f.write("è©³ç´°çµæœ:\n")
        f.write("-" * 30 + "\n")
        
        for result in results:
            f.write(f"ç¶²ç«™: {result['site']}\n")
            f.write(f"  ç‹€æ…‹: {result['status']}\n")
            f.write(f"  è·ä½æ•¸é‡: {result['jobs_count']}\n")
            f.write(f"  åŸ·è¡Œæ™‚é–“: {result['execution_time']:.2f} ç§’\n")
            f.write(f"  å˜—è©¦æ¬¡æ•¸: {result['attempts']}\n")
            
            if result['status'] == 'success':
                avg_time = result['execution_time'] / result['jobs_count'] if result['jobs_count'] > 0 else 0
                f.write(f"  å¹³å‡æ¯å€‹è·ä½è€—æ™‚: {avg_time:.2f} ç§’\n")
                f.write(f"  CSVæ–‡ä»¶: {os.path.basename(result['csv_file'])}\n")
                f.write(f"  JSONæ–‡ä»¶: {os.path.basename(result['json_file'])}\n")
            elif 'error' in result:
                f.write(f"  éŒ¯èª¤ä¿¡æ¯: {result['error']}\n")
            
            f.write("\n")
        
        f.write("æœ€çµ‚ä¿®æ­£æªæ–½èªªæ˜:\n")
        f.write("-" * 30 + "\n")
        f.write("- ZipRecruiter: æ·»åŠ 3-6ç§’éš¨æ©Ÿå»¶é²ï¼Œä½¿ç”¨ç¾åœ‹åœ°å€\n")
        f.write("- Google: èª¿æ•´æœå°‹è©ç‚º'Machine Learning Engineer'ï¼ŒæŒ‡å®šæ‚‰å°¼åœ°å€\n")
        f.write("- Bayt: æ·»åŠ é‡è©¦æ©Ÿåˆ¶ï¼Œä½¿ç”¨æœæ‹œåœ°å€ï¼Œå¢åŠ å»¶é²\n")
        f.write("- BDJobs: å®Œå…¨ç§»é™¤user_agentåƒæ•¸ï¼Œä½¿ç”¨é”å¡åœ°å€\n")
        f.write("- æ‰€æœ‰ç¶²ç«™: æ·»åŠ éš¨æ©Ÿå»¶é²å’Œé‡è©¦æ©Ÿåˆ¶\n\n")
        
        f.write("ç”Ÿæˆçš„æ–‡ä»¶:\n")
        f.write("-" * 30 + "\n")
        
        for result in results:
            if result['status'] == 'success':
                f.write(f"- {os.path.basename(result['csv_file'])}\n")
                f.write(f"- {os.path.basename(result['json_file'])}\n")
        
        f.write("- all_sites_ml_engineer_combined.csv\n")
        f.write("- ml_engineer_test_report_final.txt\n")
    
    print(f"\nğŸ“‹ æ¸¬è©¦å ±å‘Šå·²ä¿å­˜: {report_filename}")
    return report_filename

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹ ML Engineer è·ä½æ¸¬è©¦ - æœ€çµ‚ä¿®æ­£ç‰ˆ")
    print("=" * 50)
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    output_dir = create_output_directory()
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {output_dir}")
    
    # æ”¯æ´çš„ç¶²ç«™åˆ—è¡¨
    sites_to_test = [
        (Site.LINKEDIN, "linkedin"),
        (Site.INDEED, "indeed"),
        (Site.ZIP_RECRUITER, "ziprecruiter"),
        (Site.GLASSDOOR, "glassdoor"),
        (Site.GOOGLE, "google"),
        (Site.BAYT, "bayt"),
        (Site.NAUKRI, "naukri"),
        (Site.BDJOBS, "bdjobs"),
        (Site.SEEK, "seek")
    ]
    
    results = []
    start_time = datetime.now()
    
    # æ¸¬è©¦æ¯å€‹ç¶²ç«™
    for site, site_name in sites_to_test:
        print(f"\nğŸ” æ¸¬è©¦ç¶²ç«™: {site_name.upper()}")
        result = scrape_site_jobs_with_retry(site, site_name, output_dir)
        results.append(result)
    
    end_time = datetime.now()
    total_time = (end_time - start_time).total_seconds()
    
    # åˆä½µçµæœ
    print("\n" + "=" * 50)
    print("ğŸ“Š åˆä½µæ‰€æœ‰çµæœ...")
    combined_file, total_jobs = combine_all_results(results, output_dir)
    
    # ç”Ÿæˆå ±å‘Š
    print("\nğŸ“‹ ç”Ÿæˆæ¸¬è©¦å ±å‘Š...")
    report_file = generate_test_report(results, output_dir, total_jobs, total_time)
    
    # ç¸½çµ
    print("\n" + "=" * 50)
    print("ğŸ¯ æ¸¬è©¦å®Œæˆï¼")
    print(f"ğŸ“ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_dir}")
    print(f"â±ï¸ ç¸½åŸ·è¡Œæ™‚é–“: {total_time:.2f} ç§’")
    print(f"ğŸ“Š ç¸½è·ä½æ•¸é‡: {total_jobs}")
    
    successful_sites = [r for r in results if r['status'] == 'success']
    print(f"âœ… æˆåŠŸç¶²ç«™: {len(successful_sites)}/{len(sites_to_test)}")
    
    if successful_sites:
        print("\næˆåŠŸçš„ç¶²ç«™:")
        for result in successful_sites:
            print(f"  - {result['site']}: {result['jobs_count']} å€‹è·ä½")
    
    failed_sites = [r for r in results if r['status'] != 'success']
    if failed_sites:
        print("\nä»ç„¶å¤±æ•—çš„ç¶²ç«™:")
        for result in failed_sites:
            print(f"  - {result['site']}: {result.get('error', 'Unknown error')}")

if __name__ == "__main__":
    main()