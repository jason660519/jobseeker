#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŠ æ‹¿å¤§ AI Research Scientist è·ä½æœå°‹æ¸¬è©¦
æ¸¬è©¦ç›®æ¨™ï¼šæœå°‹å¤šå€«å¤šå’Œæº«å“¥è¯çš„ AI Research Scientist è·ä½
æ¸¬è©¦é¡å‹ï¼šç ”ç©¶è·ä½ + åŠ æ‹¿å¤§AIç”Ÿæ…‹ç³»çµ±æ¸¬è©¦
"""

import sys
import os
import json
import pandas as pd
from datetime import datetime
from pathlib import Path

# æ·»åŠ  jobseeker æ¨¡çµ„è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jobseeker import scrape_jobs
from jobseeker.model import Site, Country

def convert_types(obj):
    """
    å°‡ numpy/pandas æ•¸æ“šé¡å‹è½‰æ›ç‚º Python åŸç”Ÿé¡å‹ï¼Œä»¥ä¾¿ JSON åºåˆ—åŒ–
    """
    if pd.isna(obj):
        return None
    elif hasattr(obj, 'item'):  # numpy types
        return obj.item()
    elif isinstance(obj, (pd.Timestamp, pd.Timedelta)):
        return str(obj)
    else:
        return obj

def test_canada_ai_research_scientist():
    """
    æ¸¬è©¦åŠ æ‹¿å¤§åœ°å€ AI Research Scientist è·ä½æœå°‹åŠŸèƒ½
    """
    print("=== åŠ æ‹¿å¤§ AI Research Scientist è·ä½æœå°‹æ¸¬è©¦ ===")
    
    # æ¸¬è©¦åƒæ•¸
    test_params = {
        'toronto': {
            'site_name': [Site.INDEED, Site.LINKEDIN],
            'search_term': 'AI Research Scientist',
            'location': 'Toronto, ON',
            'country': Country.CANADA,
            'results_wanted': 50,
            'hours_old': 168,  # ä¸€é€±å…§
            'job_type': 'fulltime'
        },
        'vancouver': {
            'site_name': [Site.INDEED, Site.LINKEDIN],
            'search_term': 'AI Research Scientist',
            'location': 'Vancouver, BC',
            'country': Country.CANADA,
            'results_wanted': 50,
            'hours_old': 168,  # ä¸€é€±å…§
            'job_type': 'fulltime'
        }
    }
    
    all_jobs = []
    test_results = {}
    
    # ç‚ºæ¯å€‹åŸå¸‚åŸ·è¡Œæœå°‹
    for city, params in test_params.items():
        print(f"\nğŸ” æœå°‹ {city.title()} çš„ AI Research Scientist è·ä½...")
        
        try:
            jobs = scrape_jobs(**params)
            
            if jobs is not None and not jobs.empty:
                print(f"âœ… {city.title()} æ‰¾åˆ° {len(jobs)} å€‹è·ä½")
                
                # æ·»åŠ åŸå¸‚æ¨™è¨˜
                jobs['search_city'] = city.title()
                all_jobs.append(jobs)
                
                # æ”¶é›†æ¸¬è©¦çµæœ
                test_results[city] = {
                    'job_count': len(jobs),
                    'sites': jobs['site'].value_counts().to_dict() if 'site' in jobs.columns else {},
                    'companies': jobs['company'].value_counts().head(5).to_dict() if 'company' in jobs.columns else {},
                    'avg_salary': jobs['salary_avg'].mean() if 'salary_avg' in jobs.columns else None
                }
                
                # é¡¯ç¤ºåŸºæœ¬çµ±è¨ˆ
                print(f"   ç¶²ç«™åˆ†å¸ƒ: {dict(jobs['site'].value_counts()) if 'site' in jobs.columns else 'N/A'}")
                if 'salary_avg' in jobs.columns and jobs['salary_avg'].notna().any():
                    avg_salary = jobs['salary_avg'].mean()
                    print(f"   å¹³å‡è–ªè³‡: CAD ${avg_salary:,.0f}")
                    
                # åˆ†æç ”ç©¶é ˜åŸŸ
                if 'description' in jobs.columns:
                    descriptions = ' '.join(jobs['description'].fillna('').astype(str))
                    research_areas = ['computer vision', 'natural language processing', 'reinforcement learning', 'robotics', 'healthcare ai']
                    area_counts = {area: descriptions.lower().count(area) for area in research_areas}
                    top_areas = sorted(area_counts.items(), key=lambda x: x[1], reverse=True)[:3]
                    print(f"   ç†±é–€ç ”ç©¶é ˜åŸŸ: {dict(top_areas)}")
                    
            else:
                print(f"âŒ {city.title()} æœªæ‰¾åˆ°è·ä½")
                test_results[city] = {'job_count': 0, 'error': 'No jobs found'}
                
        except Exception as e:
            print(f"âŒ {city.title()} æœå°‹å¤±æ•—: {str(e)}")
            test_results[city] = {'job_count': 0, 'error': str(e)}
    
    # åˆä½µæ‰€æœ‰çµæœ
    if all_jobs:
        combined_jobs = pd.concat(all_jobs, ignore_index=True)
        total_jobs = len(combined_jobs)
        print(f"\nğŸ“Š ç¸½è¨ˆæ‰¾åˆ° {total_jobs} å€‹ AI Research Scientist è·ä½")
        
        # å‰µå»ºçµæœç›®éŒ„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_dir = Path(f"tests_collection/user_prompt_tests/phase3_specialized_tests/canada_ai_research_scientist_{timestamp}")
        result_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è©³ç´°çµæœ
        combined_jobs.to_csv(result_dir / "canada_ai_research_scientist_jobs.csv", index=False, encoding='utf-8')
        
        # ç ”ç©¶é ˜åŸŸåˆ†æ
        research_analysis = {}
        if 'description' in combined_jobs.columns:
            all_descriptions = ' '.join(combined_jobs['description'].fillna('').astype(str))
            research_keywords = {
                'computer_vision': ['computer vision', 'cv', 'image recognition', 'object detection'],
                'nlp': ['nlp', 'natural language processing', 'text mining', 'language model'],
                'machine_learning': ['machine learning', 'ml', 'supervised learning', 'unsupervised learning'],
                'deep_learning': ['deep learning', 'neural network', 'cnn', 'rnn', 'transformer'],
                'reinforcement_learning': ['reinforcement learning', 'rl', 'q-learning'],
                'robotics': ['robotics', 'autonomous', 'robot'],
                'healthcare_ai': ['healthcare', 'medical ai', 'biomedical', 'clinical'],
                'autonomous_vehicles': ['autonomous vehicle', 'self-driving', 'automotive'],
                'quantum_computing': ['quantum', 'quantum computing'],
                'ethics_ai': ['ai ethics', 'fairness', 'bias', 'responsible ai']
            }
            
            for area, keywords in research_keywords.items():
                count = sum(all_descriptions.lower().count(keyword) for keyword in keywords)
                research_analysis[area] = count
        
        # æ©Ÿæ§‹é¡å‹åˆ†æ
        institution_analysis = {}
        if 'company' in combined_jobs.columns:
            companies = combined_jobs['company'].value_counts()
            # åˆ†é¡æ©Ÿæ§‹é¡å‹
            tech_companies = ['Google', 'Microsoft', 'Amazon', 'Facebook', 'Apple', 'Nvidia', 'OpenAI']
            universities = ['University of Toronto', 'UBC', 'McGill', 'Waterloo', 'Alberta']
            research_labs = ['Vector Institute', 'Mila', 'CIFAR', 'Element AI']
            canadian_companies = ['Shopify', 'BlackBerry', 'Cohere', 'Layer 6']
            
            institution_analysis = {
                'tech_companies': sum(companies.get(company, 0) for company in tech_companies),
                'universities': sum(companies.get(company, 0) for company in universities),
                'research_labs': sum(companies.get(company, 0) for company in research_labs),
                'canadian_companies': sum(companies.get(company, 0) for company in canadian_companies),
                'others': len(combined_jobs) - sum(companies.get(company, 0) for company in tech_companies + universities + research_labs + canadian_companies)
            }
        
        # ç”Ÿæˆåˆ†æå ±å‘Š
        analysis = {
            'test_info': {
                'test_name': 'Canada AI Research Scientist Search',
                'test_date': datetime.now().isoformat(),
                'total_jobs': total_jobs,
                'cities_tested': list(test_params.keys())
            },
            'research_analysis': research_analysis,
            'institution_analysis': institution_analysis,
            'city_results': {}
        }
        
        for city in test_params.keys():
            city_data = combined_jobs[combined_jobs['search_city'] == city.title()]
            if not city_data.empty:
                analysis['city_results'][city] = {
                    'job_count': len(city_data),
                    'site_distribution': {k: convert_types(v) for k, v in city_data['site'].value_counts().to_dict().items()} if 'site' in city_data.columns else {},
                    'top_companies': {k: convert_types(v) for k, v in city_data['company'].value_counts().head(5).to_dict().items()} if 'company' in city_data.columns else {},
                    'salary_info': {
                        'avg_salary': convert_types(city_data['salary_avg'].mean()) if 'salary_avg' in city_data.columns else None,
                        'min_salary': convert_types(city_data['salary_min'].min()) if 'salary_min' in city_data.columns else None,
                        'max_salary': convert_types(city_data['salary_max'].max()) if 'salary_max' in city_data.columns else None
                    }
                }
        
        # ä¿å­˜åˆ†æçµæœ
        with open(result_dir / "analysis_report.json", 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆæ¸¬è©¦å ±å‘Š
        top_research_areas = sorted(research_analysis.items(), key=lambda x: x[1], reverse=True)[:5] if research_analysis else []
        
        report_content = f"""# åŠ æ‹¿å¤§ AI Research Scientist è·ä½æœå°‹æ¸¬è©¦å ±å‘Š

## æ¸¬è©¦æ¦‚è¦
- **æ¸¬è©¦æ—¥æœŸ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **æ¸¬è©¦åŸå¸‚**: Toronto, Vancouver
- **æœå°‹è·ä½**: AI Research Scientist
- **ç¸½è·ä½æ•¸**: {total_jobs}

## ç ”ç©¶é ˜åŸŸåˆ†æ
{chr(10).join([f"- **{area.replace('_', ' ').title()}**: {count} æ¬¡æåŠ" for area, count in top_research_areas]) if top_research_areas else "- ç ”ç©¶é ˜åŸŸåˆ†æè³‡æ–™ä¸è¶³"}

## æ©Ÿæ§‹é¡å‹åˆ†å¸ƒ
{f"- **è·¨åœ‹ç§‘æŠ€å…¬å¸**: {institution_analysis.get('tech_companies', 0)} å€‹è·ä½" if institution_analysis else ""}
{f"- **å¤§å­¸é™¢æ ¡**: {institution_analysis.get('universities', 0)} å€‹è·ä½" if institution_analysis else ""}
{f"- **ç ”ç©¶æ©Ÿæ§‹**: {institution_analysis.get('research_labs', 0)} å€‹è·ä½" if institution_analysis else ""}
{f"- **åŠ æ‹¿å¤§æœ¬åœŸå…¬å¸**: {institution_analysis.get('canadian_companies', 0)} å€‹è·ä½" if institution_analysis else ""}
{f"- **å…¶ä»–æ©Ÿæ§‹**: {institution_analysis.get('others', 0)} å€‹è·ä½" if institution_analysis else ""}

## åŸå¸‚åˆ†æ

### Toronto
- è·ä½æ•¸é‡: {test_results.get('toronto', {}).get('job_count', 0)}
- ç¶²ç«™åˆ†å¸ƒ: {test_results.get('toronto', {}).get('sites', {})}
{f"- å¹³å‡è–ªè³‡: CAD ${test_results.get('toronto', {}).get('avg_salary', 0):,.0f}" if test_results.get('toronto', {}).get('avg_salary') else ""}

### Vancouver
- è·ä½æ•¸é‡: {test_results.get('vancouver', {}).get('job_count', 0)}
- ç¶²ç«™åˆ†å¸ƒ: {test_results.get('vancouver', {}).get('sites', {})}
{f"- å¹³å‡è–ªè³‡: CAD ${test_results.get('vancouver', {}).get('avg_salary', 0):,.0f}" if test_results.get('vancouver', {}).get('avg_salary') else ""}

## åŠ æ‹¿å¤§AIç”Ÿæ…‹ç³»çµ±æ´å¯Ÿ
- Toronto: Vector Instituteã€å¤šå€«å¤šå¤§å­¸ç­‰é ‚å°–AIç ”ç©¶æ©Ÿæ§‹èšé›†åœ°
- Vancouver: UBCã€åŠ æ‹¿å¤§AIç ”ç©¶çš„é‡è¦æ“šé»
- æ”¿åºœæ”¯æŒï¼šåŠ æ‹¿å¤§AIæˆ°ç•¥æŠ•è³‡è¶…é$125M
- äººæ‰å„ªå‹¢ï¼šä¸–ç•Œç´šAIç ”ç©¶äººå“¡å¦‚Geoffrey Hintonã€Yoshua Bengio
- ç§»æ°‘æ”¿ç­–ï¼šGlobal Talent Streamå¸å¼•åœ‹éš›AIäººæ‰

## æ¸¬è©¦çµè«–
âœ… åŠ æ‹¿å¤§ AI Research Scientist è·ä½æœå°‹æ¸¬è©¦å®Œæˆ
ğŸ“ çµæœå·²ä¿å­˜è‡³: {result_dir}
"""
        
        with open(result_dir / "test_report.md", 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"\nğŸ“ æ¸¬è©¦çµæœå·²ä¿å­˜è‡³: {result_dir}")
        return True
        
    else:
        print("\nâŒ æ‰€æœ‰åŸå¸‚éƒ½æœªæ‰¾åˆ°è·ä½")
        return False

if __name__ == "__main__":
    success = test_canada_ai_research_scientist()
    if success:
        print("\nğŸ‰ åŠ æ‹¿å¤§ AI Research Scientist æ¸¬è©¦æˆåŠŸå®Œæˆï¼")
    else:
        print("\nğŸ’¥ åŠ æ‹¿å¤§ AI Research Scientist æ¸¬è©¦å¤±æ•—")
        sys.exit(1)