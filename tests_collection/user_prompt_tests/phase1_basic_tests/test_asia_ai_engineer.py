#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
äºæ´²AIå·¥ç¨‹å¸«è·ä½æœå°‹æ¸¬è©¦ (å°åŒ—ã€æ±äº¬)

æ¸¬è©¦ç”¨æˆ¶æç¤º: "æœå°‹å°åŒ—å’Œæ±äº¬è¿‘7æ—¥å‰µå»ºçš„AI Engineerè·ä½"

Author: JobSpy Team
Date: 2025-01-09
"""

import os
import sys
import pandas as pd
from datetime import datetime
from pathlib import Path

# æ·»åŠ jobseekeræ¨¡çµ„åˆ°è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jobseeker import scrape_jobs
from jobseeker.model import Site


def test_asia_ai_engineer():
    """
    åŸ·è¡Œäºæ´²AIå·¥ç¨‹å¸«è·ä½æœå°‹æ¸¬è©¦ (å°åŒ—ã€æ±äº¬)
    
    æ¸¬è©¦åƒæ•¸:
    - è·ä½: AI Engineer
    - åœ°é»: Taipei, Tokyo
    - ç¶²ç«™: Indeed, LinkedIn
    - æ™‚é–“é™åˆ¶: è¿‘7æ—¥
    
    Returns:
        dict: æ¸¬è©¦çµæœ
    """
    print("ğŸš€ é–‹å§‹åŸ·è¡Œäºæ´²AIå·¥ç¨‹å¸«è·ä½æœå°‹æ¸¬è©¦")
    print("ğŸ“ ç”¨æˆ¶æç¤º: æœå°‹å°åŒ—å’Œæ±äº¬è¿‘7æ—¥å‰µå»ºçš„AI Engineerè·ä½")
    
    # æ¸¬è©¦é…ç½®
    test_config = {
        'job_title': 'AI Engineer',
        'locations': [
            {'name': 'Taipei, Taiwan', 'country': 'Taiwan'},
            {'name': 'Tokyo, Japan', 'country': 'Japan'}
        ],
        'sites': [Site.INDEED, Site.LINKEDIN],
        'results_wanted': 50,
        'hours_old': 168  # 7å¤© = 168å°æ™‚
    }
    
    # å‰µå»ºçµæœç›®éŒ„
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = Path(__file__).parent / f"asia_ai_engineer_{timestamp}"
    results_dir.mkdir(exist_ok=True)
    
    all_jobs = []
    test_results = {
        'test_name': 'asia_ai_engineer',
        'user_prompt': 'æœå°‹å°åŒ—å’Œæ±äº¬è¿‘7æ—¥å‰µå»ºçš„AI Engineerè·ä½',
        'start_time': datetime.now(),
        'locations_tested': [],
        'total_jobs': 0,
        'jobs_by_location': {},
        'jobs_by_site': {},
        'recent_jobs_count': 0,
        'status': 'running'
    }
    
    try:
        # ç‚ºæ¯å€‹åœ°é»åŸ·è¡Œæœå°‹
        for location_info in test_config['locations']:
            location = location_info['name']
            country = location_info['country']
            print(f"\nğŸ” æœå°‹åœ°é»: {location}")
            print(f"ğŸŒ åœ‹å®¶è¨­å®š: {country}")
            print(f"ğŸ“… æ™‚é–“é™åˆ¶: è¿‘7æ—¥")
            
            try:
                # åŸ·è¡ŒJobSpyæœå°‹
                jobs_df = scrape_jobs(
                    site_name=test_config['sites'],
                    search_term=test_config['job_title'],
                    location=location,
                    results_wanted=test_config['results_wanted'],
                    hours_old=test_config['hours_old'],
                    country_indeed=country
                )
                
                if jobs_df is not None and not jobs_df.empty:
                    all_jobs.append(jobs_df)
                    job_count = len(jobs_df)
                    test_results['jobs_by_location'][location] = job_count
                    test_results['locations_tested'].append(location)
                    print(f"âœ… {location}: æ‰¾åˆ° {job_count} å€‹è·ä½")
                    
                    # å„²å­˜å–®ä¸€åœ°é»çµæœ
                    location_file = results_dir / f"{location.replace(', ', '_').replace(' ', '_').lower()}_jobs.csv"
                    jobs_df.to_csv(location_file, index=False, encoding='utf-8-sig')
                    
                    # åˆ†ææ—¥æœŸè³‡è¨Š
                    if 'date_posted' in jobs_df.columns:
                        recent_count = len(jobs_df.dropna(subset=['date_posted']))
                        print(f"ğŸ“… æœ‰ç™¼å¸ƒæ—¥æœŸè³‡è¨Šçš„è·ä½: {recent_count}")
                    
                else:
                    print(f"âš ï¸  {location}: æ²’æœ‰æ‰¾åˆ°è·ä½")
                    test_results['jobs_by_location'][location] = 0
                    
            except Exception as e:
                print(f"âŒ {location} æœå°‹å¤±æ•—: {str(e)}")
                test_results['jobs_by_location'][location] = 0
        
        # åˆä½µæ‰€æœ‰çµæœ
        if all_jobs:
            combined_df = pd.concat(all_jobs, ignore_index=True)
            
            # ç§»é™¤é‡è¤‡è·ä½ (åŸºæ–¼job_url)
            original_count = len(combined_df)
            combined_df = combined_df.drop_duplicates(subset=['job_url'], keep='first')
            final_count = len(combined_df)
            
            print(f"\nğŸ“Š åˆä½µçµæœ: {original_count} â†’ {final_count} (ç§»é™¤ {original_count - final_count} å€‹é‡è¤‡è·ä½)")
            
            # çµ±è¨ˆç¶²ç«™åˆ†å¸ƒ
            if 'site' in combined_df.columns:
                site_counts = combined_df['site'].value_counts()
                test_results['jobs_by_site'] = site_counts.to_dict()
                print("\nğŸŒ ç¶²ç«™åˆ†å¸ƒ:")
                for site, count in site_counts.items():
                    print(f"   {site}: {count} å€‹è·ä½")
            
            # åˆ†ææ—¥æœŸè³‡è¨Š
            if 'date_posted' in combined_df.columns:
                date_info = combined_df['date_posted'].dropna()
                test_results['recent_jobs_count'] = len(date_info)
                print(f"\nğŸ“… æ—¥æœŸåˆ†æ:")
                print(f"   æœ‰ç™¼å¸ƒæ—¥æœŸçš„è·ä½: {len(date_info)}")
                if len(date_info) > 0:
                    print(f"   æœ€æ–°è·ä½: {date_info.max()}")
                    print(f"   æœ€èˆŠè·ä½: {date_info.min()}")
            
            # å„²å­˜åˆä½µçµæœ
            combined_file = results_dir / "asia_ai_engineer_combined.csv"
            combined_df.to_csv(combined_file, index=False, encoding='utf-8-sig')
            
            # å„²å­˜åŸå§‹JSONè³‡æ–™
            json_file = results_dir / "asia_ai_engineer_raw_data.json"
            combined_df.to_json(json_file, orient='records', indent=2, force_ascii=False)
            
            # æ›´æ–°æ¸¬è©¦çµæœ
            test_results['total_jobs'] = final_count
            test_results['status'] = 'success'
            test_results['csv_file'] = str(combined_file)
            test_results['json_file'] = str(json_file)
            
            # ç”ŸæˆåŸºæœ¬çµ±è¨ˆ
            stats = {
                'unique_companies': combined_df['company'].nunique() if 'company' in combined_df.columns else 0,
                'unique_locations': combined_df['location'].nunique() if 'location' in combined_df.columns else 0,
                'has_salary_info': len(combined_df.dropna(subset=['min_amount'])) if 'min_amount' in combined_df.columns else 0,
                'remote_jobs': len(combined_df[combined_df['is_remote'] == True]) if 'is_remote' in combined_df.columns else 0
            }
            test_results['statistics'] = stats
            
            print(f"\nğŸ“ˆ çµ±è¨ˆè³‡è¨Š:")
            print(f"   ğŸ’¼ ç¸½è·ä½æ•¸: {final_count}")
            print(f"   ğŸ¢ å…¬å¸æ•¸: {stats['unique_companies']}")
            print(f"   ğŸ“ åœ°é»æ•¸: {stats['unique_locations']}")
            print(f"   ğŸ’° æœ‰è–ªè³‡è³‡è¨Š: {stats['has_salary_info']}")
            print(f"   ğŸ  é ç¨‹å·¥ä½œ: {stats['remote_jobs']}")
            
            # åˆ†æè·ä½æ¨™é¡Œé—œéµå­—
            if 'title' in combined_df.columns:
                ai_keywords = ['AI', 'Machine Learning', 'Deep Learning', 'Neural', 'Artificial Intelligence']
                keyword_counts = {}
                for keyword in ai_keywords:
                    count = combined_df['title'].str.contains(keyword, case=False, na=False).sum()
                    if count > 0:
                        keyword_counts[keyword] = count
                
                if keyword_counts:
                    test_results['keyword_analysis'] = keyword_counts
                    print(f"\nğŸ” é—œéµå­—åˆ†æ:")
                    for keyword, count in keyword_counts.items():
                        print(f"   {keyword}: {count} å€‹è·ä½")
            
        else:
            test_results['status'] = 'no_results'
            test_results['total_jobs'] = 0
            print("âŒ æ‰€æœ‰åœ°é»éƒ½æ²’æœ‰æ‰¾åˆ°è·ä½")
    
    except Exception as e:
        test_results['status'] = 'error'
        test_results['error'] = str(e)
        print(f"âŒ æ¸¬è©¦åŸ·è¡Œå‡ºéŒ¯: {str(e)}")
    
    finally:
        test_results['end_time'] = datetime.now()
        test_results['execution_time'] = (test_results['end_time'] - test_results['start_time']).total_seconds()
        
        # å„²å­˜æ¸¬è©¦çµæœ
        import json
        results_file = results_dir / "test_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            # è½‰æ›datetimeç‚ºå­—ä¸²ä»¥ä¾¿JSONåºåˆ—åŒ–
            results_copy = test_results.copy()
            results_copy['start_time'] = test_results['start_time'].isoformat()
            results_copy['end_time'] = test_results['end_time'].isoformat()
            
            # è½‰æ›numpy/pandasæ•¸æ“šé¡å‹ç‚ºPythonåŸç”Ÿé¡å‹
            def convert_types(obj):
                if hasattr(obj, 'item'):
                    return obj.item()
                elif isinstance(obj, dict):
                    return {k: convert_types(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_types(v) for v in obj]
                return obj
            
            results_copy = convert_types(results_copy)
            json.dump(results_copy, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“ æ¸¬è©¦çµæœå·²å„²å­˜è‡³: {results_dir}")
        print(f"â±ï¸  åŸ·è¡Œæ™‚é–“: {test_results['execution_time']:.2f} ç§’")
    
    return test_results


def generate_test_report(test_results: dict, results_dir: Path):
    """
    ç”Ÿæˆæ¸¬è©¦å ±å‘Š
    
    Args:
        test_results: æ¸¬è©¦çµæœå­—å…¸
        results_dir: çµæœç›®éŒ„è·¯å¾‘
    """
    report_content = f"""# äºæ´²AIå·¥ç¨‹å¸«è·ä½æœå°‹æ¸¬è©¦å ±å‘Š

## ğŸ“‹ æ¸¬è©¦æ¦‚è¦

- **æ¸¬è©¦åç¨±**: {test_results['test_name']}
- **ç”¨æˆ¶æç¤º**: "{test_results['user_prompt']}"
- **åŸ·è¡Œæ™‚é–“**: {test_results['start_time'].strftime('%Y-%m-%d %H:%M:%S')}
- **æ¸¬è©¦ç‹€æ…‹**: {test_results['status']}
- **åŸ·è¡Œæ™‚é•·**: {test_results['execution_time']:.2f} ç§’

## ğŸ¯ æ¸¬è©¦çµæœ

### ç¸½é«”çµ±è¨ˆ
- **ç¸½è·ä½æ•¸**: {test_results['total_jobs']}
- **æ¸¬è©¦åœ°é»æ•¸**: {len(test_results['locations_tested'])}
- **æœ‰æ—¥æœŸè³‡è¨Šçš„è·ä½**: {test_results.get('recent_jobs_count', 0)}

### åœ°é»åˆ†å¸ƒ
"""
    
    for location, count in test_results.get('jobs_by_location', {}).items():
        report_content += f"- **{location}**: {count} å€‹è·ä½\n"
    
    if test_results.get('jobs_by_site'):
        report_content += "\n### ç¶²ç«™åˆ†å¸ƒ\n"
        for site, count in test_results['jobs_by_site'].items():
            report_content += f"- **{site}**: {count} å€‹è·ä½\n"
    
    if test_results.get('statistics'):
        stats = test_results['statistics']
        report_content += f"\n### è©³ç´°çµ±è¨ˆ\n"
        report_content += f"- **å…¬å¸æ•¸é‡**: {stats['unique_companies']}\n"
        report_content += f"- **åœ°é»æ•¸é‡**: {stats['unique_locations']}\n"
        report_content += f"- **æœ‰è–ªè³‡è³‡è¨Š**: {stats['has_salary_info']}\n"
        report_content += f"- **é ç¨‹å·¥ä½œ**: {stats['remote_jobs']}\n"
    
    if test_results.get('keyword_analysis'):
        report_content += "\n### é—œéµå­—åˆ†æ\n"
        for keyword, count in test_results['keyword_analysis'].items():
            report_content += f"- **{keyword}**: {count} å€‹è·ä½\n"
    
    # æ¸¬è©¦çµè«–
    report_content += "\n## ğŸ¯ æ¸¬è©¦çµè«–\n\n"
    if test_results['status'] == 'success':
        if test_results['total_jobs'] >= 5:
            report_content += "âœ… **æ¸¬è©¦é€šé**: æˆåŠŸæ‰¾åˆ°äºæ´²åœ°å€çš„AIå·¥ç¨‹å¸«è·ä½\n"
        else:
            report_content += "âš ï¸ **éƒ¨åˆ†æˆåŠŸ**: æ‰¾åˆ°è·ä½ä½†æ•¸é‡è¼ƒå°‘ï¼Œå¯èƒ½å—åˆ°åœ°å€å’Œæ™‚é–“é™åˆ¶å½±éŸ¿\n"
        
        if test_results.get('recent_jobs_count', 0) > 0:
            report_content += "âœ… **æ—¥æœŸç¯©é¸æœ‰æ•ˆ**: æ‰¾åˆ°æœ‰ç™¼å¸ƒæ—¥æœŸè³‡è¨Šçš„è·ä½\n"
        else:
            report_content += "âš ï¸ **æ—¥æœŸè³‡è¨Šç¼ºå¤±**: å¤§éƒ¨åˆ†è·ä½ç¼ºå°‘ç™¼å¸ƒæ—¥æœŸè³‡è¨Š\n"
            
    elif test_results['status'] == 'no_results':
        report_content += "âŒ **æ¸¬è©¦å¤±æ•—**: æ²’æœ‰æ‰¾åˆ°ä»»ä½•è·ä½ï¼Œå¯èƒ½åŸå› :\n"
        report_content += "   - äºæ´²åœ°å€AIå·¥ç¨‹å¸«è·ä½è¼ƒå°‘\n"
        report_content += "   - 7æ—¥æ™‚é–“é™åˆ¶éæ–¼åš´æ ¼\n"
        report_content += "   - ç¶²ç«™å°äºæ´²åœ°å€æ”¯æ´æœ‰é™\n"
    else:
        report_content += f"âŒ **æ¸¬è©¦éŒ¯èª¤**: {test_results.get('error', 'æœªçŸ¥éŒ¯èª¤')}\n"
    
    report_content += "\n### å»ºè­°\n"
    report_content += "- å¦‚æœè·ä½æ•¸é‡è¼ƒå°‘ï¼Œå¯ä»¥è€ƒæ…®æ“´å¤§æœå°‹ç¯„åœåˆ°å…¶ä»–äºæ´²åŸå¸‚\n"
    report_content += "- å¯ä»¥å˜—è©¦ç§»é™¤æ™‚é–“é™åˆ¶ä»¥ç²å¾—æ›´å¤šçµæœ\n"
    report_content += "- è€ƒæ…®ä½¿ç”¨ç•¶åœ°çš„æ±‚è·ç¶²ç«™é€²è¡Œè£œå……æœå°‹\n"
    
    report_content += f"\n---\n\n*å ±å‘Šç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"
    
    # å„²å­˜å ±å‘Š
    report_file = results_dir / "test_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"ğŸ“„ æ¸¬è©¦å ±å‘Šå·²ç”Ÿæˆ: {report_file}")


if __name__ == "__main__":
    # åŸ·è¡Œæ¸¬è©¦
    results = test_asia_ai_engineer()
    
    # ç”Ÿæˆå ±å‘Š
    if 'csv_file' in results:
        results_dir = Path(results['csv_file']).parent
        generate_test_report(results, results_dir)
    
    # é¡¯ç¤ºæœ€çµ‚ç‹€æ…‹
    if results['status'] == 'success':
        print(f"\nğŸ‰ æ¸¬è©¦æˆåŠŸå®Œæˆ! æ‰¾åˆ° {results['total_jobs']} å€‹AIå·¥ç¨‹å¸«è·ä½")
        if results.get('recent_jobs_count', 0) > 0:
            print(f"ğŸ“… å…¶ä¸­ {results['recent_jobs_count']} å€‹è·ä½æœ‰ç™¼å¸ƒæ—¥æœŸè³‡è¨Š")
    else:
        print(f"\nâš ï¸ æ¸¬è©¦ç‹€æ…‹: {results['status']}")