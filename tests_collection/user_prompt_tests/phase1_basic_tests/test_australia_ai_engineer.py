#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¾³æ´²AIå·¥ç¨‹å¸«è·ä½æœå°‹æ¸¬è©¦

æ¸¬è©¦ç”¨æˆ¶æç¤º: "æœå°‹æ¾³æ´²Sydneyå’ŒMelbourneçš„AI Engineerå·¥ä½œ"

Author: JobSpy Team
Date: 2025-01-09
"""

import os
import sys
import pandas as pd
from datetime import datetime
from pathlib import Path

# æ·»åŠ jobseekeræ¨¡çµ„åˆ°è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jobseeker import scrape_jobs
from jobseeker.model import Site


def test_australia_ai_engineer():
    """
    åŸ·è¡Œæ¾³æ´²AIå·¥ç¨‹å¸«è·ä½æœå°‹æ¸¬è©¦
    
    æ¸¬è©¦åƒæ•¸:
    - è·ä½: AI Engineer
    - åœ°é»: Sydney, Melbourne
    - ç¶²ç«™: Indeed, LinkedIn
    - åœ‹å®¶: Australia
    
    Returns:
        dict: æ¸¬è©¦çµæœ
    """
    print("ğŸš€ é–‹å§‹åŸ·è¡Œæ¾³æ´²AIå·¥ç¨‹å¸«è·ä½æœå°‹æ¸¬è©¦")
    print("ğŸ“ ç”¨æˆ¶æç¤º: æœå°‹æ¾³æ´²Sydneyå’ŒMelbourneçš„AI Engineerå·¥ä½œ")
    
    # æ¸¬è©¦é…ç½®
    test_config = {
        'job_title': 'AI Engineer',
        'locations': ['Sydney, Australia', 'Melbourne, Australia'],
        'sites': [Site.INDEED, Site.LINKEDIN],
        'results_wanted': 50,
        'country': 'Australia'
    }
    
    # å‰µå»ºçµæœç›®éŒ„
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = Path(__file__).parent / f"australia_ai_engineer_{timestamp}"
    results_dir.mkdir(exist_ok=True)
    
    all_jobs = []
    test_results = {
        'test_name': 'australia_ai_engineer',
        'user_prompt': 'æœå°‹æ¾³æ´²Sydneyå’ŒMelbourneçš„AI Engineerå·¥ä½œ',
        'start_time': datetime.now(),
        'locations_tested': [],
        'total_jobs': 0,
        'jobs_by_location': {},
        'jobs_by_site': {},
        'status': 'running'
    }
    
    try:
        # ç‚ºæ¯å€‹åœ°é»åŸ·è¡Œæœå°‹
        for location in test_config['locations']:
            print(f"\nğŸ” æœå°‹åœ°é»: {location}")
            
            # åŸ·è¡ŒJobSpyæœå°‹
            jobs_df = scrape_jobs(
                site_name=test_config['sites'],
                search_term=test_config['job_title'],
                location=location,
                results_wanted=test_config['results_wanted'],
                country_indeed=test_config['country']
            )
            
            if jobs_df is not None and not jobs_df.empty:
                all_jobs.append(jobs_df)
                job_count = len(jobs_df)
                test_results['jobs_by_location'][location] = job_count
                test_results['locations_tested'].append(location)
                print(f"âœ… {location}: æ‰¾åˆ° {job_count} å€‹è·ä½")
                
                # å„²å­˜å–®ä¸€åœ°é»çµæœ
                location_file = results_dir / f"{location.replace(', ', '_').replace(' ', '_').lower()}_jobs.csv"
                jobs_df.to_csv(location_file, index=False, encoding='utf-8-sig')
                
            else:
                print(f"âš ï¸  {location}: æ²’æœ‰æ‰¾åˆ°è·ä½")
                test_results['jobs_by_location'][location] = 0
        
        # åˆä½µæ‰€æœ‰çµæœ
        if all_jobs:
            combined_df = pd.concat(all_jobs, ignore_index=True)
            
            # ç§»é™¤é‡è¤‡è·ä½ (åŸºæ–¼job_url)
            original_count = len(combined_df)
            combined_df = combined_df.drop_duplicates(subset=['job_url'], keep='first')
            final_count = len(combined_df)
            
            print(f"\nğŸ“Š åˆä½µçµæœ: {original_count} â†’ {final_count} (ç§»é™¤ {original_count - final_count} å€‹é‡è¤‡è·ä½)")
            
            # çµ±è¨ˆç¶²ç«™åˆ†å¸ƒ
            if 'site' in combined_df.columns:
                site_counts = combined_df['site'].value_counts()
                test_results['jobs_by_site'] = site_counts.to_dict()
                print("\nğŸŒ ç¶²ç«™åˆ†å¸ƒ:")
                for site, count in site_counts.items():
                    print(f"   {site}: {count} å€‹è·ä½")
            
            # å„²å­˜åˆä½µçµæœ
            combined_file = results_dir / "australia_ai_engineer_combined.csv"
            combined_df.to_csv(combined_file, index=False, encoding='utf-8-sig')
            
            # å„²å­˜åŸå§‹JSONè³‡æ–™
            json_file = results_dir / "australia_ai_engineer_raw_data.json"
            combined_df.to_json(json_file, orient='records', indent=2, force_ascii=False)
            
            # æ›´æ–°æ¸¬è©¦çµæœ
            test_results['total_jobs'] = final_count
            test_results['status'] = 'success'
            test_results['csv_file'] = str(combined_file)
            test_results['json_file'] = str(json_file)
            
            # ç”ŸæˆåŸºæœ¬çµ±è¨ˆ
            stats = {
                'unique_companies': combined_df['company'].nunique() if 'company' in combined_df.columns else 0,
                'unique_locations': combined_df['location'].nunique() if 'location' in combined_df.columns else 0,
                'has_salary_info': len(combined_df.dropna(subset=['min_amount'])) if 'min_amount' in combined_df.columns else 0
            }
            test_results['statistics'] = stats
            
            print(f"\nğŸ“ˆ çµ±è¨ˆè³‡è¨Š:")
            print(f"   ğŸ’¼ ç¸½è·ä½æ•¸: {final_count}")
            print(f"   ğŸ¢ å…¬å¸æ•¸: {stats['unique_companies']}")
            print(f"   ğŸ“ åœ°é»æ•¸: {stats['unique_locations']}")
            print(f"   ğŸ’° æœ‰è–ªè³‡è³‡è¨Š: {stats['has_salary_info']}")
            
        else:
            test_results['status'] = 'no_results'
            test_results['total_jobs'] = 0
            print("âŒ æ‰€æœ‰åœ°é»éƒ½æ²’æœ‰æ‰¾åˆ°è·ä½")
    
    except Exception as e:
        test_results['status'] = 'error'
        test_results['error'] = str(e)
        print(f"âŒ æ¸¬è©¦åŸ·è¡Œå‡ºéŒ¯: {str(e)}")
    
    finally:
        test_results['end_time'] = datetime.now()
        test_results['execution_time'] = (test_results['end_time'] - test_results['start_time']).total_seconds()
        
        # å„²å­˜æ¸¬è©¦çµæœ
        import json
        results_file = results_dir / "test_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            # è½‰æ›datetimeç‚ºå­—ä¸²ä»¥ä¾¿JSONåºåˆ—åŒ–
            results_copy = test_results.copy()
            results_copy['start_time'] = test_results['start_time'].isoformat()
            results_copy['end_time'] = test_results['end_time'].isoformat()
            
            # è½‰æ›numpy/pandasæ•¸æ“šé¡å‹ç‚ºPythonåŸç”Ÿé¡å‹
            def convert_types(obj):
                if hasattr(obj, 'item'):
                    return obj.item()
                elif isinstance(obj, dict):
                    return {k: convert_types(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_types(v) for v in obj]
                return obj
            
            results_copy = convert_types(results_copy)
            json.dump(results_copy, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“ æ¸¬è©¦çµæœå·²å„²å­˜è‡³: {results_dir}")
        print(f"â±ï¸  åŸ·è¡Œæ™‚é–“: {test_results['execution_time']:.2f} ç§’")
    
    return test_results


def generate_test_report(test_results: dict, results_dir: Path):
    """
    ç”Ÿæˆæ¸¬è©¦å ±å‘Š
    
    Args:
        test_results: æ¸¬è©¦çµæœå­—å…¸
        results_dir: çµæœç›®éŒ„è·¯å¾‘
    """
    report_content = f"""# æ¾³æ´²AIå·¥ç¨‹å¸«è·ä½æœå°‹æ¸¬è©¦å ±å‘Š

## ğŸ“‹ æ¸¬è©¦æ¦‚è¦

- **æ¸¬è©¦åç¨±**: {test_results['test_name']}
- **ç”¨æˆ¶æç¤º**: "{test_results['user_prompt']}"
- **åŸ·è¡Œæ™‚é–“**: {test_results['start_time'].strftime('%Y-%m-%d %H:%M:%S')}
- **æ¸¬è©¦ç‹€æ…‹**: {test_results['status']}
- **åŸ·è¡Œæ™‚é•·**: {test_results['execution_time']:.2f} ç§’

## ğŸ¯ æ¸¬è©¦çµæœ

### ç¸½é«”çµ±è¨ˆ
- **ç¸½è·ä½æ•¸**: {test_results['total_jobs']}
- **æ¸¬è©¦åœ°é»æ•¸**: {len(test_results['locations_tested'])}

### åœ°é»åˆ†å¸ƒ
"""
    
    for location, count in test_results.get('jobs_by_location', {}).items():
        report_content += f"- **{location}**: {count} å€‹è·ä½\n"
    
    if test_results.get('jobs_by_site'):
        report_content += "\n### ç¶²ç«™åˆ†å¸ƒ\n"
        for site, count in test_results['jobs_by_site'].items():
            report_content += f"- **{site}**: {count} å€‹è·ä½\n"
    
    if test_results.get('statistics'):
        stats = test_results['statistics']
        report_content += f"\n### è©³ç´°çµ±è¨ˆ\n"
        report_content += f"- **å…¬å¸æ•¸é‡**: {stats['unique_companies']}\n"
        report_content += f"- **åœ°é»æ•¸é‡**: {stats['unique_locations']}\n"
        report_content += f"- **æœ‰è–ªè³‡è³‡è¨Š**: {stats['has_salary_info']}\n"
    
    # æ¸¬è©¦çµè«–
    report_content += "\n## ğŸ¯ æ¸¬è©¦çµè«–\n\n"
    if test_results['status'] == 'success':
        if test_results['total_jobs'] >= 10:
            report_content += "âœ… **æ¸¬è©¦é€šé**: æˆåŠŸæ‰¾åˆ°è¶³å¤ æ•¸é‡çš„AIå·¥ç¨‹å¸«è·ä½\n"
        else:
            report_content += "âš ï¸ **éƒ¨åˆ†æˆåŠŸ**: æ‰¾åˆ°è·ä½ä½†æ•¸é‡è¼ƒå°‘\n"
    elif test_results['status'] == 'no_results':
        report_content += "âŒ **æ¸¬è©¦å¤±æ•—**: æ²’æœ‰æ‰¾åˆ°ä»»ä½•è·ä½\n"
    else:
        report_content += f"âŒ **æ¸¬è©¦éŒ¯èª¤**: {test_results.get('error', 'æœªçŸ¥éŒ¯èª¤')}\n"
    
    report_content += f"\n---\n\n*å ±å‘Šç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"
    
    # å„²å­˜å ±å‘Š
    report_file = results_dir / "test_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"ğŸ“„ æ¸¬è©¦å ±å‘Šå·²ç”Ÿæˆ: {report_file}")


if __name__ == "__main__":
    # åŸ·è¡Œæ¸¬è©¦
    results = test_australia_ai_engineer()
    
    # ç”Ÿæˆå ±å‘Š
    if 'csv_file' in results:
        results_dir = Path(results['csv_file']).parent
        generate_test_report(results, results_dir)
    
    # é¡¯ç¤ºæœ€çµ‚ç‹€æ…‹
    if results['status'] == 'success':
        print(f"\nğŸ‰ æ¸¬è©¦æˆåŠŸå®Œæˆ! æ‰¾åˆ° {results['total_jobs']} å€‹AIå·¥ç¨‹å¸«è·ä½")
    else:
        print(f"\nâš ï¸ æ¸¬è©¦ç‹€æ…‹: {results['status']}")