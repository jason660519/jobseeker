#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
jobseeker ç¶²é æ‡‰ç”¨
æä¾›ç°¡å–®æ˜“ç”¨çš„ç¶²é ç•Œé¢ï¼Œè®“ä¸€èˆ¬æ°‘çœ¾ç„¡éœ€ç¨‹å¼è¨­è¨ˆçŸ¥è­˜å³å¯ä½¿ç”¨è·ä½æœå°‹åŠŸèƒ½

åŠŸèƒ½:
1. ç¶²é ç•Œé¢ - ç°¡æ½”çš„æœå°‹è¡¨å–®
2. API ç«¯é» - RESTful API æ”¯æ´
3. å³æ™‚æœå°‹ - AJAX æ”¯æ´çš„å‹•æ…‹æœå°‹
4. çµæœå±•ç¤º - ç¾è§€çš„è·ä½åˆ—è¡¨å’Œè©³ç´°è³‡è¨Š
5. ä¸‹è¼‰åŠŸèƒ½ - CSV å’Œ JSON æ ¼å¼ä¸‹è¼‰

Author: jobseeker Team
Date: 2025-01-27
"""

import os
import sys
import json
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from flask import Flask, render_template, request, jsonify, send_file, flash, redirect, url_for, make_response
from flask_cors import CORS
import pandas as pd

# å°å…¥ jobseeker æ ¸å¿ƒåŠŸèƒ½
try:
    from jobseeker.route_manager import smart_scrape_jobs
    from jobseeker.model import Site, Country
    from jobseeker.query_parser import parse_user_query_smart
except ImportError as e:
    print(f"è­¦å‘Š: ç„¡æ³•å°å…¥ jobseeker æ¨¡çµ„: {e}")
    print("è«‹ç¢ºä¿å·²æ­£ç¢ºå®‰è£ jobseeker å¥—ä»¶")
    sys.exit(1)

# å‰µå»º Flask æ‡‰ç”¨
app = Flask(__name__)
app.secret_key = os.environ.get('SECRET_KEY', 'jobseeker-web-app-secret-key-2025')

# å•Ÿç”¨ CORS æ”¯æ´
CORS(app)

# é…ç½®
app.config.update(
    MAX_CONTENT_LENGTH=16 * 1024 * 1024,  # 16MB æœ€å¤§ä¸Šå‚³å¤§å°
    UPLOAD_FOLDER=project_root / 'web_app' / 'downloads',
    TEMPLATES_AUTO_RELOAD=True,
    DEBUG=os.environ.get('FLASK_DEBUG', 'False').lower() == 'true'
)

# ç¢ºä¿ä¸‹è¼‰ç›®éŒ„å­˜åœ¨
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

# å…¨åŸŸè®Šæ•¸å„²å­˜æœå°‹çµæœ
search_results_cache = {}


@app.route('/')
def index():
    """
    é¦–é  - é¡¯ç¤ºæœå°‹è¡¨å–®
    """
    return render_template('index.html')


@app.route('/search', methods=['POST'])
def search_jobs():
    """
    åŸ·è¡Œè·ä½æœå°‹ (æ”¯æ´åˆ†é )
    
    æ¥å— JSON æˆ–è¡¨å–®æ•¸æ“š
    è¿”å›æœå°‹çµæœçš„ JSON éŸ¿æ‡‰ï¼ŒåŒ…å«åˆ†é è³‡è¨Š
    """
    try:
        # ç²å–è«‹æ±‚æ•¸æ“š
        if request.is_json:
            data = request.get_json()
        else:
            data = request.form.to_dict()
        
        # æå–æœå°‹åƒæ•¸ï¼ˆå–®æ¬„è¼¸å…¥ï¼Œåœ°é»å¯ç”±è§£æå¾—å‡ºï¼‰
        user_query = data.get('query', '').strip()
        location = data.get('location', '').strip() if data.get('location') is not None else ''
        results_wanted = int(data.get('results_wanted', 20))
        hours_old = data.get('hours_old')
        selected_sites = data.get('selected_sites', '')
        
        # æ–°å¢åˆ†é åƒæ•¸
        page = int(data.get('page', 1))
        per_page = int(data.get('per_page', 20))
        
        # é©—è­‰åˆ†é åƒæ•¸
        if page < 1:
            page = 1
        if per_page < 1 or per_page > 100:
            per_page = 20
        
        # è§£æå–®æ¬„æŸ¥è©¢ï¼ˆLLM-ready è¦å‰‡å¼è§£æï¼‰
        parsed = parse_user_query_smart(user_query)

        # è‹¥æœªé¡¯å¼æä¾›åœ°é»ï¼Œä½¿ç”¨è§£æåˆ°çš„åœ°é»
        derived_location = location if location else (parsed.location or '')

        # è™•ç†é¸æ“‡çš„ç¶²ç«™
        site_name = None
        if selected_sites:
            # å¦‚æœæœ‰é¸æ“‡ç‰¹å®šç¶²ç«™ï¼Œå°‡é€—è™Ÿåˆ†éš”çš„å­—ç¬¦ä¸²è½‰æ›ç‚ºåˆ—è¡¨
            sites_list = [site.strip() for site in selected_sites.split(',') if site.strip()]
            if len(sites_list) == 1:
                # å¦‚æœåªé¸æ“‡äº†ä¸€å€‹ç¶²ç«™ï¼Œä½¿ç”¨è©²ç¶²ç«™
                site_name = sites_list[0]
            # å¦‚æœé¸æ“‡äº†å¤šå€‹ç¶²ç«™ï¼Œä¿æŒ site_name ç‚º Noneï¼ˆæœå°‹æ‰€æœ‰ç¶²ç«™ï¼‰
        elif parsed.site_hints and len(parsed.site_hints) == 1:
            # ä½¿ç”¨æŸ¥è©¢ä¸­çš„å–®ä¸€ç«™é»æç¤º
            site_name = parsed.site_hints[0]
        
        # é©—è­‰è¼¸å…¥
        if not user_query:
            return jsonify({
                'success': False,
                'error': 'è«‹è¼¸å…¥æœå°‹é—œéµå­—'
            }), 400
        
        if results_wanted < 1 or results_wanted > 100:
            return jsonify({
                'success': False,
                'error': 'æœå°‹çµæœæ•¸é‡å¿…é ˆåœ¨ 1-100 ä¹‹é–“'
            }), 400
        
        # è™•ç†æ™‚é–“é™åˆ¶
        if hours_old:
            try:
                hours_old = int(hours_old)
                if hours_old < 1:
                    hours_old = None
            except (ValueError, TypeError):
                hours_old = None
        
        # ç”Ÿæˆæœå°‹ ID
        search_id = str(uuid.uuid4())
        
        # åŸ·è¡Œæ™ºèƒ½æœå°‹
        print(f"é–‹å§‹æœå°‹: {parsed.search_term}")
        if site_name:
            print(f"æŒ‡å®šæœå°‹ç¶²ç«™: {site_name}")
        else:
            print(f"é¸æ“‡çš„ç¶²ç«™: {selected_sites if selected_sites else 'æ‰€æœ‰ç¶²ç«™'}")
            
        result = smart_scrape_jobs(
            user_query=parsed.search_term,
            location=derived_location if derived_location else None,
            results_wanted=results_wanted,
            hours_old=hours_old,
            site_name=site_name,
            is_remote=parsed.is_remote
        )
        
        # è™•ç†æœå°‹çµæœ
        if result.total_jobs == 0:
            return jsonify({
                'success': True,
                'search_id': search_id,
                'total_jobs': 0,
                'message': 'æœªæ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„è·ä½ï¼Œè«‹å˜—è©¦èª¿æ•´æœå°‹æ¢ä»¶',
                'jobs': [],
                'routing_info': {
                    'successful_agents': [agent.value for agent in result.successful_agents],
                    'confidence_score': result.routing_decision.confidence_score
                },
                'pagination': {
                    'page': page,
                    'per_page': per_page,
                    'total': 0,
                    'total_pages': 0,
                    'has_next': False,
                    'has_prev': False
                }
            })
        
        # è½‰æ›è·ä½æ•¸æ“šç‚ºå­—å…¸æ ¼å¼
        jobs_list = []
        if result.combined_jobs_data is not None:
            # è™•ç† NaN å€¼å’Œæ•¸æ“šé¡å‹
            df_clean = result.combined_jobs_data.fillna('')
            
            for _, row in df_clean.iterrows():
                job_dict = {
                    'title': str(row.get('title', '')),
                    'company': str(row.get('company', '')),
                    'location': str(row.get('location', '')),
                    'description': str(row.get('description', ''))[:500] + ('...' if len(str(row.get('description', ''))) > 500 else ''),
                    'salary_min': float(row['salary_min']) if pd.notna(row.get('salary_min')) else None,
                    'salary_max': float(row['salary_max']) if pd.notna(row.get('salary_max')) else None,
                    'salary_currency': str(row.get('salary_currency', '')),
                    'date_posted': str(row.get('date_posted', '')),
                    'job_url': str(row.get('job_url', '')),
                    'job_url_direct': str(row.get('job_url_direct', '')),
                    'site': str(row.get('site', '')),
                    'job_type': str(row.get('job_type', '')),
                    'is_remote': bool(row.get('is_remote', False))
                }
                jobs_list.append(job_dict)
        
        # å¿«å–æœå°‹çµæœ
        search_results_cache[search_id] = {
            'result': result,
            'timestamp': datetime.now(),
            'query': parsed.search_term,
            'location': derived_location
        }
        
        # ä¼ºæœå™¨ç«¯åˆ†é åˆ‡ç‰‡
        total = len(jobs_list)
        total_pages = (total + per_page - 1) // per_page if per_page else 1
        start = (page - 1) * per_page
        end = start + per_page
        page_jobs = jobs_list[start:end]

        # è¿”å›æˆåŠŸéŸ¿æ‡‰ï¼ˆå«åˆ†é è³‡è¨Šï¼‰
        return jsonify({
            'success': True,
            'search_id': search_id,
            'total_jobs': result.total_jobs,
            'jobs': page_jobs,
            'routing_info': {
                'successful_agents': [agent.value for agent in result.successful_agents],
                'failed_agents': [agent.value for agent in result.failed_agents],
                'confidence_score': result.routing_decision.confidence_score,
                'execution_time': result.total_execution_time
            },
            'search_params': {
                'query': user_query,
                'location': location,
                'results_wanted': results_wanted,
                'hours_old': hours_old
            },
            'pagination': {
                'page': page,
                'per_page': per_page,
                'total': total,
                'total_pages': total_pages,
                'has_next': page < total_pages,
                'has_prev': page > 1
            }
        })
        
    except Exception as e:
        print(f"æœå°‹éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        
        return jsonify({
            'success': False,
            'error': f'æœå°‹éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}'
        }), 500


@app.route('/download/<search_id>/<format>')
def download_results(search_id, format):
    """
    ä¸‹è¼‰æœå°‹çµæœ
    
    Args:
        search_id: æœå°‹ ID
        format: ä¸‹è¼‰æ ¼å¼ (csv, json)
    """
    try:
        # æª¢æŸ¥æœå°‹çµæœæ˜¯å¦å­˜åœ¨
        if search_id not in search_results_cache:
            flash('æœå°‹çµæœä¸å­˜åœ¨æˆ–å·²éæœŸ', 'error')
            return redirect(url_for('index'))
        
        cached_result = search_results_cache[search_id]
        result = cached_result['result']
        
        if result.combined_jobs_data is None or result.total_jobs == 0:
            flash('æ²’æœ‰å¯ä¸‹è¼‰çš„æœå°‹çµæœ', 'warning')
            return redirect(url_for('index'))
        
        # ç”Ÿæˆæª”æ¡ˆåç¨±
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        query_safe = ''.join(c for c in cached_result['query'][:20] if c.isalnum() or c in (' ', '-', '_')).strip()
        
        if format.lower() == 'csv':
            filename = f"jobseeker_{query_safe}_{timestamp}.csv"
            filepath = app.config['UPLOAD_FOLDER'] / filename
            
            # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
            standardized_data = convert_to_standard_format(result.combined_jobs_data)
            
            # å„²å­˜ CSV æª”æ¡ˆ
            standardized_data.to_csv(filepath, index=False, encoding='utf-8-sig')
            
            return send_file(
                filepath,
                as_attachment=True,
                download_name=filename,
                mimetype='text/csv'
            )
            
        elif format.lower() == 'json':
            filename = f"jobseeker_{query_safe}_{timestamp}.json"
            filepath = app.config['UPLOAD_FOLDER'] / filename
            
            # è½‰æ›ç‚º JSON æ ¼å¼
            jobs_data = result.combined_jobs_data.fillna('').to_dict('records')
            
            export_data = {
                'search_info': {
                    'query': cached_result['query'],
                    'location': cached_result['location'],
                    'timestamp': cached_result['timestamp'].isoformat(),
                    'total_jobs': result.total_jobs
                },
                'routing_info': {
                    'successful_agents': [agent.value for agent in result.successful_agents],
                    'failed_agents': [agent.value for agent in result.failed_agents],
                    'confidence_score': result.routing_decision.confidence_score,
                    'execution_time': result.total_execution_time
                },
                'jobs': jobs_data
            }
            
            # å„²å­˜ JSON æª”æ¡ˆ
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2, default=str)
            
            return send_file(
                filepath,
                as_attachment=True,
                download_name=filename,
                mimetype='application/json'
            )
        
        else:
            flash('ä¸æ”¯æ´çš„ä¸‹è¼‰æ ¼å¼', 'error')
            return redirect(url_for('index'))
            
    except Exception as e:
        print(f"ä¸‹è¼‰éŒ¯èª¤: {e}")
        flash(f'ä¸‹è¼‰å¤±æ•—: {str(e)}', 'error')
        return redirect(url_for('index'))


@app.route('/demo')
def demo_results():
    """
    å±•ç¤ºæ¸¬è©¦çµæœé é¢
    """
    # ç¦æ­¢å¿«å–ï¼Œç¢ºä¿é¡¯ç¤ºæœ€æ–°è³‡æ–™
    resp = make_response(render_template('demo_results.html'))
    resp.headers['Cache-Control'] = 'no-store, no-cache, must-revalidate, max-age=0'
    resp.headers['Pragma'] = 'no-cache'
    return resp


@app.route('/api/sites')
def get_supported_sites():
    """
    ç²å–æ”¯æ´çš„æ±‚è·ç¶²ç«™åˆ—è¡¨
    """
    try:
        sites = [{
            'value': site.value,
            'name': site.value.title(),
            'description': get_site_description(site)
        } for site in Site]
        
        return jsonify({
            'success': True,
            'sites': sites
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/search/<search_id>')
def get_search_result(search_id: str):
    """
    å–å¾—æŒ‡å®šæœå°‹ ID çš„çµæœï¼ˆç”¨æ–¼ Demo/çµæœé å‹•æ…‹é¡¯ç¤ºï¼‰
    """
    try:
        if search_id not in search_results_cache:
            return jsonify({'success': False, 'error': 'æœå°‹çµæœä¸å­˜åœ¨æˆ–å·²éæœŸ'}), 404

        cached = search_results_cache[search_id]
        result = cached['result']

        # å¯é¸åˆ†é åƒæ•¸
        try:
            page = int(request.args.get('page', 0))
        except Exception:
            page = 0
        try:
            per_page = int(request.args.get('per_page', 0))
        except Exception:
            per_page = 0

        jobs_list = []
        if result.combined_jobs_data is not None:
            df_clean = result.combined_jobs_data.fillna('')
            for _, row in df_clean.iterrows():
                jobs_list.append({
                    'title': str(row.get('title', '')),
                    'company': str(row.get('company', '')),
                    'location': str(row.get('location', '')),
                    'description': str(row.get('description', ''))[:500] + ('...' if len(str(row.get('description', ''))) > 500 else ''),
                    'salary_min': float(row['salary_min']) if pd.notna(row.get('salary_min')) else None,
                    'salary_max': float(row['salary_max']) if pd.notna(row.get('salary_max')) else None,
                    'salary_currency': str(row.get('salary_currency', '')),
                    'date_posted': str(row.get('date_posted', '')),
                    'job_url': str(row.get('job_url', '')),
                    'job_url_direct': str(row.get('job_url_direct', '')),
                    'site': str(row.get('site', '')),
                    'job_type': str(row.get('job_type', '')),
                    'is_remote': bool(row.get('is_remote', False))
                })

        # åˆ†é åˆ‡ç‰‡ï¼ˆè‹¥æä¾› page/per_pageï¼‰
        total = len(jobs_list)
        if page and per_page:
            if page < 1:
                page = 1
            if per_page < 1:
                per_page = 10
            total_pages = (total + per_page - 1) // per_page
            start = (page - 1) * per_page
            end = start + per_page
            page_jobs = jobs_list[start:end]
        else:
            total_pages = 1
            page_jobs = jobs_list
            page = 1
            per_page = total or 1

        return jsonify({
            'success': True,
            'search_id': search_id,
            'timestamp': cached['timestamp'].isoformat(),
            'query': cached.get('query'),
            'location': cached.get('location'),
            'total_jobs': result.total_jobs,
            'routing_info': {
                'successful_agents': [agent.value for agent in result.successful_agents],
                'failed_agents': [agent.value for agent in result.failed_agents],
                'confidence_score': result.routing_decision.confidence_score,
                'execution_time': result.total_execution_time
            },
            'jobs': page_jobs,
            'pagination': {
                'page': page,
                'per_page': per_page,
                'total': total,
                'total_pages': total_pages,
                'has_next': page < total_pages,
                'has_prev': page > 1
            }
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/search/latest')
def get_latest_search_result():
    """
    å–å¾—æœ€è¿‘ä¸€æ¬¡æœå°‹çµæœæ‘˜è¦
    """
    try:
        if not search_results_cache:
            return jsonify({'success': False, 'error': 'å°šç„¡æœå°‹è¨˜éŒ„'}), 404

        # æ‰¾å‡ºæœ€æ–°çš„æœå°‹
        latest_id = max(search_results_cache.items(), key=lambda kv: kv[1]['timestamp'])[0]
        return get_search_result(latest_id)
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/countries')
def get_supported_countries():
    """
    ç²å–æ”¯æ´çš„åœ‹å®¶åˆ—è¡¨
    """
    try:
        countries = [{
            'value': country.value[0],  # åœ‹å®¶åç¨±
            'code': country.value[1],   # åœ‹å®¶ä»£ç¢¼
            'name': country.name
        } for country in Country]
        
        return jsonify({
            'success': True,
            'countries': countries
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/health')
def health_check():
    """
    å¥åº·æª¢æŸ¥ç«¯é»
    """
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'version': '1.0.0'
    })


def convert_to_standard_format(df: pd.DataFrame) -> pd.DataFrame:
    """
    è½‰æ›è·ä½æ•¸æ“šç‚ºæ¨™æº–æ ¼å¼
    æŒ‰ç…§ç”¨æˆ¶æŒ‡å®šçš„æ ¼å¼ï¼šSITE, TITLE, COMPANY, CITY, STATE, JOB_TYPE, INTERVAL, MIN_AMOUNT, MAX_AMOUNT, JOB_URL, DESCRIPTION
    
    Args:
        df: åŸå§‹è·ä½æ•¸æ“š DataFrame
        
    Returns:
        æ¨™æº–åŒ–çš„ DataFrame
    """
    # å‰µå»ºæ¨™æº–åŒ–çš„ DataFrame
    standardized_df = pd.DataFrame()
    
    # æŒ‰ç…§æ¨™æº–æ ¼å¼æ˜ å°„æ¬„ä½
    standardized_df['SITE'] = df.get('site', '').fillna('')
    standardized_df['TITLE'] = df.get('title', '').fillna('')
    standardized_df['COMPANY'] = df.get('company', '').fillna('')
    
    # è™•ç†åœ°é»è³‡è¨Š - åˆ†é›¢åŸå¸‚å’Œå·/çœ
    location = df.get('location', '').fillna('')
    standardized_df['CITY'] = location.apply(extract_city)
    standardized_df['STATE'] = location.apply(extract_state)
    
    standardized_df['JOB_TYPE'] = df.get('job_type', '').fillna('')
    standardized_df['INTERVAL'] = df.get('interval', '').fillna('')
    standardized_df['MIN_AMOUNT'] = df.get('min_amount', '').fillna('')
    standardized_df['MAX_AMOUNT'] = df.get('max_amount', '').fillna('')
    standardized_df['JOB_URL'] = df.get('job_url', '').fillna('')
    standardized_df['DESCRIPTION'] = df.get('description', '').fillna('')
    
    return standardized_df


def extract_city(location_str):
    """
    å¾åœ°é»å­—ä¸²ä¸­æå–åŸå¸‚åç¨±
    
    Args:
        location_str: åœ°é»å­—ä¸²
        
    Returns:
        åŸå¸‚åç¨±
    """
    if pd.isna(location_str) or location_str == '':
        return ''
    
    # åˆ†å‰²åœ°é»å­—ä¸²ï¼Œé€šå¸¸æ ¼å¼ç‚º "City, State" æˆ– "City"
    parts = str(location_str).split(',')
    return parts[0].strip() if parts else ''


def extract_state(location_str):
    """
    å¾åœ°é»å­—ä¸²ä¸­æå–å·/çœåç¨±
    
    Args:
        location_str: åœ°é»å­—ä¸²
        
    Returns:
        å·/çœåç¨±
    """
    if pd.isna(location_str) or location_str == '':
        return ''
    
    # åˆ†å‰²åœ°é»å­—ä¸²ï¼Œé€šå¸¸æ ¼å¼ç‚º "City, State" æˆ– "City"
    parts = str(location_str).split(',')
    return parts[1].strip() if len(parts) > 1 else ''


def get_site_description(site: Site) -> str:
    """
    ç²å–æ±‚è·ç¶²ç«™æè¿°
    
    Args:
        site: Site æšèˆ‰å€¼
        
    Returns:
        ç¶²ç«™æè¿°å­—ä¸²
    """
    descriptions = {
        'indeed': 'å…¨çƒæœ€å¤§çš„æ±‚è·ç¶²ç«™ï¼Œæ¶µè“‹å„è¡Œå„æ¥­',
        'linkedin': 'å°ˆæ¥­äººè„ˆç¶²ç«™ï¼Œé©åˆç™½é ˜å’Œç®¡ç†è·ä½',
        'glassdoor': 'æä¾›å…¬å¸è©•åƒ¹å’Œè–ªè³‡è³‡è¨Š',
        'seek': 'æ¾³æ´²å’Œç´è¥¿è˜­æœ€å¤§çš„æ±‚è·ç¶²ç«™',
        'zip_recruiter': 'ç¾åœ‹çŸ¥åæ±‚è·ç¶²ç«™ï¼ŒAI æ™ºèƒ½åŒ¹é…',
        'google': 'Google è·ä½æœå°‹ï¼Œæ•´åˆå¤šå€‹å¹³å°',
        'naukri': 'å°åº¦æœ€å¤§çš„æ±‚è·ç¶²ç«™',
        'bayt': 'ä¸­æ±åœ°å€é ˜å…ˆçš„æ±‚è·å¹³å°',
        'bdjobs': 'å­ŸåŠ æ‹‰åœ‹æœ¬åœ°æ±‚è·ç¶²ç«™'
    }
    
    return descriptions.get(site.value, f'{site.value.title()} æ±‚è·ç¶²ç«™')


if __name__ == '__main__':
    print("ğŸš€ jobseeker ç¶²é æ‡‰ç”¨å•Ÿå‹•ä¸­...")
    print("ğŸ“± è¨ªå• http://localhost:5000 é–‹å§‹ä½¿ç”¨")
    print("ğŸ“Š API æ–‡æª”: http://localhost:5000/api/sites")
    
    app.run(
        host='0.0.0.0',
        port=int(os.environ.get('PORT', 5000)),
        debug=app.config['DEBUG']
    )
