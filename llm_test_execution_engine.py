#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMæ¸¬è©¦æ¡ˆä¾‹åŸ·è¡Œå¼•æ“
è‡ªå‹•åŸ·è¡Œæ¸¬è©¦æ¡ˆä¾‹ï¼Œæ”¶é›†çµæœä¸¦é€²è¡Œåˆ†æ

Author: JobSpy Team
Date: 2025-01-27
"""

import sys
import os
import json
import time
import asyncio
import threading
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Callable
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from collections import defaultdict, Counter
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed
import queue

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from jobseeker.llm_intent_analyzer import LLMIntentAnalyzer, LLMProvider
from llm_config import LLMConfig


class ExecutionStatus(Enum):
    """åŸ·è¡Œç‹€æ…‹"""
    PENDING = "pending"  # å¾…åŸ·è¡Œ
    RUNNING = "running"  # åŸ·è¡Œä¸­
    COMPLETED = "completed"  # å·²å®Œæˆ
    FAILED = "failed"  # åŸ·è¡Œå¤±æ•—
    TIMEOUT = "timeout"  # è¶…æ™‚
    CANCELLED = "cancelled"  # å·²å–æ¶ˆ


class ExecutionMode(Enum):
    """åŸ·è¡Œæ¨¡å¼"""
    SEQUENTIAL = "sequential"  # é †åºåŸ·è¡Œ
    PARALLEL = "parallel"  # ä¸¦è¡ŒåŸ·è¡Œ
    BATCH = "batch"  # æ‰¹æ¬¡åŸ·è¡Œ
    STREAMING = "streaming"  # æµå¼åŸ·è¡Œ


@dataclass
class TestExecution:
    """æ¸¬è©¦åŸ·è¡Œè¨˜éŒ„"""
    test_case_id: str
    provider: LLMProvider
    status: ExecutionStatus
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    duration: Optional[float] = None
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    retry_count: int = 0
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ExecutionBatch:
    """åŸ·è¡Œæ‰¹æ¬¡"""
    batch_id: str
    test_cases: List[Dict[str, Any]]
    providers: List[LLMProvider]
    executions: List[TestExecution] = field(default_factory=list)
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    total_duration: Optional[float] = None
    success_count: int = 0
    failure_count: int = 0
    timeout_count: int = 0


@dataclass
class ExecutionConfig:
    """åŸ·è¡Œé…ç½®"""
    mode: ExecutionMode = ExecutionMode.PARALLEL
    max_workers: int = 5
    timeout_seconds: int = 30
    retry_attempts: int = 3
    retry_delay: float = 1.0
    batch_size: int = 10
    rate_limit_delay: float = 0.1
    enable_caching: bool = True
    save_intermediate_results: bool = True
    progress_callback: Optional[Callable] = None


class ExecutionMonitor:
    """åŸ·è¡Œç›£æ§å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç›£æ§å™¨"""
        self.start_time = None
        self.total_tests = 0
        self.completed_tests = 0
        self.failed_tests = 0
        self.current_batch = None
        self.progress_queue = queue.Queue()
        self.is_monitoring = False
        
    def start_monitoring(self, total_tests: int) -> None:
        """é–‹å§‹ç›£æ§"""
        self.start_time = datetime.now()
        self.total_tests = total_tests
        self.completed_tests = 0
        self.failed_tests = 0
        self.is_monitoring = True
        
    def update_progress(self, completed: int, failed: int = 0) -> None:
        """æ›´æ–°é€²åº¦"""
        self.completed_tests = completed
        self.failed_tests = failed
        
        if self.is_monitoring:
            progress = (completed + failed) / self.total_tests if self.total_tests > 0 else 0
            elapsed = (datetime.now() - self.start_time).total_seconds() if self.start_time else 0
            eta = (elapsed / progress * (1 - progress)) if progress > 0 else 0
            
            self.progress_queue.put({
                "progress": progress,
                "completed": completed,
                "failed": failed,
                "total": self.total_tests,
                "elapsed": elapsed,
                "eta": eta
            })
    
    def stop_monitoring(self) -> None:
        """åœæ­¢ç›£æ§"""
        self.is_monitoring = False
    
    def get_progress(self) -> Dict[str, Any]:
        """ç²å–é€²åº¦ä¿¡æ¯"""
        try:
            return self.progress_queue.get_nowait()
        except queue.Empty:
            return {}


class LLMTestExecutionEngine:
    """LLMæ¸¬è©¦åŸ·è¡Œå¼•æ“"""
    
    def __init__(self, config: ExecutionConfig = None):
        """åˆå§‹åŒ–åŸ·è¡Œå¼•æ“"""
        self.config = config or ExecutionConfig()
        self.monitor = ExecutionMonitor()
        self.analyzers = {}
        self.execution_cache = {}
        self.results_storage = []
        self.is_cancelled = False
        
    def _get_analyzer(self, provider: LLMProvider) -> LLMIntentAnalyzer:
        """ç²å–åˆ†æå™¨å¯¦ä¾‹"""
        if provider not in self.analyzers:
            # å‰µå»ºLLMé…ç½®
            llm_config = LLMConfig(
                provider=provider,
                api_key=os.getenv(f"{provider.value.upper()}_API_KEY", "test_key"),
                model_name=self._get_default_model(provider),
                temperature=0.1,
                max_tokens=1000
            )
            
            self.analyzers[provider] = LLMIntentAnalyzer(llm_config)
        
        return self.analyzers[provider]
    
    def _get_default_model(self, provider: LLMProvider) -> str:
        """ç²å–é»˜èªæ¨¡å‹"""
        default_models = {
            LLMProvider.OPENAI: "gpt-3.5-turbo",
            LLMProvider.ANTHROPIC: "claude-3-sonnet-20240229",
            LLMProvider.GOOGLE: "gemini-pro",
            LLMProvider.AZURE_OPENAI: "gpt-35-turbo",
            LLMProvider.HUGGINGFACE: "microsoft/DialoGPT-medium",
            LLMProvider.COHERE: "command",
            LLMProvider.OLLAMA: "llama2"
        }
        return default_models.get(provider, "gpt-3.5-turbo")
    
    def execute_test_cases(self, test_cases: List[Dict[str, Any]], 
                          providers: List[LLMProvider]) -> List[ExecutionBatch]:
        """åŸ·è¡Œæ¸¬è©¦æ¡ˆä¾‹"""
        print(f"ğŸš€ é–‹å§‹åŸ·è¡Œ {len(test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹ï¼Œä½¿ç”¨ {len(providers)} å€‹æä¾›å•†...")
        
        total_executions = len(test_cases) * len(providers)
        self.monitor.start_monitoring(total_executions)
        
        batches = []
        
        try:
            if self.config.mode == ExecutionMode.SEQUENTIAL:
                batches = self._execute_sequential(test_cases, providers)
            elif self.config.mode == ExecutionMode.PARALLEL:
                batches = self._execute_parallel(test_cases, providers)
            elif self.config.mode == ExecutionMode.BATCH:
                batches = self._execute_batch(test_cases, providers)
            elif self.config.mode == ExecutionMode.STREAMING:
                batches = self._execute_streaming(test_cases, providers)
            
        except KeyboardInterrupt:
            print("\nâŒ åŸ·è¡Œè¢«ç”¨æˆ¶ä¸­æ–·")
            self.is_cancelled = True
        except Exception as e:
            print(f"\nâŒ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        finally:
            self.monitor.stop_monitoring()
        
        print(f"\nâœ… åŸ·è¡Œå®Œæˆï¼Œå…±è™•ç† {len(batches)} å€‹æ‰¹æ¬¡")
        return batches
    
    def _execute_sequential(self, test_cases: List[Dict[str, Any]], 
                          providers: List[LLMProvider]) -> List[ExecutionBatch]:
        """é †åºåŸ·è¡Œ"""
        print("   ğŸ“‹ ä½¿ç”¨é †åºåŸ·è¡Œæ¨¡å¼")
        
        batch = ExecutionBatch(
            batch_id=f"sequential_{int(time.time())}",
            test_cases=test_cases,
            providers=providers,
            start_time=datetime.now()
        )
        
        completed = 0
        failed = 0
        
        for test_case in test_cases:
            if self.is_cancelled:
                break
                
            for provider in providers:
                if self.is_cancelled:
                    break
                    
                execution = self._execute_single_test(test_case, provider)
                batch.executions.append(execution)
                
                if execution.status == ExecutionStatus.COMPLETED:
                    completed += 1
                    batch.success_count += 1
                else:
                    failed += 1
                    if execution.status == ExecutionStatus.FAILED:
                        batch.failure_count += 1
                    elif execution.status == ExecutionStatus.TIMEOUT:
                        batch.timeout_count += 1
                
                self.monitor.update_progress(completed, failed)
                
                # é€Ÿç‡é™åˆ¶
                if self.config.rate_limit_delay > 0:
                    time.sleep(self.config.rate_limit_delay)
        
        batch.end_time = datetime.now()
        batch.total_duration = (batch.end_time - batch.start_time).total_seconds()
        
        return [batch]
    
    def _execute_parallel(self, test_cases: List[Dict[str, Any]], 
                         providers: List[LLMProvider]) -> List[ExecutionBatch]:
        """ä¸¦è¡ŒåŸ·è¡Œ"""
        print(f"   ğŸ”„ ä½¿ç”¨ä¸¦è¡ŒåŸ·è¡Œæ¨¡å¼ï¼Œæœ€å¤§å·¥ä½œç·šç¨‹: {self.config.max_workers}")
        
        batch = ExecutionBatch(
            batch_id=f"parallel_{int(time.time())}",
            test_cases=test_cases,
            providers=providers,
            start_time=datetime.now()
        )
        
        # å‰µå»ºåŸ·è¡Œä»»å‹™
        tasks = []
        for test_case in test_cases:
            for provider in providers:
                tasks.append((test_case, provider))
        
        completed = 0
        failed = 0
        
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_task = {
                executor.submit(self._execute_single_test, test_case, provider): (test_case, provider)
                for test_case, provider in tasks
            }
            
            # æ”¶é›†çµæœ
            for future in as_completed(future_to_task):
                if self.is_cancelled:
                    break
                    
                try:
                    execution = future.result()
                    batch.executions.append(execution)
                    
                    if execution.status == ExecutionStatus.COMPLETED:
                        completed += 1
                        batch.success_count += 1
                    else:
                        failed += 1
                        if execution.status == ExecutionStatus.FAILED:
                            batch.failure_count += 1
                        elif execution.status == ExecutionStatus.TIMEOUT:
                            batch.timeout_count += 1
                    
                    self.monitor.update_progress(completed, failed)
                    
                except Exception as e:
                    print(f"   âš ï¸ ä»»å‹™åŸ·è¡Œç•°å¸¸: {str(e)}")
                    failed += 1
                    batch.failure_count += 1
                    self.monitor.update_progress(completed, failed)
        
        batch.end_time = datetime.now()
        batch.total_duration = (batch.end_time - batch.start_time).total_seconds()
        
        return [batch]
    
    def _execute_batch(self, test_cases: List[Dict[str, Any]], 
                      providers: List[LLMProvider]) -> List[ExecutionBatch]:
        """æ‰¹æ¬¡åŸ·è¡Œ"""
        print(f"   ğŸ“¦ ä½¿ç”¨æ‰¹æ¬¡åŸ·è¡Œæ¨¡å¼ï¼Œæ‰¹æ¬¡å¤§å°: {self.config.batch_size}")
        
        batches = []
        
        # åˆ†å‰²æ¸¬è©¦æ¡ˆä¾‹
        for i in range(0, len(test_cases), self.config.batch_size):
            if self.is_cancelled:
                break
                
            batch_cases = test_cases[i:i + self.config.batch_size]
            
            batch = ExecutionBatch(
                batch_id=f"batch_{i//self.config.batch_size + 1}_{int(time.time())}",
                test_cases=batch_cases,
                providers=providers,
                start_time=datetime.now()
            )
            
            print(f"   ğŸ”„ åŸ·è¡Œæ‰¹æ¬¡ {len(batches) + 1}: {len(batch_cases)} å€‹æ¡ˆä¾‹")
            
            # ä¸¦è¡ŒåŸ·è¡Œç•¶å‰æ‰¹æ¬¡
            parallel_batches = self._execute_parallel(batch_cases, providers)
            if parallel_batches:
                batch.executions = parallel_batches[0].executions
                batch.success_count = parallel_batches[0].success_count
                batch.failure_count = parallel_batches[0].failure_count
                batch.timeout_count = parallel_batches[0].timeout_count
            
            batch.end_time = datetime.now()
            batch.total_duration = (batch.end_time - batch.start_time).total_seconds()
            
            batches.append(batch)
            
            # æ‰¹æ¬¡é–“å»¶é²
            if i + self.config.batch_size < len(test_cases):
                time.sleep(self.config.rate_limit_delay * 10)  # æ‰¹æ¬¡é–“è¼ƒé•·å»¶é²
        
        return batches
    
    def _execute_streaming(self, test_cases: List[Dict[str, Any]], 
                          providers: List[LLMProvider]) -> List[ExecutionBatch]:
        """æµå¼åŸ·è¡Œ"""
        print("   ğŸŒŠ ä½¿ç”¨æµå¼åŸ·è¡Œæ¨¡å¼")
        
        batch = ExecutionBatch(
            batch_id=f"streaming_{int(time.time())}",
            test_cases=test_cases,
            providers=providers,
            start_time=datetime.now()
        )
        
        completed = 0
        failed = 0
        
        # å‰µå»ºåŸ·è¡ŒéšŠåˆ—
        execution_queue = queue.Queue()
        
        # å¡«å……éšŠåˆ—
        for test_case in test_cases:
            for provider in providers:
                execution_queue.put((test_case, provider))
        
        def worker():
            """å·¥ä½œç·šç¨‹"""
            nonlocal completed, failed
            
            while not execution_queue.empty() and not self.is_cancelled:
                try:
                    test_case, provider = execution_queue.get(timeout=1)
                    execution = self._execute_single_test(test_case, provider)
                    batch.executions.append(execution)
                    
                    if execution.status == ExecutionStatus.COMPLETED:
                        completed += 1
                        batch.success_count += 1
                    else:
                        failed += 1
                        if execution.status == ExecutionStatus.FAILED:
                            batch.failure_count += 1
                        elif execution.status == ExecutionStatus.TIMEOUT:
                            batch.timeout_count += 1
                    
                    self.monitor.update_progress(completed, failed)
                    execution_queue.task_done()
                    
                except queue.Empty:
                    break
                except Exception as e:
                    print(f"   âš ï¸ å·¥ä½œç·šç¨‹ç•°å¸¸: {str(e)}")
                    failed += 1
                    batch.failure_count += 1
                    self.monitor.update_progress(completed, failed)
        
        # å•Ÿå‹•å·¥ä½œç·šç¨‹
        threads = []
        for _ in range(min(self.config.max_workers, execution_queue.qsize())):
            thread = threading.Thread(target=worker)
            thread.start()
            threads.append(thread)
        
        # ç­‰å¾…æ‰€æœ‰ç·šç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        batch.end_time = datetime.now()
        batch.total_duration = (batch.end_time - batch.start_time).total_seconds()
        
        return [batch]
    
    def _execute_single_test(self, test_case: Dict[str, Any], provider: LLMProvider) -> TestExecution:
        """åŸ·è¡Œå–®å€‹æ¸¬è©¦"""
        execution = TestExecution(
            test_case_id=test_case.get("id", "unknown"),
            provider=provider,
            status=ExecutionStatus.PENDING,
            start_time=datetime.now()
        )
        
        # æª¢æŸ¥ç·©å­˜
        cache_key = f"{test_case.get('id')}_{provider.value}"
        if self.config.enable_caching and cache_key in self.execution_cache:
            cached_result = self.execution_cache[cache_key]
            execution.result = cached_result
            execution.status = ExecutionStatus.COMPLETED
            execution.end_time = datetime.now()
            execution.duration = 0.001  # ç·©å­˜å‘½ä¸­ï¼Œå¹¾ä¹ç„¡å»¶é²
            execution.metadata["from_cache"] = True
            return execution
        
        execution.status = ExecutionStatus.RUNNING
        
        for attempt in range(self.config.retry_attempts):
            try:
                # ç²å–åˆ†æå™¨
                analyzer = self._get_analyzer(provider)
                
                # åŸ·è¡Œåˆ†æ
                query = test_case.get("query", "")
                
                start_time = time.time()
                
                # è¨­ç½®è¶…æ™‚
                result = self._analyze_with_timeout(analyzer, query, self.config.timeout_seconds)
                
                end_time = time.time()
                
                # è¨˜éŒ„çµæœ
                execution.result = {
                    "intent": result.get("intent", "unknown"),
                    "confidence": result.get("confidence", 0.0),
                    "entities": result.get("entities", {}),
                    "response_time": end_time - start_time,
                    "provider": provider.value,
                    "model": self._get_default_model(provider)
                }
                
                execution.status = ExecutionStatus.COMPLETED
                execution.duration = end_time - start_time
                
                # ç·©å­˜çµæœ
                if self.config.enable_caching:
                    self.execution_cache[cache_key] = execution.result
                
                break
                
            except TimeoutError:
                execution.status = ExecutionStatus.TIMEOUT
                execution.error = "åŸ·è¡Œè¶…æ™‚"
                execution.retry_count = attempt + 1
                
                if attempt < self.config.retry_attempts - 1:
                    time.sleep(self.config.retry_delay * (attempt + 1))  # æŒ‡æ•¸é€€é¿
                
            except Exception as e:
                execution.status = ExecutionStatus.FAILED
                execution.error = str(e)
                execution.retry_count = attempt + 1
                
                if attempt < self.config.retry_attempts - 1:
                    time.sleep(self.config.retry_delay * (attempt + 1))
        
        execution.end_time = datetime.now()
        if execution.duration is None:
            execution.duration = (execution.end_time - execution.start_time).total_seconds()
        
        execution.metadata.update({
            "retry_count": execution.retry_count,
            "test_case_category": test_case.get("category", ""),
            "test_case_complexity": test_case.get("complexity", ""),
            "expected_intent": test_case.get("expected_intent", "")
        })
        
        return execution
    
    def _analyze_with_timeout(self, analyzer: LLMIntentAnalyzer, query: str, timeout: int) -> Dict[str, Any]:
        """å¸¶è¶…æ™‚çš„åˆ†æ"""
        import signal
        
        def timeout_handler(signum, frame):
            raise TimeoutError("åˆ†æè¶…æ™‚")
        
        # è¨­ç½®è¶…æ™‚ä¿¡è™Ÿ (åƒ…åœ¨Unixç³»çµ±ä¸Šå¯ç”¨)
        if hasattr(signal, 'SIGALRM'):
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(timeout)
        
        try:
            result = analyzer.analyze_intent(query)
            return {
                "intent": result.intent,
                "confidence": result.confidence,
                "entities": result.entities
            }
        finally:
            if hasattr(signal, 'SIGALRM'):
                signal.alarm(0)  # å–æ¶ˆè¶…æ™‚
    
    def analyze_execution_results(self, batches: List[ExecutionBatch]) -> Dict[str, Any]:
        """åˆ†æåŸ·è¡Œçµæœ"""
        print("ğŸ“Š åˆ†æåŸ·è¡Œçµæœ...")
        
        all_executions = []
        for batch in batches:
            all_executions.extend(batch.executions)
        
        if not all_executions:
            return {"error": "æ²’æœ‰åŸ·è¡Œçµæœå¯åˆ†æ"}
        
        # åŸºæœ¬çµ±è¨ˆ
        total_executions = len(all_executions)
        successful_executions = [e for e in all_executions if e.status == ExecutionStatus.COMPLETED]
        failed_executions = [e for e in all_executions if e.status == ExecutionStatus.FAILED]
        timeout_executions = [e for e in all_executions if e.status == ExecutionStatus.TIMEOUT]
        
        success_rate = len(successful_executions) / total_executions if total_executions > 0 else 0
        failure_rate = len(failed_executions) / total_executions if total_executions > 0 else 0
        timeout_rate = len(timeout_executions) / total_executions if total_executions > 0 else 0
        
        # æ€§èƒ½çµ±è¨ˆ
        durations = [e.duration for e in successful_executions if e.duration is not None]
        avg_duration = statistics.mean(durations) if durations else 0
        median_duration = statistics.median(durations) if durations else 0
        min_duration = min(durations) if durations else 0
        max_duration = max(durations) if durations else 0
        
        # æä¾›å•†çµ±è¨ˆ
        provider_stats = defaultdict(lambda: {"total": 0, "success": 0, "failed": 0, "timeout": 0, "avg_duration": 0})
        
        for execution in all_executions:
            provider = execution.provider.value
            provider_stats[provider]["total"] += 1
            
            if execution.status == ExecutionStatus.COMPLETED:
                provider_stats[provider]["success"] += 1
                if execution.duration:
                    provider_stats[provider]["avg_duration"] += execution.duration
            elif execution.status == ExecutionStatus.FAILED:
                provider_stats[provider]["failed"] += 1
            elif execution.status == ExecutionStatus.TIMEOUT:
                provider_stats[provider]["timeout"] += 1
        
        # è¨ˆç®—å¹³å‡æŒçºŒæ™‚é–“
        for provider, stats in provider_stats.items():
            if stats["success"] > 0:
                stats["avg_duration"] /= stats["success"]
            stats["success_rate"] = stats["success"] / stats["total"] if stats["total"] > 0 else 0
        
        # æ„åœ–æº–ç¢ºæ€§åˆ†æ
        intent_accuracy = self._analyze_intent_accuracy(successful_executions)
        
        # éŒ¯èª¤åˆ†æ
        error_analysis = self._analyze_errors(failed_executions)
        
        # æ€§èƒ½è¶¨å‹¢
        performance_trends = self._analyze_performance_trends(successful_executions)
        
        analysis = {
            "execution_summary": {
                "total_executions": total_executions,
                "successful_executions": len(successful_executions),
                "failed_executions": len(failed_executions),
                "timeout_executions": len(timeout_executions),
                "success_rate": success_rate,
                "failure_rate": failure_rate,
                "timeout_rate": timeout_rate
            },
            "performance_metrics": {
                "average_duration": avg_duration,
                "median_duration": median_duration,
                "min_duration": min_duration,
                "max_duration": max_duration,
                "total_duration": sum(durations)
            },
            "provider_statistics": dict(provider_stats),
            "intent_accuracy": intent_accuracy,
            "error_analysis": error_analysis,
            "performance_trends": performance_trends,
            "batch_summary": {
                "total_batches": len(batches),
                "avg_batch_duration": statistics.mean([b.total_duration for b in batches if b.total_duration]) if batches else 0
            }
        }
        
        return analysis
    
    def _analyze_intent_accuracy(self, executions: List[TestExecution]) -> Dict[str, Any]:
        """åˆ†ææ„åœ–æº–ç¢ºæ€§"""
        correct_predictions = 0
        total_predictions = 0
        
        intent_confusion = defaultdict(lambda: defaultdict(int))
        
        for execution in executions:
            if not execution.result or not execution.metadata:
                continue
                
            predicted_intent = execution.result.get("intent", "unknown")
            expected_intent = execution.metadata.get("expected_intent", "unknown")
            
            if expected_intent != "unknown":
                total_predictions += 1
                intent_confusion[expected_intent][predicted_intent] += 1
                
                if predicted_intent == expected_intent:
                    correct_predictions += 1
        
        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
        
        return {
            "overall_accuracy": accuracy,
            "correct_predictions": correct_predictions,
            "total_predictions": total_predictions,
            "confusion_matrix": dict(intent_confusion)
        }
    
    def _analyze_errors(self, failed_executions: List[TestExecution]) -> Dict[str, Any]:
        """åˆ†æéŒ¯èª¤"""
        error_types = Counter()
        error_by_provider = defaultdict(list)
        
        for execution in failed_executions:
            error_msg = execution.error or "æœªçŸ¥éŒ¯èª¤"
            error_types[error_msg] += 1
            error_by_provider[execution.provider.value].append(error_msg)
        
        return {
            "error_types": dict(error_types),
            "error_by_provider": dict(error_by_provider),
            "total_errors": len(failed_executions)
        }
    
    def _analyze_performance_trends(self, executions: List[TestExecution]) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½è¶¨å‹¢"""
        # æŒ‰æ™‚é–“æ’åº
        sorted_executions = sorted(executions, key=lambda x: x.start_time or datetime.min)
        
        # è¨ˆç®—ç§»å‹•å¹³å‡
        window_size = min(10, len(sorted_executions))
        moving_averages = []
        
        for i in range(len(sorted_executions) - window_size + 1):
            window = sorted_executions[i:i + window_size]
            avg_duration = statistics.mean([e.duration for e in window if e.duration])
            moving_averages.append(avg_duration)
        
        # æ€§èƒ½è®ŠåŒ–è¶¨å‹¢
        if len(moving_averages) >= 2:
            trend = "improving" if moving_averages[-1] < moving_averages[0] else "degrading"
        else:
            trend = "stable"
        
        return {
            "moving_averages": moving_averages,
            "trend": trend,
            "performance_variance": statistics.variance([e.duration for e in executions if e.duration]) if len(executions) > 1 else 0
        }
    
    def export_execution_results(self, batches: List[ExecutionBatch], 
                               analysis: Dict[str, Any], 
                               output_file: str = None) -> str:
        """å°å‡ºåŸ·è¡Œçµæœ"""
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"execution_results_{timestamp}.json"
        
        print(f"ğŸ’¾ å°å‡ºåŸ·è¡Œçµæœåˆ° {output_file}...")
        
        # æº–å‚™å°å‡ºæ•¸æ“š
        export_data = {
            "execution_info": {
                "execution_time": datetime.now().isoformat(),
                "config": {
                    "mode": self.config.mode.value,
                    "max_workers": self.config.max_workers,
                    "timeout_seconds": self.config.timeout_seconds,
                    "retry_attempts": self.config.retry_attempts,
                    "batch_size": self.config.batch_size
                }
            },
            "batches": [],
            "analysis": analysis
        }
        
        # åºåˆ—åŒ–æ‰¹æ¬¡æ•¸æ“š
        for batch in batches:
            batch_data = {
                "batch_id": batch.batch_id,
                "start_time": batch.start_time.isoformat() if batch.start_time else None,
                "end_time": batch.end_time.isoformat() if batch.end_time else None,
                "total_duration": batch.total_duration,
                "success_count": batch.success_count,
                "failure_count": batch.failure_count,
                "timeout_count": batch.timeout_count,
                "executions": []
            }
            
            for execution in batch.executions:
                execution_data = {
                    "test_case_id": execution.test_case_id,
                    "provider": execution.provider.value,
                    "status": execution.status.value,
                    "start_time": execution.start_time.isoformat() if execution.start_time else None,
                    "end_time": execution.end_time.isoformat() if execution.end_time else None,
                    "duration": execution.duration,
                    "result": execution.result,
                    "error": execution.error,
                    "retry_count": execution.retry_count,
                    "metadata": execution.metadata
                }
                batch_data["executions"].append(execution_data)
            
            export_data["batches"].append(batch_data)
        
        # å¯«å…¥æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        print(f"   âœ… åŸ·è¡Œçµæœå·²å°å‡º")
        return output_file
    
    def print_execution_summary(self, analysis: Dict[str, Any]) -> None:
        """æ‰“å°åŸ·è¡Œæ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸš€ LLMæ¸¬è©¦åŸ·è¡Œçµæœæ‘˜è¦")
        print("=" * 60)
        
        # åŸ·è¡Œæ‘˜è¦
        summary = analysis.get("execution_summary", {})
        print(f"\nğŸ“Š åŸ·è¡Œæ‘˜è¦:")
        print(f"   ç¸½åŸ·è¡Œæ¬¡æ•¸: {summary.get('total_executions', 0)}")
        print(f"   æˆåŠŸåŸ·è¡Œ: {summary.get('successful_executions', 0)}")
        print(f"   å¤±æ•—åŸ·è¡Œ: {summary.get('failed_executions', 0)}")
        print(f"   è¶…æ™‚åŸ·è¡Œ: {summary.get('timeout_executions', 0)}")
        print(f"   æˆåŠŸç‡: {summary.get('success_rate', 0):.1%}")
        
        # æ€§èƒ½æŒ‡æ¨™
        performance = analysis.get("performance_metrics", {})
        print(f"\nâš¡ æ€§èƒ½æŒ‡æ¨™:")
        print(f"   å¹³å‡éŸ¿æ‡‰æ™‚é–“: {performance.get('average_duration', 0):.3f} ç§’")
        print(f"   ä¸­ä½æ•¸éŸ¿æ‡‰æ™‚é–“: {performance.get('median_duration', 0):.3f} ç§’")
        print(f"   æœ€å¿«éŸ¿æ‡‰: {performance.get('min_duration', 0):.3f} ç§’")
        print(f"   æœ€æ…¢éŸ¿æ‡‰: {performance.get('max_duration', 0):.3f} ç§’")
        
        # æä¾›å•†çµ±è¨ˆ
        provider_stats = analysis.get("provider_statistics", {})
        if provider_stats:
            print(f"\nğŸ¢ æä¾›å•†çµ±è¨ˆ:")
            for provider, stats in provider_stats.items():
                print(f"   {provider}:")
                print(f"     æˆåŠŸç‡: {stats.get('success_rate', 0):.1%}")
                print(f"     å¹³å‡éŸ¿æ‡‰æ™‚é–“: {stats.get('avg_duration', 0):.3f} ç§’")
                print(f"     åŸ·è¡Œæ¬¡æ•¸: {stats.get('total', 0)}")
        
        # æ„åœ–æº–ç¢ºæ€§
        intent_accuracy = analysis.get("intent_accuracy", {})
        if intent_accuracy:
            print(f"\nğŸ¯ æ„åœ–æº–ç¢ºæ€§:")
            print(f"   ç¸½é«”æº–ç¢ºç‡: {intent_accuracy.get('overall_accuracy', 0):.1%}")
            print(f"   æ­£ç¢ºé æ¸¬: {intent_accuracy.get('correct_predictions', 0)}")
            print(f"   ç¸½é æ¸¬æ•¸: {intent_accuracy.get('total_predictions', 0)}")
        
        # éŒ¯èª¤åˆ†æ
        error_analysis = analysis.get("error_analysis", {})
        if error_analysis.get("total_errors", 0) > 0:
            print(f"\nâŒ éŒ¯èª¤åˆ†æ:")
            print(f"   ç¸½éŒ¯èª¤æ•¸: {error_analysis.get('total_errors', 0)}")
            
            error_types = error_analysis.get("error_types", {})
            if error_types:
                print(f"   ä¸»è¦éŒ¯èª¤é¡å‹:")
                for error, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True)[:3]:
                    print(f"     {error}: {count} æ¬¡")
        
        print("\n" + "=" * 60)


def load_test_cases_from_file(file_path: str) -> List[Dict[str, Any]]:
    """å¾æ–‡ä»¶è¼‰å…¥æ¸¬è©¦æ¡ˆä¾‹"""
    print(f"ğŸ“‚ è¼‰å…¥æ¸¬è©¦æ¡ˆä¾‹å¾ {file_path}...")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if isinstance(data, dict) and "test_cases" in data:
            test_cases = data["test_cases"]
        elif isinstance(data, list):
            test_cases = data
        else:
            raise ValueError("ç„¡æ•ˆçš„æ–‡ä»¶æ ¼å¼")
        
        print(f"   âœ… æˆåŠŸè¼‰å…¥ {len(test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹")
        return test_cases
        
    except Exception as e:
        print(f"   âŒ è¼‰å…¥å¤±æ•—: {str(e)}")
        return []


def main():
    """ä¸»å‡½æ•¸ - æ¸¬è©¦åŸ·è¡Œå¼•æ“å…¥å£é»"""
    print("ğŸš€ LLMæ¸¬è©¦åŸ·è¡Œå¼•æ“")
    print("=" * 60)
    
    try:
        # ç²å–è¼¸å…¥æ–‡ä»¶
        input_file = input("è«‹è¼¸å…¥æ¸¬è©¦æ¡ˆä¾‹æ–‡ä»¶è·¯å¾‘ (æˆ–æŒ‰Enterä½¿ç”¨é»˜èª): ").strip()
        if not input_file:
            input_file = "generated_test_cases.json"
        
        # è¼‰å…¥æ¸¬è©¦æ¡ˆä¾‹
        test_cases = load_test_cases_from_file(input_file)
        
        if not test_cases:
            print("âŒ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ¸¬è©¦æ¡ˆä¾‹")
            return
        
        # é¸æ“‡æä¾›å•†
        print("\nğŸ¢ é¸æ“‡LLMæä¾›å•†:")
        available_providers = list(LLMProvider)
        for i, provider in enumerate(available_providers, 1):
            print(f"{i}. {provider.value}")
        
        provider_input = input("è«‹é¸æ“‡æä¾›å•† (ç”¨é€—è™Ÿåˆ†éš”ç·¨è™Ÿï¼Œæˆ–æŒ‰Enteré¸æ“‡æ‰€æœ‰): ").strip()
        
        if provider_input:
            try:
                indices = [int(x.strip()) - 1 for x in provider_input.split(",")]
                providers = [available_providers[i] for i in indices if 0 <= i < len(available_providers)]
            except:
                print("ç„¡æ•ˆçš„é¸æ“‡ï¼Œä½¿ç”¨æ‰€æœ‰æä¾›å•†")
                providers = available_providers
        else:
            providers = available_providers
        
        # é…ç½®åŸ·è¡Œåƒæ•¸
        print("\nâš™ï¸ åŸ·è¡Œé…ç½®:")
        print("1. å¿«é€Ÿæ¨¡å¼ (ä¸¦è¡Œï¼Œ5ç·šç¨‹)")
        print("2. æ¨™æº–æ¨¡å¼ (ä¸¦è¡Œï¼Œ10ç·šç¨‹)")
        print("3. æ‰¹æ¬¡æ¨¡å¼ (æ‰¹æ¬¡å¤§å°: 20)")
        print("4. é †åºæ¨¡å¼ (å–®ç·šç¨‹)")
        print("5. è‡ªå®šç¾©")
        
        config_choice = input("è«‹é¸æ“‡åŸ·è¡Œæ¨¡å¼ (1-5): ").strip()
        
        if config_choice == "1":
            config = ExecutionConfig(mode=ExecutionMode.PARALLEL, max_workers=5, timeout_seconds=15)
        elif config_choice == "2":
            config = ExecutionConfig(mode=ExecutionMode.PARALLEL, max_workers=10, timeout_seconds=30)
        elif config_choice == "3":
            config = ExecutionConfig(mode=ExecutionMode.BATCH, batch_size=20, max_workers=5)
        elif config_choice == "4":
            config = ExecutionConfig(mode=ExecutionMode.SEQUENTIAL, timeout_seconds=30)
        elif config_choice == "5":
            # è‡ªå®šç¾©é…ç½®
            mode_input = input("åŸ·è¡Œæ¨¡å¼ (sequential/parallel/batch/streaming): ").strip().lower()
            mode = ExecutionMode.PARALLEL
            if mode_input in ["sequential", "parallel", "batch", "streaming"]:
                mode = ExecutionMode(mode_input)
            
            max_workers = int(input("æœ€å¤§å·¥ä½œç·šç¨‹æ•¸ (é»˜èª5): ").strip() or "5")
            timeout = int(input("è¶…æ™‚æ™‚é–“(ç§’) (é»˜èª30): ").strip() or "30")
            
            config = ExecutionConfig(mode=mode, max_workers=max_workers, timeout_seconds=timeout)
        else:
            config = ExecutionConfig()
        
        # å‰µå»ºåŸ·è¡Œå¼•æ“
        engine = LLMTestExecutionEngine(config)
        
        # åŸ·è¡Œæ¸¬è©¦
        print(f"\nğŸš€ é–‹å§‹åŸ·è¡Œæ¸¬è©¦ï¼Œæ¨¡å¼: {config.mode.value}")
        print(f"   æ¸¬è©¦æ¡ˆä¾‹: {len(test_cases)}")
        print(f"   æä¾›å•†: {[p.value for p in providers]}")
        print(f"   ç¸½åŸ·è¡Œæ¬¡æ•¸: {len(test_cases) * len(providers)}")
        
        batches = engine.execute_test_cases(test_cases, providers)
        
        # åˆ†æçµæœ
        analysis = engine.analyze_execution_results(batches)
        
        # é¡¯ç¤ºæ‘˜è¦
        engine.print_execution_summary(analysis)
        
        # å°å‡ºçµæœ
        export_result = input("\næ˜¯å¦å°å‡ºåŸ·è¡Œçµæœï¼Ÿ (y/n): ").strip().lower()
        if export_result == 'y':
            output_file = engine.export_execution_results(batches, analysis)
            print(f"   ğŸ“„ çµæœæ–‡ä»¶: {output_file}")
        
        print("\nğŸ‰ æ¸¬è©¦åŸ·è¡Œå®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâŒ åŸ·è¡Œéç¨‹è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()