#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€²éšLLMæ€§èƒ½åˆ†æå·¥å…·
æä¾›æ·±å…¥çš„æ¨¡å‹æ€§èƒ½åˆ†æã€æ¯”è¼ƒå’Œå¯è¦–åŒ–åŠŸèƒ½

Author: JobSpy Team
Date: 2025-01-27
"""

import sys
import os
import json
import time
import asyncio
import statistics
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
from concurrent.futures import ThreadPoolExecutor, as_completed
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from jobseeker.llm_intent_analyzer import LLMIntentAnalyzer, LLMProvider
from jobseeker.llm_config import LLMConfigManager
from llm_test_scenarios_config import LLMTestScenariosConfig, TestScenario, TestComplexity, LanguageType


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ¨™æ•¸æ“šçµæ§‹"""
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    response_time_mean: float
    response_time_std: float
    response_time_p95: float
    confidence_mean: float
    confidence_std: float
    error_rate: float
    consistency_score: float
    robustness_score: float


@dataclass
class ModelComparison:
    """æ¨¡å‹æ¯”è¼ƒçµæœ"""
    model_a: str
    model_b: str
    accuracy_diff: float
    speed_ratio: float
    consistency_diff: float
    statistical_significance: bool
    p_value: float
    effect_size: float
    recommendation: str


@dataclass
class AnalysisInsight:
    """åˆ†ææ´å¯Ÿ"""
    category: str
    insight_type: str
    description: str
    confidence: float
    supporting_data: Dict[str, Any]
    recommendations: List[str]


class AdvancedLLMPerformanceAnalyzer:
    """é€²éšLLMæ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.config_manager = LLMConfigManager()
        self.scenario_config = LLMTestScenariosConfig()
        self.test_results: Dict[str, List[Dict[str, Any]]] = {}
        self.performance_metrics: Dict[str, PerformanceMetrics] = {}
        self.model_comparisons: List[ModelComparison] = []
        self.insights: List[AnalysisInsight] = []
        
        # è¨­ç½®matplotlibä¸­æ–‡å­—é«”
        plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei']
        plt.rcParams['axes.unicode_minus'] = False
    
    def run_comprehensive_analysis(self, providers: Optional[List[LLMProvider]] = None, 
                                 scenarios: Optional[List[TestScenario]] = None) -> Dict[str, Any]:
        """é‹è¡Œå…¨é¢çš„æ€§èƒ½åˆ†æ"""
        print("ğŸ”¬ é–‹å§‹é€²éšLLMæ€§èƒ½åˆ†æ")
        print("=" * 60)
        
        # ç¢ºå®šè¦æ¸¬è©¦çš„æä¾›å•†å’Œå ´æ™¯
        if providers is None:
            providers = self.config_manager.get_available_providers()
        
        if scenarios is None:
            scenarios = list(self.scenario_config.get_all_scenarios().keys())
        
        print(f"ğŸ“‹ åˆ†ææä¾›å•†: {[p.value for p in providers]}")
        print(f"ğŸ¯ åˆ†æå ´æ™¯: {[s.value for s in scenarios]}")
        
        # æ”¶é›†æ¸¬è©¦æ•¸æ“š
        self._collect_test_data(providers, scenarios)
        
        # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
        self._calculate_performance_metrics()
        
        # é€²è¡Œæ¨¡å‹æ¯”è¼ƒ
        self._perform_model_comparisons()
        
        # ç”Ÿæˆåˆ†ææ´å¯Ÿ
        self._generate_insights()
        
        # å‰µå»ºå¯è¦–åŒ–å ±å‘Š
        self._create_visualizations()
        
        # ç”Ÿæˆç¶œåˆå ±å‘Š
        comprehensive_report = self._generate_comprehensive_report()
        
        # ä¿å­˜çµæœ
        self._save_analysis_results(comprehensive_report)
        
        return comprehensive_report
    
    def _collect_test_data(self, providers: List[LLMProvider], scenarios: List[TestScenario]) -> None:
        """æ”¶é›†æ¸¬è©¦æ•¸æ“š"""
        print("\nğŸ“Š æ”¶é›†æ¸¬è©¦æ•¸æ“š...")
        
        for provider in providers:
            print(f"\nğŸ”„ æ¸¬è©¦æä¾›å•†: {provider.value}")
            
            try:
                analyzer = LLMIntentAnalyzer(provider=provider)
                provider_results = []
                
                for scenario in scenarios:
                    print(f"   ğŸ“ å ´æ™¯: {scenario.value}")
                    
                    # ç²å–å ´æ™¯çš„æ¸¬è©¦æŸ¥è©¢
                    queries = self.scenario_config.get_queries_by_scenario(scenario)
                    scenario_config = self.scenario_config.get_scenario_config(scenario)
                    
                    for query_data in queries:
                        # åŸ·è¡Œå¤šæ¬¡æ¸¬è©¦ä»¥è©•ä¼°ä¸€è‡´æ€§
                        retry_count = scenario_config.retry_count if scenario_config else 3
                        
                        for attempt in range(retry_count):
                            result = self._execute_single_test(
                                analyzer, query_data, provider.value, scenario.value, attempt
                            )
                            provider_results.append(result)
                
                self.test_results[provider.value] = provider_results
                print(f"   âœ… å®Œæˆ {len(provider_results)} å€‹æ¸¬è©¦")
                
            except Exception as e:
                print(f"   âŒ æ¸¬è©¦å¤±æ•—: {str(e)}")
                continue
    
    def _execute_single_test(self, analyzer: LLMIntentAnalyzer, query_data: Dict[str, Any], 
                           provider: str, scenario: str, attempt: int) -> Dict[str, Any]:
        """åŸ·è¡Œå–®å€‹æ¸¬è©¦"""
        start_time = time.time()
        
        try:
            # åŸ·è¡Œæ„åœ–åˆ†æ
            result = analyzer.analyze_intent(query_data["query"])
            
            response_time = time.time() - start_time
            
            # æ§‹å»ºæ¸¬è©¦çµæœ
            test_result = {
                "provider": provider,
                "scenario": scenario,
                "query_id": query_data["id"],
                "query": query_data["query"],
                "attempt": attempt,
                "expected_job_related": query_data["expected_job_related"],
                "actual_job_related": result.is_job_related,
                "confidence": result.confidence,
                "response_time": response_time,
                "intent_type": result.intent_type.value if hasattr(result.intent_type, 'value') else str(result.intent_type),
                "reasoning": getattr(result, 'llm_reasoning', ''),
                "complexity": query_data.get("complexity", TestComplexity.SIMPLE).value,
                "language": query_data.get("language", LanguageType.CHINESE_TRADITIONAL).value,
                "tags": query_data.get("tags", []),
                "group": query_data.get("group", None),
                "timestamp": datetime.now().isoformat(),
                "error": None
            }
            
            # æå–çµæ§‹åŒ–æ„åœ–
            if result.structured_intent and result.is_job_related:
                intent = result.structured_intent
                test_result["structured_intent"] = {
                    "job_titles": getattr(intent, 'job_titles', []),
                    "skills": getattr(intent, 'skills', []),
                    "locations": getattr(intent, 'locations', []),
                    "salary_range": getattr(intent, 'salary_range', None),
                    "work_type": getattr(intent, 'work_type', None)
                }
            else:
                test_result["structured_intent"] = {}
            
            return test_result
            
        except Exception as e:
            response_time = time.time() - start_time
            return {
                "provider": provider,
                "scenario": scenario,
                "query_id": query_data["id"],
                "query": query_data["query"],
                "attempt": attempt,
                "expected_job_related": query_data["expected_job_related"],
                "actual_job_related": False,
                "confidence": 0.0,
                "response_time": response_time,
                "intent_type": "error",
                "reasoning": "",
                "complexity": query_data.get("complexity", TestComplexity.SIMPLE).value,
                "language": query_data.get("language", LanguageType.CHINESE_TRADITIONAL).value,
                "tags": query_data.get("tags", []),
                "group": query_data.get("group", None),
                "timestamp": datetime.now().isoformat(),
                "structured_intent": {},
                "error": str(e)
            }
    
    def _calculate_performance_metrics(self) -> None:
        """è¨ˆç®—è©³ç´°çš„æ€§èƒ½æŒ‡æ¨™"""
        print("\nğŸ“ˆ è¨ˆç®—æ€§èƒ½æŒ‡æ¨™...")
        
        for provider, results in self.test_results.items():
            if not results:
                continue
            
            # éæ¿¾æœ‰æ•ˆçµæœ
            valid_results = [r for r in results if r["error"] is None]
            
            if not valid_results:
                continue
            
            # è¨ˆç®—åŸºæœ¬æŒ‡æ¨™
            y_true = [r["expected_job_related"] for r in valid_results]
            y_pred = [r["actual_job_related"] for r in valid_results]
            
            # æº–ç¢ºç‡
            accuracy = sum(1 for t, p in zip(y_true, y_pred) if t == p) / len(y_true)
            
            # ç²¾ç¢ºç‡ã€å¬å›ç‡ã€F1åˆ†æ•¸
            tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[False, True]).ravel()
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            # éŸ¿æ‡‰æ™‚é–“çµ±è¨ˆ
            response_times = [r["response_time"] for r in valid_results]
            response_time_mean = statistics.mean(response_times)
            response_time_std = statistics.stdev(response_times) if len(response_times) > 1 else 0
            response_time_p95 = np.percentile(response_times, 95)
            
            # ç½®ä¿¡åº¦çµ±è¨ˆ
            confidences = [r["confidence"] for r in valid_results]
            confidence_mean = statistics.mean(confidences)
            confidence_std = statistics.stdev(confidences) if len(confidences) > 1 else 0
            
            # éŒ¯èª¤ç‡
            error_count = len([r for r in results if r["error"] is not None])
            error_rate = error_count / len(results)
            
            # ä¸€è‡´æ€§åˆ†æ•¸ï¼ˆåŒä¸€æŸ¥è©¢å¤šæ¬¡åŸ·è¡Œçš„ä¸€è‡´æ€§ï¼‰
            consistency_score = self._calculate_consistency_score(valid_results)
            
            # é­¯æ£’æ€§åˆ†æ•¸ï¼ˆå°å™ªéŸ³å’Œç•°å¸¸è¼¸å…¥çš„è™•ç†èƒ½åŠ›ï¼‰
            robustness_score = self._calculate_robustness_score(valid_results)
            
            self.performance_metrics[provider] = PerformanceMetrics(
                accuracy=accuracy,
                precision=precision,
                recall=recall,
                f1_score=f1_score,
                response_time_mean=response_time_mean,
                response_time_std=response_time_std,
                response_time_p95=response_time_p95,
                confidence_mean=confidence_mean,
                confidence_std=confidence_std,
                error_rate=error_rate,
                consistency_score=consistency_score,
                robustness_score=robustness_score
            )
            
            print(f"   âœ… {provider}: æº–ç¢ºç‡={accuracy:.3f}, F1={f1_score:.3f}, éŸ¿æ‡‰æ™‚é–“={response_time_mean:.2f}s")
    
    def _calculate_consistency_score(self, results: List[Dict[str, Any]]) -> float:
        """è¨ˆç®—ä¸€è‡´æ€§åˆ†æ•¸"""
        # æŒ‰æŸ¥è©¢IDåˆ†çµ„
        query_groups = defaultdict(list)
        for result in results:
            if result.get("group"):
                query_groups[result["group"]].append(result)
        
        if not query_groups:
            return 1.0  # æ²’æœ‰é‡è¤‡æŸ¥è©¢ï¼Œå‡è¨­å®Œå…¨ä¸€è‡´
        
        consistency_scores = []
        
        for group, group_results in query_groups.items():
            if len(group_results) < 2:
                continue
            
            # è¨ˆç®—è©²çµ„å…§çš„ä¸€è‡´æ€§
            predictions = [r["actual_job_related"] for r in group_results]
            confidences = [r["confidence"] for r in group_results]
            
            # é æ¸¬ä¸€è‡´æ€§
            prediction_consistency = len(set(predictions)) == 1
            
            # ç½®ä¿¡åº¦ä¸€è‡´æ€§ï¼ˆæ¨™æº–å·®è¶Šå°è¶Šä¸€è‡´ï¼‰
            confidence_consistency = 1.0 - (statistics.stdev(confidences) if len(confidences) > 1 else 0)
            
            group_consistency = (prediction_consistency * 0.7 + confidence_consistency * 0.3)
            consistency_scores.append(group_consistency)
        
        return statistics.mean(consistency_scores) if consistency_scores else 1.0
    
    def _calculate_robustness_score(self, results: List[Dict[str, Any]]) -> float:
        """è¨ˆç®—é­¯æ£’æ€§åˆ†æ•¸"""
        # æ‰¾å‡ºé­¯æ£’æ€§æ¸¬è©¦çš„çµæœ
        robustness_results = [r for r in results if "robustness" in r.get("scenario", "").lower()]
        
        if not robustness_results:
            return 0.5  # æ²’æœ‰é­¯æ£’æ€§æ¸¬è©¦ï¼Œçµ¦äºˆä¸­ç­‰åˆ†æ•¸
        
        # è¨ˆç®—åœ¨å™ªéŸ³å’Œç•°å¸¸è¼¸å…¥ä¸‹çš„æº–ç¢ºç‡
        correct_predictions = sum(1 for r in robustness_results 
                                if r["expected_job_related"] == r["actual_job_related"])
        
        return correct_predictions / len(robustness_results)
    
    def _perform_model_comparisons(self) -> None:
        """é€²è¡Œæ¨¡å‹é–“çš„çµ±è¨ˆæ¯”è¼ƒ"""
        print("\nğŸ” é€²è¡Œæ¨¡å‹æ¯”è¼ƒåˆ†æ...")
        
        providers = list(self.performance_metrics.keys())
        
        for i in range(len(providers)):
            for j in range(i + 1, len(providers)):
                model_a, model_b = providers[i], providers[j]
                
                comparison = self._compare_two_models(model_a, model_b)
                self.model_comparisons.append(comparison)
                
                print(f"   ğŸ“Š {model_a} vs {model_b}: æº–ç¢ºç‡å·®ç•°={comparison.accuracy_diff:.3f}, é€Ÿåº¦æ¯”={comparison.speed_ratio:.2f}")
    
    def _compare_two_models(self, model_a: str, model_b: str) -> ModelComparison:
        """æ¯”è¼ƒå…©å€‹æ¨¡å‹çš„æ€§èƒ½"""
        metrics_a = self.performance_metrics[model_a]
        metrics_b = self.performance_metrics[model_b]
        
        results_a = [r for r in self.test_results[model_a] if r["error"] is None]
        results_b = [r for r in self.test_results[model_b] if r["error"] is None]
        
        # æº–ç¢ºç‡å·®ç•°
        accuracy_diff = metrics_a.accuracy - metrics_b.accuracy
        
        # é€Ÿåº¦æ¯”è¼ƒï¼ˆmodel_aç›¸å°æ–¼model_bçš„é€Ÿåº¦ï¼‰
        speed_ratio = metrics_b.response_time_mean / metrics_a.response_time_mean
        
        # ä¸€è‡´æ€§å·®ç•°
        consistency_diff = metrics_a.consistency_score - metrics_b.consistency_score
        
        # çµ±è¨ˆé¡¯è‘—æ€§æª¢é©—ï¼ˆä½¿ç”¨æº–ç¢ºç‡ï¼‰
        accuracies_a = [1 if r["expected_job_related"] == r["actual_job_related"] else 0 for r in results_a]
        accuracies_b = [1 if r["expected_job_related"] == r["actual_job_related"] else 0 for r in results_b]
        
        if len(accuracies_a) > 1 and len(accuracies_b) > 1:
            t_stat, p_value = stats.ttest_ind(accuracies_a, accuracies_b)
            statistical_significance = p_value < 0.05
            
            # æ•ˆæ‡‰å¤§å°ï¼ˆCohen's dï¼‰
            pooled_std = np.sqrt(((len(accuracies_a) - 1) * np.var(accuracies_a, ddof=1) + 
                                 (len(accuracies_b) - 1) * np.var(accuracies_b, ddof=1)) / 
                                (len(accuracies_a) + len(accuracies_b) - 2))
            effect_size = (np.mean(accuracies_a) - np.mean(accuracies_b)) / pooled_std if pooled_std > 0 else 0
        else:
            p_value = 1.0
            statistical_significance = False
            effect_size = 0.0
        
        # ç”Ÿæˆå»ºè­°
        recommendation = self._generate_model_recommendation(model_a, model_b, metrics_a, metrics_b, 
                                                           accuracy_diff, speed_ratio, statistical_significance)
        
        return ModelComparison(
            model_a=model_a,
            model_b=model_b,
            accuracy_diff=accuracy_diff,
            speed_ratio=speed_ratio,
            consistency_diff=consistency_diff,
            statistical_significance=statistical_significance,
            p_value=p_value,
            effect_size=effect_size,
            recommendation=recommendation
        )
    
    def _generate_model_recommendation(self, model_a: str, model_b: str, 
                                     metrics_a: PerformanceMetrics, metrics_b: PerformanceMetrics,
                                     accuracy_diff: float, speed_ratio: float, 
                                     statistical_significance: bool) -> str:
        """ç”Ÿæˆæ¨¡å‹é¸æ“‡å»ºè­°"""
        if not statistical_significance:
            if speed_ratio > 1.2:
                return f"{model_a}é€Ÿåº¦æ›´å¿«ï¼Œæº–ç¢ºç‡ç„¡é¡¯è‘—å·®ç•°ï¼Œæ¨è–¦ç”¨æ–¼å³æ™‚æ‡‰ç”¨"
            elif speed_ratio < 0.8:
                return f"{model_b}é€Ÿåº¦æ›´å¿«ï¼Œæº–ç¢ºç‡ç„¡é¡¯è‘—å·®ç•°ï¼Œæ¨è–¦ç”¨æ–¼å³æ™‚æ‡‰ç”¨"
            else:
                return "å…©æ¨¡å‹æ€§èƒ½ç›¸è¿‘ï¼Œå¯æ ¹æ“šå…¶ä»–å› ç´ é¸æ“‡"
        
        if accuracy_diff > 0.05:
            return f"{model_a}æº–ç¢ºç‡é¡¯è‘—æ›´é«˜ï¼Œæ¨è–¦ç”¨æ–¼æº–ç¢ºæ€§è¦æ±‚é«˜çš„å ´æ™¯"
        elif accuracy_diff < -0.05:
            return f"{model_b}æº–ç¢ºç‡é¡¯è‘—æ›´é«˜ï¼Œæ¨è–¦ç”¨æ–¼æº–ç¢ºæ€§è¦æ±‚é«˜çš„å ´æ™¯"
        else:
            if speed_ratio > 1.2:
                return f"æº–ç¢ºç‡ç›¸è¿‘ï¼Œ{model_a}é€Ÿåº¦æ›´å¿«ï¼Œæ¨è–¦ç”¨æ–¼å³æ™‚æ‡‰ç”¨"
            elif speed_ratio < 0.8:
                return f"æº–ç¢ºç‡ç›¸è¿‘ï¼Œ{model_b}é€Ÿåº¦æ›´å¿«ï¼Œæ¨è–¦ç”¨æ–¼å³æ™‚æ‡‰ç”¨"
            else:
                return "å…©æ¨¡å‹ç¶œåˆæ€§èƒ½ç›¸è¿‘"
    
    def _generate_insights(self) -> None:
        """ç”Ÿæˆåˆ†ææ´å¯Ÿ"""
        print("\nğŸ’¡ ç”Ÿæˆåˆ†ææ´å¯Ÿ...")
        
        # æ€§èƒ½è¶¨å‹¢æ´å¯Ÿ
        self._analyze_performance_trends()
        
        # å ´æ™¯ç‰¹å®šæ´å¯Ÿ
        self._analyze_scenario_performance()
        
        # èªè¨€å’Œè¤‡é›œåº¦æ´å¯Ÿ
        self._analyze_language_complexity_impact()
        
        # ç•°å¸¸æª¢æ¸¬æ´å¯Ÿ
        self._detect_performance_anomalies()
        
        print(f"   âœ… ç”Ÿæˆäº† {len(self.insights)} å€‹æ´å¯Ÿ")
    
    def _analyze_performance_trends(self) -> None:
        """åˆ†ææ€§èƒ½è¶¨å‹¢"""
        if len(self.performance_metrics) < 2:
            return
        
        # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®æ€§èƒ½çš„æ¨¡å‹
        best_accuracy = max(self.performance_metrics.items(), key=lambda x: x[1].accuracy)
        worst_accuracy = min(self.performance_metrics.items(), key=lambda x: x[1].accuracy)
        
        fastest_model = min(self.performance_metrics.items(), key=lambda x: x[1].response_time_mean)
        slowest_model = max(self.performance_metrics.items(), key=lambda x: x[1].response_time_mean)
        
        # æº–ç¢ºç‡æ´å¯Ÿ
        accuracy_gap = best_accuracy[1].accuracy - worst_accuracy[1].accuracy
        if accuracy_gap > 0.1:
            self.insights.append(AnalysisInsight(
                category="æ€§èƒ½å·®ç•°",
                insight_type="æº–ç¢ºç‡å·®è·",
                description=f"æ¨¡å‹é–“æº–ç¢ºç‡å·®è·è¼ƒå¤§({accuracy_gap:.1%})ï¼Œ{best_accuracy[0]}è¡¨ç¾æœ€ä½³({best_accuracy[1].accuracy:.1%})ï¼Œ{worst_accuracy[0]}éœ€è¦æ”¹é€²({worst_accuracy[1].accuracy:.1%})",
                confidence=0.9,
                supporting_data={
                    "best_model": best_accuracy[0],
                    "best_accuracy": best_accuracy[1].accuracy,
                    "worst_model": worst_accuracy[0],
                    "worst_accuracy": worst_accuracy[1].accuracy,
                    "gap": accuracy_gap
                },
                recommendations=[
                    f"å„ªå…ˆä½¿ç”¨{best_accuracy[0]}é€²è¡Œç”Ÿç”¢éƒ¨ç½²",
                    f"æª¢æŸ¥{worst_accuracy[0]}çš„é…ç½®å’Œæç¤ºè©è¨­è¨ˆ",
                    "è€ƒæ…®å°è¡¨ç¾è¼ƒå·®çš„æ¨¡å‹é€²è¡Œå¾®èª¿æˆ–åƒæ•¸å„ªåŒ–"
                ]
            ))
        
        # é€Ÿåº¦æ´å¯Ÿ
        speed_ratio = slowest_model[1].response_time_mean / fastest_model[1].response_time_mean
        if speed_ratio > 2.0:
            self.insights.append(AnalysisInsight(
                category="æ€§èƒ½å·®ç•°",
                insight_type="éŸ¿æ‡‰é€Ÿåº¦å·®è·",
                description=f"æ¨¡å‹é–“éŸ¿æ‡‰é€Ÿåº¦å·®è·é¡¯è‘—ï¼Œ{fastest_model[0]}æ¯”{slowest_model[0]}å¿«{speed_ratio:.1f}å€",
                confidence=0.85,
                supporting_data={
                    "fastest_model": fastest_model[0],
                    "fastest_time": fastest_model[1].response_time_mean,
                    "slowest_model": slowest_model[0],
                    "slowest_time": slowest_model[1].response_time_mean,
                    "speed_ratio": speed_ratio
                },
                recommendations=[
                    f"å³æ™‚æ‡‰ç”¨å ´æ™¯å„ªå…ˆé¸æ“‡{fastest_model[0]}",
                    f"æª¢æŸ¥{slowest_model[0]}çš„ç¶²è·¯é€£æ¥å’ŒAPIé…ç½®",
                    "è€ƒæ…®ä½¿ç”¨å¿«å–æ©Ÿåˆ¶æ¸›å°‘é‡è¤‡æŸ¥è©¢çš„éŸ¿æ‡‰æ™‚é–“"
                ]
            ))
    
    def _analyze_scenario_performance(self) -> None:
        """åˆ†æå ´æ™¯ç‰¹å®šæ€§èƒ½"""
        scenario_performance = defaultdict(lambda: defaultdict(list))
        
        # æŒ‰å ´æ™¯å’Œæä¾›å•†åˆ†çµ„çµ±è¨ˆ
        for provider, results in self.test_results.items():
            for result in results:
                if result["error"] is None:
                    scenario = result["scenario"]
                    accuracy = 1 if result["expected_job_related"] == result["actual_job_related"] else 0
                    scenario_performance[scenario][provider].append(accuracy)
        
        # åˆ†ææ¯å€‹å ´æ™¯çš„è¡¨ç¾
        for scenario, provider_results in scenario_performance.items():
            if len(provider_results) < 2:
                continue
            
            # è¨ˆç®—å„æä¾›å•†åœ¨è©²å ´æ™¯çš„å¹³å‡æº–ç¢ºç‡
            scenario_accuracies = {}
            for provider, accuracies in provider_results.items():
                scenario_accuracies[provider] = statistics.mean(accuracies)
            
            # æ‰¾å‡ºè©²å ´æ™¯çš„æœ€ä½³å’Œæœ€å·®è¡¨ç¾
            best_provider = max(scenario_accuracies.items(), key=lambda x: x[1])
            worst_provider = min(scenario_accuracies.items(), key=lambda x: x[1])
            
            performance_gap = best_provider[1] - worst_provider[1]
            
            if performance_gap > 0.15:  # 15%ä»¥ä¸Šçš„å·®è·
                self.insights.append(AnalysisInsight(
                    category="å ´æ™¯ç‰¹å®šæ€§èƒ½",
                    insight_type=f"{scenario}å ´æ™¯åˆ†æ",
                    description=f"åœ¨{scenario}å ´æ™¯ä¸­ï¼Œ{best_provider[0]}è¡¨ç¾æœ€ä½³({best_provider[1]:.1%})ï¼Œ{worst_provider[0]}è¡¨ç¾è¼ƒå·®({worst_provider[1]:.1%})",
                    confidence=0.8,
                    supporting_data={
                        "scenario": scenario,
                        "best_provider": best_provider[0],
                        "best_accuracy": best_provider[1],
                        "worst_provider": worst_provider[0],
                        "worst_accuracy": worst_provider[1],
                        "all_accuracies": scenario_accuracies
                    },
                    recommendations=[
                        f"åœ¨{scenario}å ´æ™¯ä¸­å„ªå…ˆä½¿ç”¨{best_provider[0]}",
                        f"åˆ†æ{best_provider[0]}åœ¨è©²å ´æ™¯çš„æˆåŠŸå› ç´ ",
                        f"é‡å°{scenario}å ´æ™¯å„ªåŒ–{worst_provider[0]}çš„è¡¨ç¾"
                    ]
                ))
    
    def _analyze_language_complexity_impact(self) -> None:
        """åˆ†æèªè¨€å’Œè¤‡é›œåº¦å°æ€§èƒ½çš„å½±éŸ¿"""
        # æŒ‰èªè¨€åˆ†æ
        language_performance = defaultdict(lambda: defaultdict(list))
        complexity_performance = defaultdict(lambda: defaultdict(list))
        
        for provider, results in self.test_results.items():
            for result in results:
                if result["error"] is None:
                    language = result["language"]
                    complexity = result["complexity"]
                    accuracy = 1 if result["expected_job_related"] == result["actual_job_related"] else 0
                    
                    language_performance[language][provider].append(accuracy)
                    complexity_performance[complexity][provider].append(accuracy)
        
        # èªè¨€å½±éŸ¿åˆ†æ
        for language, provider_results in language_performance.items():
            if len(provider_results) < 2:
                continue
            
            language_accuracies = {}
            for provider, accuracies in provider_results.items():
                language_accuracies[provider] = statistics.mean(accuracies)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰é¡¯è‘—çš„èªè¨€åå¥½
            accuracy_values = list(language_accuracies.values())
            if len(accuracy_values) > 1:
                accuracy_std = statistics.stdev(accuracy_values)
                if accuracy_std > 0.1:  # æ¨™æº–å·®å¤§æ–¼10%
                    best_provider = max(language_accuracies.items(), key=lambda x: x[1])
                    
                    self.insights.append(AnalysisInsight(
                        category="èªè¨€ç‰¹å®šæ€§èƒ½",
                        insight_type=f"{language}èªè¨€åˆ†æ",
                        description=f"åœ¨{language}èªè¨€æ¸¬è©¦ä¸­ï¼Œæ¨¡å‹è¡¨ç¾å·®ç•°è¼ƒå¤§ï¼Œ{best_provider[0]}è¡¨ç¾æœ€ä½³({best_provider[1]:.1%})",
                        confidence=0.75,
                        supporting_data={
                            "language": language,
                            "provider_accuracies": language_accuracies,
                            "std_dev": accuracy_std
                        },
                        recommendations=[
                            f"è™•ç†{language}èªè¨€æŸ¥è©¢æ™‚å„ªå…ˆä½¿ç”¨{best_provider[0]}",
                            "è€ƒæ…®é‡å°ç‰¹å®šèªè¨€é€²è¡Œæ¨¡å‹å„ªåŒ–",
                            "åˆ†æèªè¨€ç‰¹å®šçš„æç¤ºè©è¨­è¨ˆ"
                        ]
                    ))
    
    def _detect_performance_anomalies(self) -> None:
        """æª¢æ¸¬æ€§èƒ½ç•°å¸¸"""
        for provider, metrics in self.performance_metrics.items():
            anomalies = []
            
            # æª¢æ¸¬ç•°å¸¸é«˜çš„éŒ¯èª¤ç‡
            if metrics.error_rate > 0.1:  # 10%ä»¥ä¸ŠéŒ¯èª¤ç‡
                anomalies.append(f"éŒ¯èª¤ç‡éé«˜({metrics.error_rate:.1%})")
            
            # æª¢æ¸¬ç•°å¸¸æ…¢çš„éŸ¿æ‡‰æ™‚é–“
            if metrics.response_time_mean > 10.0:  # è¶…é10ç§’
                anomalies.append(f"éŸ¿æ‡‰æ™‚é–“éæ…¢({metrics.response_time_mean:.1f}ç§’)")
            
            # æª¢æ¸¬ç•°å¸¸ä½çš„ä¸€è‡´æ€§
            if metrics.consistency_score < 0.7:  # ä¸€è‡´æ€§ä½æ–¼70%
                anomalies.append(f"ä¸€è‡´æ€§è¼ƒä½({metrics.consistency_score:.1%})")
            
            # æª¢æ¸¬ç½®ä¿¡åº¦ç•°å¸¸
            if metrics.confidence_std > 0.3:  # ç½®ä¿¡åº¦æ¨™æº–å·®éå¤§
                anomalies.append(f"ç½®ä¿¡åº¦ä¸ç©©å®š(æ¨™æº–å·®={metrics.confidence_std:.2f})")
            
            if anomalies:
                self.insights.append(AnalysisInsight(
                    category="æ€§èƒ½ç•°å¸¸",
                    insight_type=f"{provider}ç•°å¸¸æª¢æ¸¬",
                    description=f"{provider}å­˜åœ¨æ€§èƒ½ç•°å¸¸: {', '.join(anomalies)}",
                    confidence=0.9,
                    supporting_data={
                        "provider": provider,
                        "anomalies": anomalies,
                        "metrics": asdict(metrics)
                    },
                    recommendations=[
                        "æª¢æŸ¥APIé…ç½®å’Œç¶²è·¯é€£æ¥",
                        "é©—è­‰æ¨¡å‹åƒæ•¸è¨­ç½®",
                        "è€ƒæ…®æ›´æ›æˆ–å„ªåŒ–è©²æä¾›å•†çš„ä½¿ç”¨",
                        "å¢åŠ é‡è©¦æ©Ÿåˆ¶å’ŒéŒ¯èª¤è™•ç†"
                    ]
                ))
    
    def _create_visualizations(self) -> None:
        """å‰µå»ºå¯è¦–åŒ–åœ–è¡¨"""
        print("\nğŸ“Š å‰µå»ºå¯è¦–åŒ–åœ–è¡¨...")
        
        if not self.performance_metrics:
            print("   âš ï¸ æ²’æœ‰æ€§èƒ½æ•¸æ“šï¼Œè·³éå¯è¦–åŒ–")
            return
        
        # è¨­ç½®åœ–è¡¨æ¨£å¼
        plt.style.use('seaborn-v0_8')
        
        # å‰µå»ºæ€§èƒ½æ¯”è¼ƒåœ–
        self._create_performance_comparison_chart()
        
        # å‰µå»ºéŸ¿æ‡‰æ™‚é–“åˆ†å¸ƒåœ–
        self._create_response_time_distribution()
        
        # å‰µå»ºå ´æ™¯æ€§èƒ½ç†±åŠ›åœ–
        self._create_scenario_heatmap()
        
        # å‰µå»ºç½®ä¿¡åº¦vsæº–ç¢ºç‡æ•£é»åœ–
        self._create_confidence_accuracy_scatter()
        
        print("   âœ… å¯è¦–åŒ–åœ–è¡¨å·²ç”Ÿæˆ")
    
    def _create_performance_comparison_chart(self) -> None:
        """å‰µå»ºæ€§èƒ½æ¯”è¼ƒåœ–è¡¨"""
        providers = list(self.performance_metrics.keys())
        metrics_names = ['æº–ç¢ºç‡', 'F1åˆ†æ•¸', 'ä¸€è‡´æ€§', 'é­¯æ£’æ€§']
        
        # æº–å‚™æ•¸æ“š
        data = []
        for provider in providers:
            metrics = self.performance_metrics[provider]
            data.append([
                metrics.accuracy,
                metrics.f1_score,
                metrics.consistency_score,
                metrics.robustness_score
            ])
        
        # å‰µå»ºé›·é”åœ–
        fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))
        
        angles = np.linspace(0, 2 * np.pi, len(metrics_names), endpoint=False).tolist()
        angles += angles[:1]  # é–‰åˆåœ–å½¢
        
        colors = plt.cm.Set3(np.linspace(0, 1, len(providers)))
        
        for i, (provider, provider_data) in enumerate(zip(providers, data)):
            values = provider_data + provider_data[:1]  # é–‰åˆæ•¸æ“š
            ax.plot(angles, values, 'o-', linewidth=2, label=provider, color=colors[i])
            ax.fill(angles, values, alpha=0.25, color=colors[i])
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics_names)
        ax.set_ylim(0, 1)
        ax.set_title('LLMæ¨¡å‹æ€§èƒ½æ¯”è¼ƒé›·é”åœ–', size=16, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax.grid(True)
        
        plt.tight_layout()
        plt.savefig('llm_performance_radar.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_response_time_distribution(self) -> None:
        """å‰µå»ºéŸ¿æ‡‰æ™‚é–“åˆ†å¸ƒåœ–"""
        fig, ax = plt.subplots(figsize=(12, 6))
        
        for provider, results in self.test_results.items():
            response_times = [r["response_time"] for r in results if r["error"] is None]
            if response_times:
                ax.hist(response_times, bins=30, alpha=0.7, label=provider, density=True)
        
        ax.set_xlabel('éŸ¿æ‡‰æ™‚é–“ (ç§’)')
        ax.set_ylabel('å¯†åº¦')
        ax.set_title('LLMæ¨¡å‹éŸ¿æ‡‰æ™‚é–“åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('llm_response_time_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_scenario_heatmap(self) -> None:
        """å‰µå»ºå ´æ™¯æ€§èƒ½ç†±åŠ›åœ–"""
        # æº–å‚™æ•¸æ“š
        scenario_data = defaultdict(lambda: defaultdict(list))
        
        for provider, results in self.test_results.items():
            for result in results:
                if result["error"] is None:
                    scenario = result["scenario"]
                    accuracy = 1 if result["expected_job_related"] == result["actual_job_related"] else 0
                    scenario_data[scenario][provider].append(accuracy)
        
        # è¨ˆç®—å¹³å‡æº–ç¢ºç‡
        scenarios = list(scenario_data.keys())
        providers = list(self.performance_metrics.keys())
        
        heatmap_data = np.zeros((len(scenarios), len(providers)))
        
        for i, scenario in enumerate(scenarios):
            for j, provider in enumerate(providers):
                if provider in scenario_data[scenario]:
                    heatmap_data[i, j] = statistics.mean(scenario_data[scenario][provider])
                else:
                    heatmap_data[i, j] = np.nan
        
        # å‰µå»ºç†±åŠ›åœ–
        fig, ax = plt.subplots(figsize=(10, 8))
        
        im = ax.imshow(heatmap_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)
        
        # è¨­ç½®æ¨™ç±¤
        ax.set_xticks(range(len(providers)))
        ax.set_xticklabels(providers, rotation=45, ha='right')
        ax.set_yticks(range(len(scenarios)))
        ax.set_yticklabels(scenarios)
        
        # æ·»åŠ æ•¸å€¼æ¨™è¨»
        for i in range(len(scenarios)):
            for j in range(len(providers)):
                if not np.isnan(heatmap_data[i, j]):
                    text = ax.text(j, i, f'{heatmap_data[i, j]:.2f}',
                                 ha="center", va="center", color="black", fontweight='bold')
        
        ax.set_title('å„å ´æ™¯ä¸‹çš„æ¨¡å‹æº–ç¢ºç‡ç†±åŠ›åœ–', fontsize=14, fontweight='bold')
        
        # æ·»åŠ é¡è‰²æ¢
        cbar = plt.colorbar(im, ax=ax)
        cbar.set_label('æº–ç¢ºç‡', rotation=270, labelpad=20)
        
        plt.tight_layout()
        plt.savefig('llm_scenario_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_confidence_accuracy_scatter(self) -> None:
        """å‰µå»ºç½®ä¿¡åº¦vsæº–ç¢ºç‡æ•£é»åœ–"""
        fig, ax = plt.subplots(figsize=(10, 8))
        
        colors = plt.cm.Set3(np.linspace(0, 1, len(self.test_results)))
        
        for i, (provider, results) in enumerate(self.test_results.items()):
            confidences = []
            accuracies = []
            
            for result in results:
                if result["error"] is None:
                    confidences.append(result["confidence"])
                    accuracy = 1 if result["expected_job_related"] == result["actual_job_related"] else 0
                    accuracies.append(accuracy)
            
            if confidences and accuracies:
                ax.scatter(confidences, accuracies, alpha=0.6, label=provider, 
                          color=colors[i], s=50)
        
        ax.set_xlabel('ç½®ä¿¡åº¦')
        ax.set_ylabel('æº–ç¢ºç‡ (0=éŒ¯èª¤, 1=æ­£ç¢º)')
        ax.set_title('ç½®ä¿¡åº¦ vs æº–ç¢ºç‡æ•£é»åœ–', fontsize=14, fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_xlim(0, 1)
        ax.set_ylim(-0.1, 1.1)
        
        plt.tight_layout()
        plt.savefig('llm_confidence_accuracy_scatter.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç¶œåˆåˆ†æå ±å‘Š"""
        return {
            "analysis_summary": {
                "timestamp": datetime.now().isoformat(),
                "total_providers": len(self.performance_metrics),
                "total_tests": sum(len(results) for results in self.test_results.values()),
                "total_insights": len(self.insights),
                "analysis_duration": "å®Œæ•´åˆ†æ"
            },
            "performance_metrics": {
                provider: asdict(metrics) for provider, metrics in self.performance_metrics.items()
            },
            "model_comparisons": [asdict(comp) for comp in self.model_comparisons],
            "insights": [asdict(insight) for insight in self.insights],
            "recommendations": self._generate_final_recommendations(),
            "raw_test_data": self.test_results
        }
    
    def _generate_final_recommendations(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€çµ‚å»ºè­°"""
        if not self.performance_metrics:
            return {}
        
        # æ‰¾å‡ºå„æ–¹é¢çš„æœ€ä½³æ¨¡å‹
        best_accuracy = max(self.performance_metrics.items(), key=lambda x: x[1].accuracy)
        fastest_model = min(self.performance_metrics.items(), key=lambda x: x[1].response_time_mean)
        most_consistent = max(self.performance_metrics.items(), key=lambda x: x[1].consistency_score)
        most_robust = max(self.performance_metrics.items(), key=lambda x: x[1].robustness_score)
        
        # è¨ˆç®—ç¶œåˆåˆ†æ•¸
        overall_scores = {}
        for provider, metrics in self.performance_metrics.items():
            # ç¶œåˆåˆ†æ•¸ = æº–ç¢ºç‡*0.3 + (1-æ¨™æº–åŒ–éŸ¿æ‡‰æ™‚é–“)*0.2 + ä¸€è‡´æ€§*0.25 + é­¯æ£’æ€§*0.25
            normalized_time = 1 - min(metrics.response_time_mean / 10.0, 1.0)  # å‡è¨­10ç§’ç‚ºæœ€å·®
            overall_score = (
                metrics.accuracy * 0.3 +
                normalized_time * 0.2 +
                metrics.consistency_score * 0.25 +
                metrics.robustness_score * 0.25
            )
            overall_scores[provider] = overall_score
        
        best_overall = max(overall_scores.items(), key=lambda x: x[1])
        
        return {
            "best_overall": {
                "provider": best_overall[0],
                "score": best_overall[1],
                "description": f"{best_overall[0]}åœ¨ç¶œåˆè©•ä¼°ä¸­è¡¨ç¾æœ€ä½³"
            },
            "best_accuracy": {
                "provider": best_accuracy[0],
                "accuracy": best_accuracy[1].accuracy,
                "description": f"{best_accuracy[0]}æº–ç¢ºç‡æœ€é«˜({best_accuracy[1].accuracy:.1%})"
            },
            "fastest": {
                "provider": fastest_model[0],
                "response_time": fastest_model[1].response_time_mean,
                "description": f"{fastest_model[0]}éŸ¿æ‡‰é€Ÿåº¦æœ€å¿«({fastest_model[1].response_time_mean:.2f}ç§’)"
            },
            "most_consistent": {
                "provider": most_consistent[0],
                "consistency": most_consistent[1].consistency_score,
                "description": f"{most_consistent[0]}ä¸€è‡´æ€§æœ€é«˜({most_consistent[1].consistency_score:.1%})"
            },
            "most_robust": {
                "provider": most_robust[0],
                "robustness": most_robust[1].robustness_score,
                "description": f"{most_robust[0]}é­¯æ£’æ€§æœ€å¼·({most_robust[1].robustness_score:.1%})"
            },
            "usage_scenarios": {
                "ç”Ÿç”¢ç’°å¢ƒ": best_overall[0],
                "å³æ™‚æ‡‰ç”¨": fastest_model[0],
                "é«˜æº–ç¢ºç‡éœ€æ±‚": best_accuracy[0],
                "ç©©å®šæ€§è¦æ±‚": most_consistent[0],
                "è¤‡é›œç’°å¢ƒ": most_robust[0]
            },
            "key_insights": [insight.description for insight in self.insights[:5]]  # å‰5å€‹é‡è¦æ´å¯Ÿ
        }
    
    def _save_analysis_results(self, report: Dict[str, Any]) -> None:
        """ä¿å­˜åˆ†æçµæœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å®Œæ•´å ±å‘Š
        full_report_filename = f"advanced_llm_analysis_full_{timestamp}.json"
        with open(full_report_filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ‘˜è¦å ±å‘Š
        summary_report = {
            "analysis_summary": report["analysis_summary"],
            "performance_metrics": report["performance_metrics"],
            "recommendations": report["recommendations"],
            "key_insights": report["insights"][:10]  # å‰10å€‹æ´å¯Ÿ
        }
        
        summary_filename = f"advanced_llm_analysis_summary_{timestamp}.json"
        with open(summary_filename, 'w', encoding='utf-8') as f:
            json.dump(summary_report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ åˆ†æçµæœå·²ä¿å­˜:")
        print(f"   å®Œæ•´å ±å‘Š: {full_report_filename}")
        print(f"   æ‘˜è¦å ±å‘Š: {summary_filename}")
        print(f"   å¯è¦–åŒ–åœ–è¡¨: llm_*.png")
    
    def print_analysis_summary(self) -> None:
        """æ‰“å°åˆ†ææ‘˜è¦"""
        if not self.performance_metrics:
            print("âŒ æ²’æœ‰åˆ†æçµæœå¯é¡¯ç¤º")
            return
        
        print("\nğŸ“Š é€²éšLLMæ€§èƒ½åˆ†ææ‘˜è¦")
        print("=" * 60)
        
        # åŸºæœ¬çµ±è¨ˆ
        total_tests = sum(len(results) for results in self.test_results.values())
        print(f"\nğŸ“‹ åˆ†ææ¦‚æ³:")
        print(f"   åˆ†ææ¨¡å‹æ•¸é‡: {len(self.performance_metrics)}")
        print(f"   ç¸½æ¸¬è©¦æ¬¡æ•¸: {total_tests}")
        print(f"   ç”Ÿæˆæ´å¯Ÿæ•¸: {len(self.insights)}")
        print(f"   æ¨¡å‹æ¯”è¼ƒæ•¸: {len(self.model_comparisons)}")
        
        # æ€§èƒ½æ’å
        print(f"\nğŸ† æ€§èƒ½æ’å:")
        
        # æº–ç¢ºç‡æ’å
        accuracy_ranking = sorted(self.performance_metrics.items(), 
                                 key=lambda x: x[1].accuracy, reverse=True)
        print(f"\n   ğŸ“ˆ æº–ç¢ºç‡æ’å:")
        for i, (provider, metrics) in enumerate(accuracy_ranking, 1):
            print(f"      {i}. {provider}: {metrics.accuracy:.1%} (F1: {metrics.f1_score:.3f})")
        
        # é€Ÿåº¦æ’å
        speed_ranking = sorted(self.performance_metrics.items(), 
                              key=lambda x: x[1].response_time_mean)
        print(f"\n   âš¡ é€Ÿåº¦æ’å:")
        for i, (provider, metrics) in enumerate(speed_ranking, 1):
            print(f"      {i}. {provider}: {metrics.response_time_mean:.2f}s (P95: {metrics.response_time_p95:.2f}s)")
        
        # ä¸€è‡´æ€§æ’å
        consistency_ranking = sorted(self.performance_metrics.items(), 
                                    key=lambda x: x[1].consistency_score, reverse=True)
        print(f"\n   ğŸ¯ ä¸€è‡´æ€§æ’å:")
        for i, (provider, metrics) in enumerate(consistency_ranking, 1):
            print(f"      {i}. {provider}: {metrics.consistency_score:.1%}")
        
        # é‡è¦æ´å¯Ÿ
        print(f"\nğŸ’¡ é‡è¦æ´å¯Ÿ:")
        for i, insight in enumerate(self.insights[:5], 1):
            print(f"   {i}. [{insight.category}] {insight.description}")
        
        # æ¨¡å‹æ¯”è¼ƒ
        if self.model_comparisons:
            print(f"\nğŸ” é—œéµæ¯”è¼ƒ:")
            for comp in self.model_comparisons[:3]:
                significance = "é¡¯è‘—" if comp.statistical_significance else "ä¸é¡¯è‘—"
                print(f"   â€¢ {comp.model_a} vs {comp.model_b}: {comp.recommendation} (å·®ç•°{significance})")
        
        print("\n" + "=" * 60)


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ”¬ é€²éšLLMæ€§èƒ½åˆ†æå·¥å…·")
    print("=" * 50)
    
    # å‰µå»ºåˆ†æå™¨
    analyzer = AdvancedLLMPerformanceAnalyzer()
    
    # ç²å–å¯ç”¨çš„æä¾›å•†
    available_providers = analyzer.config_manager.get_available_providers()
    
    if not available_providers:
        print("âŒ æ²’æœ‰å¯ç”¨çš„LLMæä¾›å•†ï¼Œè«‹æª¢æŸ¥é…ç½®")
        return
    
    print(f"ğŸ“‹ ç™¼ç¾ {len(available_providers)} å€‹å¯ç”¨æä¾›å•†: {[p.value for p in available_providers]}")
    
    # ç²å–å¯ç”¨çš„æ¸¬è©¦å ´æ™¯
    available_scenarios = list(analyzer.scenario_config.get_all_scenarios().keys())
    print(f"ğŸ¯ å¯ç”¨æ¸¬è©¦å ´æ™¯: {[s.value for s in available_scenarios]}")
    
    # è©¢å•ç”¨æˆ¶é¸æ“‡
    print("\nè«‹é¸æ“‡åˆ†ææ¨¡å¼:")
    print("1. å¿«é€Ÿåˆ†æ (åŸºç¤åŠŸèƒ½æ¸¬è©¦)")
    print("2. æ¨™æº–åˆ†æ (å¤šå ´æ™¯æ¸¬è©¦)")
    print("3. å…¨é¢åˆ†æ (æ‰€æœ‰å ´æ™¯)")
    print("4. è‡ªå®šç¾©åˆ†æ")
    
    choice = input("\nè«‹è¼¸å…¥é¸æ“‡ (1-4): ").strip()
    
    if choice == "1":
        test_scenarios = [TestScenario.BASIC_FUNCTIONALITY]
    elif choice == "2":
        test_scenarios = [TestScenario.BASIC_FUNCTIONALITY, TestScenario.EDGE_CASES, TestScenario.MULTILINGUAL]
    elif choice == "3":
        test_scenarios = available_scenarios
    elif choice == "4":
        print("\nå¯ç”¨å ´æ™¯:")
        for i, scenario in enumerate(available_scenarios, 1):
            config = analyzer.scenario_config.get_scenario_config(scenario)
            print(f"  {i}. {config.name} - {config.description}")
        
        selected_indices = input("\nè«‹è¼¸å…¥è¦æ¸¬è©¦çš„å ´æ™¯ç·¨è™Ÿï¼ˆç”¨é€—è™Ÿåˆ†éš”ï¼‰: ").strip()
        try:
            indices = [int(x.strip()) - 1 for x in selected_indices.split(',')]
            test_scenarios = [available_scenarios[i] for i in indices if 0 <= i < len(available_scenarios)]
        except (ValueError, IndexError):
            print("âŒ è¼¸å…¥æ ¼å¼éŒ¯èª¤ï¼Œä½¿ç”¨æ¨™æº–åˆ†æ")
            test_scenarios = [TestScenario.BASIC_FUNCTIONALITY, TestScenario.EDGE_CASES]
    else:
        print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œä½¿ç”¨å¿«é€Ÿåˆ†æ")
        test_scenarios = [TestScenario.BASIC_FUNCTIONALITY]
    
    print(f"\nğŸ¯ å°‡åŸ·è¡Œä»¥ä¸‹å ´æ™¯æ¸¬è©¦: {[s.value for s in test_scenarios]}")
    
    # é‹è¡Œåˆ†æ
    try:
        print("\nğŸš€ é–‹å§‹åˆ†æ...")
        comprehensive_report = analyzer.run_comprehensive_analysis(
            providers=available_providers,
            scenarios=test_scenarios
        )
        
        if comprehensive_report:
            # é¡¯ç¤ºæ‘˜è¦å ±å‘Š
            analyzer.print_analysis_summary()
            
            print("\nâœ… åˆ†æå®Œæˆï¼")
            print("ğŸ“„ è©³ç´°å ±å‘Šå’Œå¯è¦–åŒ–åœ–è¡¨å·²ä¿å­˜")
        else:
            print("âŒ åˆ†æå¤±æ•—ï¼Œæ²’æœ‰ç”Ÿæˆå ±å‘Š")
            
    except KeyboardInterrupt:
        print("\nâš ï¸ åˆ†æè¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()