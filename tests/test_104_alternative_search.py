#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
104äººåŠ›éŠ€è¡Œæ›¿ä»£æœå°‹ç­–ç•¥æ¸¬è©¦
ç›´æ¥è¨ªå•æœå°‹URLï¼Œé¿å…é¦–é å½ˆå‡ºè¦–çª—å•é¡Œ
"""

import asyncio
import json
import urllib.parse
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright


class AlternativeSearchStrategy:
    """æ›¿ä»£æœå°‹ç­–ç•¥"""
    
    def __init__(self):
        self.base_url = "https://www.104.com.tw"
        self.search_base_url = "https://www.104.com.tw/jobs/search/"
        self.test_results = []
    
    async def test_direct_search_url(self, keyword="Pythonå·¥ç¨‹å¸«"):
        """æ¸¬è©¦ç›´æ¥è¨ªå•æœå°‹URL"""
        print(f"ğŸ” æ¸¬è©¦ç›´æ¥æœå°‹URL: {keyword}")
        
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                )
                page = await context.new_page()
                
                # æ§‹å»ºæœå°‹URL
                search_url = f"{self.search_base_url}?keyword={urllib.parse.quote(keyword)}"
                print(f"ğŸ“„ è¨ªå•æœå°‹URL: {search_url}")
                
                # ç›´æ¥è¨ªå•æœå°‹é é¢
                await page.goto(search_url, timeout=60000)
                await page.wait_for_load_state('networkidle')
                
                # æª¢æŸ¥é é¢æ¨™é¡Œ
                title = await page.title()
                print(f"ğŸ“„ é é¢æ¨™é¡Œ: {title}")
                
                # æª¢æŸ¥æœå°‹çµæœ
                job_elements = await self._find_job_elements(page)
                print(f"ğŸ“Š æ‰¾åˆ° {len(job_elements)} å€‹è·ä½çµæœ")
                
                # æª¢æŸ¥æœå°‹é—œéµå­—æ˜¯å¦æ­£ç¢ºé¡¯ç¤º
                search_keyword_displayed = await self._check_search_keyword(page, keyword)
                print(f"ğŸ” æœå°‹é—œéµå­—é¡¯ç¤º: {'âœ…' if search_keyword_displayed else 'âŒ'}")
                
                # æª¢æŸ¥æ˜¯å¦æœ‰åˆ†é 
                pagination_info = await self._check_pagination(page)
                print(f"ğŸ“„ åˆ†é è³‡è¨Š: {pagination_info}")
                
                await browser.close()
                
                result = {
                    "keyword": keyword,
                    "search_url": search_url,
                    "page_title": title,
                    "job_count": len(job_elements),
                    "keyword_displayed": search_keyword_displayed,
                    "pagination": pagination_info,
                    "success": len(job_elements) > 0,
                    "timestamp": datetime.now().isoformat()
                }
                
                print(f"âœ… æœå°‹æ¸¬è©¦å®Œæˆ: {'æˆåŠŸ' if result['success'] else 'å¤±æ•—'}")
                return result
                
        except Exception as e:
            print(f"âŒ æœå°‹æ¸¬è©¦å¤±æ•—: {e}")
            return {
                "keyword": keyword,
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    async def _find_job_elements(self, page):
        """å°‹æ‰¾è·ä½å…ƒç´ """
        job_selectors = [
            '.job-list-item',
            '.job-item', 
            '.job-card',
            '[data-testid="job-item"]',
            '.job-list .job-item',
            '.job-list-item-container',
            '.job-info',
            '.job-title',
            'article[data-job-id]',
            '.job-list > div'
        ]
        
        for selector in job_selectors:
            try:
                elements = await page.query_selector_all(selector)
                if elements:
                    print(f"âœ… ä½¿ç”¨é¸æ“‡å™¨æ‰¾åˆ°è·ä½: {selector}")
                    return elements
            except:
                continue
        
        print("âŒ æœªæ‰¾åˆ°è·ä½å…ƒç´ ")
        return []
    
    async def _check_search_keyword(self, page, keyword):
        """æª¢æŸ¥æœå°‹é—œéµå­—æ˜¯å¦æ­£ç¢ºé¡¯ç¤º"""
        try:
            # æª¢æŸ¥æœå°‹æ¡†ä¸­çš„å€¼
            search_input = await page.query_selector('input[type="text"]')
            if search_input:
                input_value = await search_input.evaluate('el => el.value')
                if keyword in input_value:
                    return True
            
            # æª¢æŸ¥é é¢ä¸­çš„æœå°‹é—œéµå­—é¡¯ç¤º
            keyword_elements = await page.query_selector_all(f'text="{keyword}"')
            if keyword_elements:
                return True
            
            # æª¢æŸ¥URLåƒæ•¸
            current_url = page.url
            if keyword in current_url:
                return True
                
            return False
        except:
            return False
    
    async def _check_pagination(self, page):
        """æª¢æŸ¥åˆ†é è³‡è¨Š"""
        try:
            # å°‹æ‰¾åˆ†é å…ƒç´ 
            pagination_selectors = [
                '.pagination',
                '.page-nav',
                '.pager',
                '[class*="page"]',
                '.job-list-pagination'
            ]
            
            for selector in pagination_selectors:
                pagination = await page.query_selector(selector)
                if pagination and await pagination.is_visible():
                    # æª¢æŸ¥ç¸½é æ•¸
                    page_links = await pagination.query_selector_all('a, button')
                    total_pages = len(page_links)
                    
                    # æª¢æŸ¥ç•¶å‰é 
                    current_page = await page.evaluate("""
                        () => {
                            const current = document.querySelector('.pagination .active, .current, .selected');
                            return current ? parseInt(current.textContent) || 1 : 1;
                        }
                    """)
                    
                    return {
                        "found": True,
                        "total_pages": total_pages,
                        "current_page": current_page,
                        "selector": selector
                    }
            
            return {"found": False}
        except Exception as e:
            return {"found": False, "error": str(e)}
    
    async def test_multiple_keywords(self, keywords=None):
        """æ¸¬è©¦å¤šå€‹é—œéµå­—"""
        if keywords is None:
            keywords = [
                "Pythonå·¥ç¨‹å¸«",
                "è»Ÿé«”å·¥ç¨‹å¸«", 
                "å‰ç«¯å·¥ç¨‹å¸«",
                "å¾Œç«¯å·¥ç¨‹å¸«",
                "è³‡æ–™åˆ†æå¸«",
                "ç”¢å“ç¶“ç†",
                "UIè¨­è¨ˆå¸«",
                "è¡ŒéŠ·å°ˆå“¡"
            ]
        
        print(f"ğŸ¯ é–‹å§‹æ¸¬è©¦ {len(keywords)} å€‹é—œéµå­—...")
        
        results = []
        for i, keyword in enumerate(keywords):
            print(f"\n--- æ¸¬è©¦ {i+1}/{len(keywords)}: {keyword} ---")
            result = await self.test_direct_search_url(keyword)
            results.append(result)
            
            # æ¸¬è©¦é–“éš”
            if i < len(keywords) - 1:
                delay = 2
                print(f"â³ ç­‰å¾… {delay} ç§’å¾Œç¹¼çºŒ...")
                await asyncio.sleep(delay)
        
        return results
    
    async def test_search_with_filters(self):
        """æ¸¬è©¦å¸¶ç¯©é¸æ¢ä»¶çš„æœå°‹"""
        print("ğŸ” æ¸¬è©¦å¸¶ç¯©é¸æ¢ä»¶çš„æœå°‹...")
        
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                )
                page = await context.new_page()
                
                # æ§‹å»ºå¸¶ç¯©é¸æ¢ä»¶çš„æœå°‹URL
                search_params = {
                    'keyword': 'Pythonå·¥ç¨‹å¸«',
                    'area': '6001001000',  # å°åŒ—å¸‚
                    'jobcat': '2007000000',  # è»Ÿé«”å·¥ç¨‹å¸«
                    'order': '1',  # ç›¸é—œæ€§æ’åº
                    'asc': '0'  # é™åº
                }
                
                query_string = '&'.join([f"{k}={v}" for k, v in search_params.items()])
                search_url = f"{self.search_base_url}?{query_string}"
                
                print(f"ğŸ“„ è¨ªå•å¸¶ç¯©é¸æ¢ä»¶çš„æœå°‹URL: {search_url}")
                
                await page.goto(search_url, timeout=60000)
                await page.wait_for_load_state('networkidle')
                
                # æª¢æŸ¥ç¯©é¸æ¢ä»¶æ˜¯å¦ç”Ÿæ•ˆ
                job_elements = await self._find_job_elements(page)
                print(f"ğŸ“Š æ‰¾åˆ° {len(job_elements)} å€‹è·ä½çµæœ")
                
                # æª¢æŸ¥åœ°å€ç¯©é¸
                area_filter_active = await self._check_filter_active(page, 'area')
                print(f"ğŸ¢ åœ°å€ç¯©é¸: {'âœ…' if area_filter_active else 'âŒ'}")
                
                # æª¢æŸ¥è·å‹™é¡åˆ¥ç¯©é¸
                jobcat_filter_active = await self._check_filter_active(page, 'jobcat')
                print(f"ğŸ’¼ è·å‹™é¡åˆ¥ç¯©é¸: {'âœ…' if jobcat_filter_active else 'âŒ'}")
                
                await browser.close()
                
                result = {
                    "search_url": search_url,
                    "job_count": len(job_elements),
                    "area_filter_active": area_filter_active,
                    "jobcat_filter_active": jobcat_filter_active,
                    "success": len(job_elements) > 0,
                    "timestamp": datetime.now().isoformat()
                }
                
                print(f"âœ… ç¯©é¸æœå°‹æ¸¬è©¦å®Œæˆ: {'æˆåŠŸ' if result['success'] else 'å¤±æ•—'}")
                return result
                
        except Exception as e:
            print(f"âŒ ç¯©é¸æœå°‹æ¸¬è©¦å¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    async def _check_filter_active(self, page, filter_type):
        """æª¢æŸ¥ç¯©é¸æ¢ä»¶æ˜¯å¦ç”Ÿæ•ˆ"""
        try:
            # æª¢æŸ¥ç¯©é¸æŒ‰éˆ•æ˜¯å¦ç‚ºæ´»èºç‹€æ…‹
            active_selectors = [
                f'[data-filter="{filter_type}"].active',
                f'[data-filter="{filter_type}"].selected',
                f'[data-filter="{filter_type}"].current',
                f'.filter-{filter_type}.active',
                f'.filter-{filter_type}.selected'
            ]
            
            for selector in active_selectors:
                element = await page.query_selector(selector)
                if element and await element.is_visible():
                    return True
            
            return False
        except:
            return False
    
    def generate_report(self, results):
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        if not results:
            print("âŒ æ²’æœ‰æ¸¬è©¦çµæœå¯å ±å‘Š")
            return
        
        # çµ±è¨ˆçµæœ
        total_tests = len(results)
        successful_tests = sum(1 for r in results if r.get("success", False))
        success_rate = successful_tests / total_tests * 100
        
        # è¨ˆç®—å¹³å‡è·ä½æ•¸é‡
        job_counts = [r.get("job_count", 0) for r in results if r.get("job_count", 0) > 0]
        avg_job_count = sum(job_counts) / len(job_counts) if job_counts else 0
        
        report = {
            "test_summary": {
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "success_rate": success_rate,
                "avg_job_count": avg_job_count,
                "test_type": "alternative_search_strategy"
            },
            "test_results": results,
            "generated_at": datetime.now().isoformat()
        }
        
        # ä¿å­˜å ±å‘Š
        report_path = Path("tests/results/104_alternative_search_report.json")
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # é¡¯ç¤ºå ±å‘Š
        print(f"\n{'='*60}")
        print(f"ğŸ“Š 104äººåŠ›éŠ€è¡Œæ›¿ä»£æœå°‹ç­–ç•¥æ¸¬è©¦å ±å‘Š")
        print(f"{'='*60}")
        print(f"ç¸½æ¸¬è©¦æ•¸: {total_tests}")
        print(f"æˆåŠŸæ¸¬è©¦æ•¸: {successful_tests}")
        print(f"æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"å¹³å‡è·ä½æ•¸é‡: {avg_job_count:.1f}")
        print(f"è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: {report_path}")
        
        return report


async def main():
    """ä¸»å‡½æ•¸"""
    strategy = AlternativeSearchStrategy()
    
    print("ğŸš€ é–‹å§‹104äººåŠ›éŠ€è¡Œæ›¿ä»£æœå°‹ç­–ç•¥æ¸¬è©¦...")
    
    # æ¸¬è©¦1: å–®ä¸€é—œéµå­—æœå°‹
    print("\n" + "="*50)
    print("æ¸¬è©¦1: å–®ä¸€é—œéµå­—æœå°‹")
    print("="*50)
    single_result = await strategy.test_direct_search_url("Pythonå·¥ç¨‹å¸«")
    
    # æ¸¬è©¦2: å¤šå€‹é—œéµå­—æœå°‹
    print("\n" + "="*50)
    print("æ¸¬è©¦2: å¤šå€‹é—œéµå­—æœå°‹")
    print("="*50)
    multiple_results = await strategy.test_multiple_keywords([
        "Pythonå·¥ç¨‹å¸«",
        "è»Ÿé«”å·¥ç¨‹å¸«",
        "å‰ç«¯å·¥ç¨‹å¸«"
    ])
    
    # æ¸¬è©¦3: å¸¶ç¯©é¸æ¢ä»¶çš„æœå°‹
    print("\n" + "="*50)
    print("æ¸¬è©¦3: å¸¶ç¯©é¸æ¢ä»¶çš„æœå°‹")
    print("="*50)
    filter_result = await strategy.test_search_with_filters()
    
    # åˆä½µæ‰€æœ‰çµæœ
    all_results = [single_result] + multiple_results + [filter_result]
    
    # ç”Ÿæˆå ±å‘Š
    strategy.generate_report(all_results)


if __name__ == "__main__":
    asyncio.run(main())
