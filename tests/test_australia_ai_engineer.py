#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
JobSpy æ¾³æ´² AI å·¥ç¨‹å¸«è·ä½çˆ¬èŸ²æ¸¬è©¦

é€™å€‹è…³æœ¬å°ˆé–€ç”¨æ–¼æ¸¬è©¦æ‰€æœ‰9å€‹ç¶²ç«™åœ¨æ¾³æ´²åœ°å€çš„AIå·¥ç¨‹å¸«è·ä½çˆ¬å–åŠŸèƒ½ã€‚
æ¯å€‹ç¶²ç«™è‡³å°‘çˆ¬å–20ç­†è³‡æ–™ï¼Œä¸¦ä¿å­˜åŸå§‹è³‡æ–™å’Œæ•´ç†å¾Œçš„CSVæª”æ¡ˆã€‚

Author: JobSpy Team
Date: 2024
"""

import os
import sys
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
import time
from typing import Dict, List, Any

# è¨­å®šè·¯å¾‘
project_root = Path(__file__).parent.parent
tests_dir = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(tests_dir))

try:
    from jobspy import scrape_jobs
    from jobspy.model import Site, Country
except ImportError as e:
    print(f"âŒ å°å…¥éŒ¯èª¤ï¼š{e}")
    print("è«‹ç¢ºä¿ JobSpy æ¨¡çµ„å¯ä»¥æ­£å¸¸å°å…¥")
    sys.exit(1)

class AustraliaAIEngineerTester:
    """
    æ¾³æ´² AI å·¥ç¨‹å¸«è·ä½æ¸¬è©¦é¡åˆ¥
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–æ¸¬è©¦å™¨
        """
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = tests_dir / "results" / f"australia_ai_engineer_{self.timestamp}"
        self.raw_data_dir = self.output_dir / "raw_data"
        self.csv_data_dir = self.output_dir / "csv_data"
        self.summary_dir = self.output_dir / "summary"
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.raw_data_dir.mkdir(parents=True, exist_ok=True)
        self.csv_data_dir.mkdir(parents=True, exist_ok=True)
        self.summary_dir.mkdir(parents=True, exist_ok=True)
        
        # æ”¯æ´çš„ç¶²ç«™åˆ—è¡¨
        self.sites = [
            Site.LINKEDIN,
            Site.INDEED,
            Site.ZIP_RECRUITER,  # æ­£ç¢ºçš„æšèˆ‰åç¨±
            Site.GLASSDOOR,
            Site.GOOGLE,
            # Site.BAYT,      # ä¸­æ±åœ°å€ï¼Œå¯èƒ½ä¸é©ç”¨æ¾³æ´²
            # Site.NAUKRI,    # å°åº¦åœ°å€ï¼Œå¯èƒ½ä¸é©ç”¨æ¾³æ´²
            # Site.BDJOBS,    # å­ŸåŠ æ‹‰åœ°å€ï¼Œå¯èƒ½ä¸é©ç”¨æ¾³æ´²
            Site.SEEK,       # æ¾³æ´²æœ¬åœ°ç¶²ç«™ï¼Œæ‡‰è©²æœ‰å¾ˆå¥½çš„çµæœ
        ]
        
        # æœå°‹åƒæ•¸
        self.search_params = {
            "search_term": "AI Engineer",
            "location": "Australia",
            "country_indeed": "australia",  # ä½¿ç”¨å­—ä¸²è€Œéæšèˆ‰
            "results_wanted": 25,  # æ¯å€‹ç¶²ç«™çˆ¬å–25ç­†ï¼Œç¢ºä¿è‡³å°‘20ç­†
            "job_type": "fulltime",
            "is_remote": False,
            "linkedin_fetch_description": True,
            "linkedin_company_ids": None,
            "offset": 0,
            "hours_old": 72,  # 3å¤©å…§çš„è·ä½
        }
        
        self.results = {}
        self.errors = {}
        
    def save_raw_data(self, site_name: str, data: Any) -> str:
        """
        ä¿å­˜åŸå§‹çˆ¬èŸ²è³‡æ–™
        
        Args:
            site_name: ç¶²ç«™åç¨±
            data: åŸå§‹è³‡æ–™
            
        Returns:
            str: ä¿å­˜çš„æª”æ¡ˆè·¯å¾‘
        """
        raw_file = self.raw_data_dir / f"{site_name}_raw_data.json"
        
        # å°‡ DataFrame è½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        if isinstance(data, pd.DataFrame):
            raw_data = {
                "metadata": {
                    "site": site_name,
                    "timestamp": self.timestamp,
                    "total_records": len(data),
                    "columns": list(data.columns)
                },
                "data": data.to_dict(orient="records")
            }
        else:
            raw_data = {
                "metadata": {
                    "site": site_name,
                    "timestamp": self.timestamp,
                    "data_type": str(type(data))
                },
                "data": str(data)
            }
        
        with open(raw_file, 'w', encoding='utf-8') as f:
            json.dump(raw_data, f, ensure_ascii=False, indent=2, default=str)
        
        return str(raw_file)
    
    def save_csv_data(self, site_name: str, df: pd.DataFrame) -> str:
        """
        ä¿å­˜æ•´ç†å¾Œçš„CSVè³‡æ–™
        
        Args:
            site_name: ç¶²ç«™åç¨±
            df: DataFrame è³‡æ–™
            
        Returns:
            str: ä¿å­˜çš„æª”æ¡ˆè·¯å¾‘
        """
        csv_file = self.csv_data_dir / f"{site_name}_jobs.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        return str(csv_file)
    
    def scrape_single_site(self, site: Site) -> Dict[str, Any]:
        """
        çˆ¬å–å–®å€‹ç¶²ç«™çš„è³‡æ–™
        
        Args:
            site: ç¶²ç«™æšèˆ‰
            
        Returns:
            Dict: åŒ…å«çµæœå’ŒéŒ¯èª¤è³‡è¨Šçš„å­—å…¸
        """
        site_name = site.value
        print(f"\nğŸ” é–‹å§‹çˆ¬å– {site_name} çš„æ¾³æ´² AI å·¥ç¨‹å¸«è·ä½...")
        
        try:
            # æ ¹æ“šä¸åŒç¶²ç«™èª¿æ•´åƒæ•¸
            params = self.search_params.copy()
            
            if site == Site.SEEK:
                # Seek æ˜¯æ¾³æ´²æœ¬åœ°ç¶²ç«™ï¼Œèª¿æ•´æœå°‹åƒæ•¸
                params["location"] = "Sydney, Australia"
            elif site == Site.LINKEDIN:
                # LinkedIn å¯èƒ½éœ€è¦æ›´å…·é«”çš„ä½ç½®
                params["location"] = "Sydney, NSW, Australia"
            elif site == Site.INDEED:
                # Indeed ä½¿ç”¨åœ‹å®¶åƒæ•¸
                params["location"] = "Sydney NSW"
            
            start_time = time.time()
            
            # åŸ·è¡Œçˆ¬èŸ² - ä½¿ç”¨æ­£ç¢ºçš„ scrape_jobs å‡½æ•¸
            result = scrape_jobs(
                site_name=site.value,  # ä½¿ç”¨å­—ä¸²å€¼è€Œéæšèˆ‰
                search_term=params["search_term"],
                location=params["location"],
                country_indeed=params["country_indeed"],
                results_wanted=params["results_wanted"],
                job_type=params["job_type"],
                is_remote=params["is_remote"],
                linkedin_fetch_description=params.get("linkedin_fetch_description", True),
                linkedin_company_ids=params.get("linkedin_company_ids"),
                offset=params.get("offset", 0),
                hours_old=params.get("hours_old", 72)
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            if isinstance(result, pd.DataFrame) and not result.empty:
                # ä¿å­˜åŸå§‹è³‡æ–™
                raw_file = self.save_raw_data(site_name, result)
                
                # ä¿å­˜CSVè³‡æ–™
                csv_file = self.save_csv_data(site_name, result)
                
                print(f"âœ… {site_name} çˆ¬å–æˆåŠŸï¼š{len(result)} ç­†è³‡æ–™")
                print(f"   åŸå§‹è³‡æ–™ï¼š{raw_file}")
                print(f"   CSVæª”æ¡ˆï¼š{csv_file}")
                print(f"   è€—æ™‚ï¼š{duration:.2f} ç§’")
                
                return {
                    "success": True,
                    "site": site_name,
                    "records_count": len(result),
                    "duration": duration,
                    "raw_file": raw_file,
                    "csv_file": csv_file,
                    "data": result
                }
            else:
                print(f"âš ï¸ {site_name} æ²’æœ‰æ‰¾åˆ°è³‡æ–™")
                return {
                    "success": False,
                    "site": site_name,
                    "error": "No data found",
                    "duration": duration
                }
                
        except Exception as e:
            print(f"âŒ {site_name} çˆ¬å–å¤±æ•—ï¼š{str(e)}")
            return {
                "success": False,
                "site": site_name,
                "error": str(e),
                "duration": 0
            }
    
    def run_all_sites(self) -> Dict[str, Any]:
        """
        åŸ·è¡Œæ‰€æœ‰ç¶²ç«™çš„çˆ¬èŸ²
        
        Returns:
            Dict: å®Œæ•´çš„æ¸¬è©¦çµæœ
        """
        print("ğŸš€ é–‹å§‹åŸ·è¡Œæ¾³æ´² AI å·¥ç¨‹å¸«è·ä½çˆ¬èŸ²æ¸¬è©¦")
        print(f"ğŸ“ æœå°‹æ¢ä»¶ï¼š{self.search_params['search_term']} in {self.search_params['location']}")
        print(f"ğŸ“Š ç›®æ¨™ï¼šæ¯å€‹ç¶²ç«™è‡³å°‘ {self.search_params['results_wanted']} ç­†è³‡æ–™")
        print(f"ğŸŒ æ¸¬è©¦ç¶²ç«™ï¼š{[site.value for site in self.sites]}")
        print("="*80)
        
        total_start_time = time.time()
        
        for site in self.sites:
            result = self.scrape_single_site(site)
            
            if result["success"]:
                self.results[site.value] = result
            else:
                self.errors[site.value] = result
            
            # åœ¨ç¶²ç«™ä¹‹é–“ç¨ä½œåœé “ï¼Œé¿å…è¢«å°é–
            time.sleep(2)
        
        total_end_time = time.time()
        total_duration = total_end_time - total_start_time
        
        # ç”Ÿæˆç¸½çµå ±å‘Š
        summary = self.generate_summary(total_duration)
        
        return summary
    
    def generate_summary(self, total_duration: float) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ¸¬è©¦ç¸½çµå ±å‘Š
        
        Args:
            total_duration: ç¸½åŸ·è¡Œæ™‚é–“
            
        Returns:
            Dict: ç¸½çµå ±å‘Š
        """
        print("\n" + "="*80)
        print("ğŸ“Š æ¸¬è©¦ç¸½çµå ±å‘Š")
        print("="*80)
        
        successful_sites = len(self.results)
        failed_sites = len(self.errors)
        total_sites = successful_sites + failed_sites
        total_records = sum(result["records_count"] for result in self.results.values())
        
        print(f"\nâœ… æˆåŠŸç¶²ç«™ï¼š{successful_sites}/{total_sites}")
        print(f"ğŸ“ˆ ç¸½è¨ˆè³‡æ–™ï¼š{total_records} ç­†")
        print(f"â±ï¸ ç¸½åŸ·è¡Œæ™‚é–“ï¼š{total_duration:.2f} ç§’")
        
        # æˆåŠŸçš„ç¶²ç«™è©³æƒ…
        if self.results:
            print("\nğŸ¯ æˆåŠŸçˆ¬å–çš„ç¶²ç«™ï¼š")
            for site, result in self.results.items():
                print(f"  â€¢ {site}: {result['records_count']} ç­†è³‡æ–™ ({result['duration']:.2f}ç§’)")
        
        # å¤±æ•—çš„ç¶²ç«™è©³æƒ…
        if self.errors:
            print("\nâŒ å¤±æ•—çš„ç¶²ç«™ï¼š")
            for site, error in self.errors.items():
                print(f"  â€¢ {site}: {error['error']}")
        
        # æª”æ¡ˆä½ç½®è³‡è¨Š
        print(f"\nğŸ“ çµæœæª”æ¡ˆä½ç½®ï¼š")
        print(f"  ğŸ“‚ ä¸»ç›®éŒ„ï¼š{self.output_dir}")
        print(f"  ğŸ“„ åŸå§‹è³‡æ–™ï¼š{self.raw_data_dir}")
        print(f"  ğŸ“Š CSVæª”æ¡ˆï¼š{self.csv_data_dir}")
        print(f"  ğŸ“‹ ç¸½çµå ±å‘Šï¼š{self.summary_dir}")
        
        # å‰µå»ºç¸½çµå ±å‘Š
        summary_data = {
            "test_info": {
                "timestamp": self.timestamp,
                "search_term": self.search_params["search_term"],
                "location": self.search_params["location"],
                "target_results_per_site": self.search_params["results_wanted"]
            },
            "results": {
                "successful_sites": successful_sites,
                "failed_sites": failed_sites,
                "total_sites": total_sites,
                "total_records": total_records,
                "total_duration": total_duration
            },
            "site_details": {
                "successful": self.results,
                "failed": self.errors
            },
            "file_locations": {
                "output_directory": str(self.output_dir),
                "raw_data_directory": str(self.raw_data_dir),
                "csv_data_directory": str(self.csv_data_dir),
                "summary_directory": str(self.summary_dir)
            }
        }
        
        # ä¿å­˜ç¸½çµå ±å‘Š
        summary_file = self.summary_dir / "test_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2, default=str)
        
        # å‰µå»ºåˆä½µçš„CSVæª”æ¡ˆ
        if self.results:
            self.create_combined_csv()
        
        print(f"\nğŸ“‹ è©³ç´°å ±å‘Šå·²ä¿å­˜ï¼š{summary_file}")
        
        return summary_data
    
    def create_combined_csv(self) -> str:
        """
        å‰µå»ºåˆä½µæ‰€æœ‰ç¶²ç«™è³‡æ–™çš„CSVæª”æ¡ˆ
        
        Returns:
            str: åˆä½µæª”æ¡ˆçš„è·¯å¾‘
        """
        combined_data = []
        
        for site, result in self.results.items():
            df = result["data"].copy()
            df["source_site"] = site
            df["scrape_timestamp"] = self.timestamp
            combined_data.append(df)
        
        if combined_data:
            combined_df = pd.concat(combined_data, ignore_index=True)
            combined_file = self.csv_data_dir / "combined_all_sites.csv"
            combined_df.to_csv(combined_file, index=False, encoding='utf-8-sig')
            
            print(f"ğŸ“Š åˆä½µæª”æ¡ˆå·²å‰µå»ºï¼š{combined_file}")
            print(f"   ç¸½è¨ˆ {len(combined_df)} ç­†è³‡æ–™ä¾†è‡ª {len(self.results)} å€‹ç¶²ç«™")
            
            return str(combined_file)
        
        return ""

def main():
    """
    ä¸»è¦åŸ·è¡Œå‡½æ•¸
    """
    print("ğŸ¯ JobSpy æ¾³æ´² AI å·¥ç¨‹å¸«è·ä½çˆ¬èŸ²æ¸¬è©¦")
    print("="*50)
    
    try:
        tester = AustraliaAIEngineerTester()
        summary = tester.run_all_sites()
        
        print("\nğŸ‰ æ¸¬è©¦å®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰çµæœå·²ä¿å­˜åˆ°ï¼š{tester.output_dir}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—ï¼š{e}")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)