# LLMç”ŸæˆJSONæª”æ¡ˆè™•ç†ç³»çµ±æ¶æ§‹è¨­è¨ˆ

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æª”è©³ç´°èªªæ˜LLMç”Ÿæˆçš„JSONæª”æ¡ˆåœ¨JobSpyç³»çµ±ä¸­çš„å­˜å„²è·¯å¾‘ã€èª¿åº¦å™¨/æ™ºèƒ½è·¯ç”±å™¨çš„è™•ç†æ©Ÿåˆ¶ï¼Œä»¥åŠç³»çµ±æ¶æ§‹çš„æœ€ä½³å¯¦è¸æ–¹æ¡ˆã€‚

## ğŸ—‚ï¸ JSONæª”æ¡ˆå­˜å„²è·¯å¾‘æ¶æ§‹

### 1. ä¸»è¦å­˜å„²ç›®éŒ„çµæ§‹

```
JobSpy/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ llm_generated/           # LLMç”Ÿæˆçš„JSONæª”æ¡ˆä¸»ç›®éŒ„
â”‚   â”‚   â”œâ”€â”€ raw/                 # åŸå§‹LLMè¼¸å‡º
â”‚   â”‚   â”‚   â”œâ”€â”€ by_date/         # æŒ‰æ—¥æœŸåˆ†é¡
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2025-01-27/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2025-01-28/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”‚   â”œâ”€â”€ by_provider/     # æŒ‰LLMæä¾›å•†åˆ†é¡
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ openai/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ anthropic/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ google/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ deepseek/
â”‚   â”‚   â”‚   â””â”€â”€ by_task_type/    # æŒ‰ä»»å‹™é¡å‹åˆ†é¡
â”‚   â”‚   â”‚       â”œâ”€â”€ intent_analysis/
â”‚   â”‚   â”‚       â”œâ”€â”€ job_extraction/
â”‚   â”‚   â”‚       â”œâ”€â”€ data_enrichment/
â”‚   â”‚   â”‚       â””â”€â”€ quality_validation/
â”‚   â”‚   â”œâ”€â”€ processed/           # è™•ç†å¾Œçš„JSONæª”æ¡ˆ
â”‚   â”‚   â”‚   â”œâ”€â”€ validated/       # å·²é©—è­‰çš„æª”æ¡ˆ
â”‚   â”‚   â”‚   â”œâ”€â”€ enriched/        # å·²è±å¯Œçš„æª”æ¡ˆ
â”‚   â”‚   â”‚   â””â”€â”€ standardized/    # å·²æ¨™æº–åŒ–çš„æª”æ¡ˆ
â”‚   â”‚   â”œâ”€â”€ queue/               # å¾…è™•ç†ä½‡åˆ—
â”‚   â”‚   â”‚   â”œâ”€â”€ pending/         # å¾…è™•ç†
â”‚   â”‚   â”‚   â”œâ”€â”€ processing/      # è™•ç†ä¸­
â”‚   â”‚   â”‚   â”œâ”€â”€ completed/       # å·²å®Œæˆ
â”‚   â”‚   â”‚   â””â”€â”€ failed/          # è™•ç†å¤±æ•—
â”‚   â”‚   â””â”€â”€ archive/             # æ­¸æª”æª”æ¡ˆ
â”‚   â”‚       â”œâ”€â”€ daily/
â”‚   â”‚       â”œâ”€â”€ weekly/
â”‚   â”‚       â””â”€â”€ monthly/
â”‚   â”œâ”€â”€ job_data/                # æœ€çµ‚è·ä½æ•¸æ“š
â”‚   â”‚   â”œâ”€â”€ raw/                 # åŸå§‹çˆ¬èŸ²æ•¸æ“š
â”‚   â”‚   â”œâ”€â”€ processed/           # ETLè™•ç†å¾Œæ•¸æ“š
â”‚   â”‚   â””â”€â”€ exports/             # å°å‡ºæ•¸æ“š
â”‚   â””â”€â”€ logs/                    # ç³»çµ±æ—¥èªŒ
â”‚       â”œâ”€â”€ llm_processing/
â”‚       â”œâ”€â”€ scheduler/
â”‚       â””â”€â”€ router/
â””â”€â”€ config/
    â”œâ”€â”€ llm_config.json          # LLMé…ç½®
    â”œâ”€â”€ scheduler_config.json    # èª¿åº¦å™¨é…ç½®
    â””â”€â”€ storage_config.json     # å­˜å„²é…ç½®
```

### 2. æª”æ¡ˆå‘½åè¦ç¯„

```python
# LLMç”ŸæˆJSONæª”æ¡ˆå‘½åæ ¼å¼
{timestamp}_{task_type}_{provider}_{session_id}.json

# ç¯„ä¾‹:
20250127_143052_intent_analysis_openai_abc123.json
20250127_143055_job_extraction_anthropic_def456.json
20250127_143058_data_enrichment_google_ghi789.json
```

### 3. æª”æ¡ˆå…ƒæ•¸æ“šçµæ§‹

```json
{
  "metadata": {
    "file_id": "uuid-string",
    "created_at": "2025-01-27T14:30:52Z",
    "task_type": "intent_analysis",
    "llm_provider": "openai",
    "model_name": "gpt-4",
    "session_id": "abc123",
    "user_query": "python developer jobs in sydney",
    "processing_status": "pending",
    "file_size": 2048,
    "checksum": "sha256-hash"
  },
  "content": {
    "intent": {
      "is_job_related": true,
      "confidence": 0.95,
      "job_title": "python developer",
      "location": "sydney",
      "skills": ["python", "programming"],
      "experience_level": "mid"
    }
  }
}
```

## ğŸ¤– èª¿åº¦å™¨/æ™ºèƒ½è·¯ç”±å™¨è™•ç†æ©Ÿåˆ¶

### 1. è™•ç†æµç¨‹æ¶æ§‹

```mermaid
graph TD
    A[ç”¨æˆ¶æŸ¥è©¢] --> B[LLMæ„åœ–åˆ†æ]
    B --> C[ç”ŸæˆJSONæª”æ¡ˆ]
    C --> D[æª”æ¡ˆå­˜å„²]
    D --> E[èª¿åº¦å™¨æª¢æ¸¬]
    E --> F[æ™ºèƒ½è·¯ç”±æ±ºç­–]
    F --> G[ä»»å‹™åˆ†ç™¼]
    G --> H[ä¸¦ç™¼è™•ç†]
    H --> I[çµæœèšåˆ]
    I --> J[å“è³ªé©—è­‰]
    J --> K[æœ€çµ‚è¼¸å‡º]
```

### 2. èª¿åº¦å™¨æ ¸å¿ƒçµ„ä»¶

#### A. æª”æ¡ˆç›£æ§å™¨ (File Watcher)

```python
class LLMFileWatcher:
    """
    LLMç”Ÿæˆæª”æ¡ˆç›£æ§å™¨
    å¯¦æ™‚ç›£æ§æ–°ç”Ÿæˆçš„JSONæª”æ¡ˆ
    """
    
    def __init__(self, watch_directory: str):
        self.watch_directory = Path(watch_directory)
        self.file_queue = asyncio.Queue()
        self.processing_status = {}
    
    async def start_watching(self):
        """é–‹å§‹ç›£æ§æª”æ¡ˆè®ŠåŒ–"""
        async for event in self._watch_directory_changes():
            if event.event_type == 'created' and event.src_path.endswith('.json'):
                await self._handle_new_file(event.src_path)
    
    async def _handle_new_file(self, file_path: str):
        """è™•ç†æ–°æª”æ¡ˆ"""
        file_info = await self._extract_file_metadata(file_path)
        await self.file_queue.put(file_info)
        self.processing_status[file_path] = 'queued'
```

#### B. æ™ºèƒ½ä»»å‹™èª¿åº¦å™¨ (Smart Task Scheduler)

```python
class SmartTaskScheduler:
    """
    æ™ºèƒ½ä»»å‹™èª¿åº¦å™¨
    æ ¹æ“šä»»å‹™é¡å‹å’Œå„ªå…ˆç´šé€²è¡Œèª¿åº¦
    """
    
    def __init__(self, max_concurrent_tasks: int = 10):
        self.max_concurrent_tasks = max_concurrent_tasks
        self.task_queues = {
            'high_priority': asyncio.PriorityQueue(),
            'normal_priority': asyncio.PriorityQueue(),
            'low_priority': asyncio.PriorityQueue()
        }
        self.active_tasks = set()
        self.task_history = []
    
    async def schedule_task(self, task_info: Dict):
        """èª¿åº¦ä»»å‹™"""
        priority = self._determine_priority(task_info)
        queue_name = f"{priority}_priority"
        
        await self.task_queues[queue_name].put(
            (self._calculate_priority_score(task_info), task_info)
        )
    
    def _determine_priority(self, task_info: Dict) -> str:
        """ç¢ºå®šä»»å‹™å„ªå…ˆç´š"""
        task_type = task_info.get('task_type')
        user_tier = task_info.get('user_tier', 'free')
        
        # é«˜å„ªå…ˆç´šï¼šä»˜è²»ç”¨æˆ¶çš„æ„åœ–åˆ†æ
        if user_tier == 'premium' and task_type == 'intent_analysis':
            return 'high'
        
        # ä¸­å„ªå…ˆç´šï¼šä¸€èˆ¬ä»»å‹™
        if task_type in ['job_extraction', 'data_enrichment']:
            return 'normal'
        
        # ä½å„ªå…ˆç´šï¼šæ‰¹é‡è™•ç†ä»»å‹™
        return 'low'
```

#### C. æ™ºèƒ½è·¯ç”±æ±ºç­–å¼•æ“

```python
class IntelligentRoutingEngine:
    """
    æ™ºèƒ½è·¯ç”±æ±ºç­–å¼•æ“
    æ ¹æ“šä»»å‹™å…§å®¹é¸æ“‡æœ€é©åˆçš„è™•ç†ç­–ç•¥
    """
    
    def __init__(self):
        self.routing_rules = self._load_routing_rules()
        self.performance_metrics = {}
        self.load_balancer = LoadBalancer()
    
    async def route_task(self, task_info: Dict) -> RoutingDecision:
        """è·¯ç”±ä»»å‹™åˆ°æœ€é©åˆçš„è™•ç†å™¨"""
        
        # 1. åˆ†æä»»å‹™ç‰¹å¾µ
        task_features = self._extract_task_features(task_info)
        
        # 2. åœ°ç†ä½ç½®æª¢æ¸¬
        geographic_context = self._detect_geographic_context(task_info)
        
        # 3. é¸æ“‡è™•ç†ç­–ç•¥
        processing_strategy = self._select_processing_strategy(
            task_features, geographic_context
        )
        
        # 4. è² è¼‰å‡è¡¡
        selected_workers = self.load_balancer.select_workers(
            processing_strategy, task_info
        )
        
        return RoutingDecision(
            strategy=processing_strategy,
            workers=selected_workers,
            estimated_time=self._estimate_processing_time(task_info),
            confidence=self._calculate_confidence(task_features)
        )
    
    def _select_processing_strategy(self, features: Dict, geo_context: Dict) -> str:
        """é¸æ“‡è™•ç†ç­–ç•¥"""
        
        # æ¾³æ´²åœ°å€å„ªå…ˆä½¿ç”¨Seek
        if geo_context.get('region') == 'Australia':
            return 'seek_focused_strategy'
        
        # ç¾åœ‹åœ°å€ä½¿ç”¨Indeed + ZipRecruiter
        elif geo_context.get('region') == 'North_America':
            return 'indeed_ziprecruiter_strategy'
        
        # å…¨çƒç­–ç•¥
        else:
            return 'global_multi_platform_strategy'
```

### 3. è™•ç†æ¨¡å¼é¸æ“‡

#### A. å³æ™‚è™•ç†æ¨¡å¼ (Real-time Processing)

```python
class RealTimeProcessor:
    """
    å³æ™‚è™•ç†æ¨¡å¼
    é©ç”¨æ–¼ï¼šç”¨æˆ¶äº’å‹•æŸ¥è©¢ã€é«˜å„ªå…ˆç´šä»»å‹™
    """
    
    async def process_immediately(self, json_file_path: str):
        """ç«‹å³è™•ç†JSONæª”æ¡ˆ"""
        
        # 1. è¼‰å…¥ä¸¦é©—è­‰JSON
        task_data = await self._load_and_validate_json(json_file_path)
        
        # 2. æ™ºèƒ½è·¯ç”±æ±ºç­–
        routing_decision = await self.routing_engine.route_task(task_data)
        
        # 3. ä¸¦ç™¼åŸ·è¡Œ
        results = await self._execute_concurrent_scraping(
            routing_decision, task_data
        )
        
        # 4. å³æ™‚è¿”å›çµæœ
        return await self._format_immediate_response(results)
```

#### B. æ‰¹é‡è™•ç†æ¨¡å¼ (Batch Processing)

```python
class BatchProcessor:
    """
    æ‰¹é‡è™•ç†æ¨¡å¼
    é©ç”¨æ–¼ï¼šå¤§é‡æ•¸æ“šè™•ç†ã€éå³æ™‚éœ€æ±‚
    """
    
    def __init__(self, batch_size: int = 50, processing_interval: int = 300):
        self.batch_size = batch_size
        self.processing_interval = processing_interval  # 5åˆ†é˜
        self.pending_files = []
    
    async def add_to_batch(self, json_file_path: str):
        """æ·»åŠ åˆ°æ‰¹é‡è™•ç†ä½‡åˆ—"""
        self.pending_files.append(json_file_path)
        
        if len(self.pending_files) >= self.batch_size:
            await self._process_batch()
    
    async def _process_batch(self):
        """è™•ç†ä¸€æ‰¹æª”æ¡ˆ"""
        batch_files = self.pending_files[:self.batch_size]
        self.pending_files = self.pending_files[self.batch_size:]
        
        # ä¸¦ç™¼è™•ç†æ‰¹é‡æª”æ¡ˆ
        tasks = [self._process_single_file(file_path) for file_path in batch_files]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        await self._save_batch_results(results)
```

#### C. æ··åˆè™•ç†æ¨¡å¼ (Hybrid Processing)

```python
class HybridProcessor:
    """
    æ··åˆè™•ç†æ¨¡å¼
    çµåˆå³æ™‚å’Œæ‰¹é‡è™•ç†çš„å„ªå‹¢
    """
    
    def __init__(self):
        self.realtime_processor = RealTimeProcessor()
        self.batch_processor = BatchProcessor()
        self.priority_classifier = PriorityClassifier()
    
    async def process_json_file(self, json_file_path: str):
        """æ™ºèƒ½é¸æ“‡è™•ç†æ¨¡å¼"""
        
        # åˆ†æä»»å‹™å„ªå…ˆç´š
        priority = await self.priority_classifier.classify(json_file_path)
        
        if priority == 'high':
            # é«˜å„ªå…ˆç´šï¼šå³æ™‚è™•ç†
            return await self.realtime_processor.process_immediately(json_file_path)
        else:
            # ä½å„ªå…ˆç´šï¼šæ‰¹é‡è™•ç†
            await self.batch_processor.add_to_batch(json_file_path)
            return {'status': 'queued_for_batch_processing'}
```

## âš™ï¸ ç³»çµ±æ¶æ§‹æœ€ä½³å¯¦è¸

### 1. é«˜å¯ç”¨æ€§è¨­è¨ˆ

#### A. å®¹éŒ¯æ©Ÿåˆ¶

```python
class FaultTolerantProcessor:
    """
    å®¹éŒ¯è™•ç†å™¨
    ç¢ºä¿ç³»çµ±åœ¨éƒ¨åˆ†çµ„ä»¶å¤±æ•—æ™‚ä»èƒ½æ­£å¸¸é‹è¡Œ
    """
    
    def __init__(self):
        self.retry_config = {
            'max_retries': 3,
            'backoff_factor': 2,
            'timeout': 30
        }
        self.circuit_breaker = CircuitBreaker()
        self.health_checker = HealthChecker()
    
    async def process_with_fallback(self, task_info: Dict):
        """å¸¶å®¹éŒ¯çš„è™•ç†"""
        
        try:
            # ä¸»è¦è™•ç†è·¯å¾‘
            return await self._primary_processing(task_info)
        
        except Exception as e:
            logger.warning(f"ä¸»è¦è™•ç†å¤±æ•—: {e}ï¼Œå˜—è©¦å‚™ç”¨æ–¹æ¡ˆ")
            
            # å‚™ç”¨è™•ç†è·¯å¾‘
            return await self._fallback_processing(task_info)
    
    async def _fallback_processing(self, task_info: Dict):
        """å‚™ç”¨è™•ç†æ–¹æ¡ˆ"""
        
        # 1. ä½¿ç”¨ç·©å­˜æ•¸æ“š
        cached_result = await self._try_cache_lookup(task_info)
        if cached_result:
            return cached_result
        
        # 2. é™ç´šåˆ°åŸºæœ¬åŠŸèƒ½
        return await self._basic_processing(task_info)
```

#### B. è² è¼‰å‡è¡¡

```python
class LoadBalancer:
    """
    è² è¼‰å‡è¡¡å™¨
    æ™ºèƒ½åˆ†é…ä»»å‹™åˆ°ä¸åŒçš„è™•ç†ç¯€é»
    """
    
    def __init__(self):
        self.worker_nodes = self._discover_worker_nodes()
        self.performance_metrics = {}
        self.load_distribution = {}
    
    async def select_optimal_workers(self, task_info: Dict) -> List[str]:
        """é¸æ“‡æœ€å„ªçš„å·¥ä½œç¯€é»"""
        
        # 1. æª¢æŸ¥ç¯€é»å¥åº·ç‹€æ…‹
        healthy_nodes = await self._filter_healthy_nodes()
        
        # 2. è¨ˆç®—ç¯€é»è² è¼‰
        node_loads = await self._calculate_node_loads(healthy_nodes)
        
        # 3. é¸æ“‡æœ€å„ªç¯€é»
        optimal_nodes = self._select_by_algorithm(
            healthy_nodes, node_loads, task_info
        )
        
        return optimal_nodes
    
    def _select_by_algorithm(self, nodes: List, loads: Dict, task_info: Dict) -> List[str]:
        """ä½¿ç”¨æ™ºèƒ½ç®—æ³•é¸æ“‡ç¯€é»"""
        
        # åŠ æ¬Šè¼ªè©¢ç®—æ³•
        if task_info.get('priority') == 'high':
            return self._weighted_round_robin(nodes, loads)
        
        # æœ€å°‘é€£æ¥ç®—æ³•
        elif task_info.get('task_type') == 'batch':
            return self._least_connections(nodes, loads)
        
        # éŸ¿æ‡‰æ™‚é–“ç®—æ³•
        else:
            return self._fastest_response(nodes, loads)
```

### 2. æ€§èƒ½å„ªåŒ–ç­–ç•¥

#### A. ç·©å­˜æ©Ÿåˆ¶

```python
class IntelligentCache:
    """
    æ™ºèƒ½ç·©å­˜ç³»çµ±
    æ¸›å°‘é‡è¤‡è™•ç†ï¼Œæå‡éŸ¿æ‡‰é€Ÿåº¦
    """
    
    def __init__(self):
        self.memory_cache = {}
        self.redis_cache = redis.Redis()
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0
        }
    
    async def get_cached_result(self, query_hash: str) -> Optional[Dict]:
        """ç²å–ç·©å­˜çµæœ"""
        
        # 1. æª¢æŸ¥å…§å­˜ç·©å­˜
        if query_hash in self.memory_cache:
            self.cache_stats['hits'] += 1
            return self.memory_cache[query_hash]
        
        # 2. æª¢æŸ¥Redisç·©å­˜
        redis_result = await self.redis_cache.get(f"job_search:{query_hash}")
        if redis_result:
            result = json.loads(redis_result)
            self.memory_cache[query_hash] = result  # å›å¡«å…§å­˜ç·©å­˜
            self.cache_stats['hits'] += 1
            return result
        
        self.cache_stats['misses'] += 1
        return None
    
    async def cache_result(self, query_hash: str, result: Dict, ttl: int = 3600):
        """ç·©å­˜çµæœ"""
        
        # å…§å­˜ç·©å­˜
        self.memory_cache[query_hash] = result
        
        # Redisç·©å­˜ï¼ˆå¸¶éæœŸæ™‚é–“ï¼‰
        await self.redis_cache.setex(
            f"job_search:{query_hash}", 
            ttl, 
            json.dumps(result)
        )
```

#### B. ä¸¦ç™¼æ§åˆ¶

```python
class ConcurrencyController:
    """
    ä¸¦ç™¼æ§åˆ¶å™¨
    ç®¡ç†ç³»çµ±ä¸¦ç™¼åº¦ï¼Œé˜²æ­¢è³‡æºéè¼‰
    """
    
    def __init__(self, max_concurrent: int = 20):
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.rate_limiter = RateLimiter()
        self.resource_monitor = ResourceMonitor()
    
    async def execute_with_control(self, coro, task_info: Dict):
        """å¸¶ä¸¦ç™¼æ§åˆ¶çš„åŸ·è¡Œ"""
        
        async with self.semaphore:
            # æª¢æŸ¥ç³»çµ±è³‡æº
            if not await self.resource_monitor.check_resources():
                await asyncio.sleep(1)  # ç­‰å¾…è³‡æºé‡‹æ”¾
            
            # é€Ÿç‡é™åˆ¶
            await self.rate_limiter.acquire(task_info.get('user_id'))
            
            # åŸ·è¡Œä»»å‹™
            return await coro
```

### 3. ç›£æ§å’Œæ—¥èªŒ

#### A. æ€§èƒ½ç›£æ§

```python
class PerformanceMonitor:
    """
    æ€§èƒ½ç›£æ§ç³»çµ±
    å¯¦æ™‚ç›£æ§ç³»çµ±æ€§èƒ½æŒ‡æ¨™
    """
    
    def __init__(self):
        self.metrics = {
            'request_count': 0,
            'response_times': [],
            'error_count': 0,
            'cache_hit_rate': 0.0,
            'worker_utilization': {}
        }
        self.alerts = AlertManager()
    
    async def record_request(self, start_time: float, end_time: float, success: bool):
        """è¨˜éŒ„è«‹æ±‚æŒ‡æ¨™"""
        
        self.metrics['request_count'] += 1
        response_time = end_time - start_time
        self.metrics['response_times'].append(response_time)
        
        if not success:
            self.metrics['error_count'] += 1
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦å‘Šè­¦
        await self._check_alerts(response_time, success)
    
    async def _check_alerts(self, response_time: float, success: bool):
        """æª¢æŸ¥å‘Šè­¦æ¢ä»¶"""
        
        # éŸ¿æ‡‰æ™‚é–“å‘Šè­¦
        if response_time > 10.0:  # 10ç§’
            await self.alerts.send_alert(
                'high_response_time', 
                f'éŸ¿æ‡‰æ™‚é–“éé•·: {response_time:.2f}ç§’'
            )
        
        # éŒ¯èª¤ç‡å‘Šè­¦
        error_rate = self.metrics['error_count'] / self.metrics['request_count']
        if error_rate > 0.05:  # 5%
            await self.alerts.send_alert(
                'high_error_rate',
                f'éŒ¯èª¤ç‡éé«˜: {error_rate:.2%}'
            )
```

#### B. çµæ§‹åŒ–æ—¥èªŒ

```python
class StructuredLogger:
    """
    çµæ§‹åŒ–æ—¥èªŒç³»çµ±
    ä¾¿æ–¼æ—¥èªŒåˆ†æå’Œå•é¡Œæ’æŸ¥
    """
    
    def __init__(self):
        self.logger = logging.getLogger('jobspy')
        self.setup_handlers()
    
    def log_task_processing(self, task_info: Dict, status: str, **kwargs):
        """è¨˜éŒ„ä»»å‹™è™•ç†æ—¥èªŒ"""
        
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'task_id': task_info.get('task_id'),
            'task_type': task_info.get('task_type'),
            'user_id': task_info.get('user_id'),
            'status': status,
            'processing_time': kwargs.get('processing_time'),
            'worker_node': kwargs.get('worker_node'),
            'error_message': kwargs.get('error_message')
        }
        
        self.logger.info(json.dumps(log_entry))
```

## ğŸš€ éƒ¨ç½²å’Œé‹ç¶­å»ºè­°

### 1. å®¹å™¨åŒ–éƒ¨ç½²

```dockerfile
# Dockerfile for LLM JSON Processor
FROM python:3.11-slim

WORKDIR /app

# å®‰è£ä¾è³´
COPY requirements.txt .
RUN pip install -r requirements.txt

# è¤‡è£½ä»£ç¢¼
COPY . .

# è¨­ç½®ç’°å¢ƒè®Šé‡
ENV PYTHONPATH=/app
ENV LOG_LEVEL=INFO

# å¥åº·æª¢æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python health_check.py

# å•Ÿå‹•æœå‹™
CMD ["python", "main.py"]
```

### 2. Kubernetesé…ç½®

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-json-processor
spec:
  replicas: 3
  selector:
    matchLabels:
      app: llm-json-processor
  template:
    metadata:
      labels:
        app: llm-json-processor
    spec:
      containers:
      - name: processor
        image: jobspy/llm-json-processor:latest
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        env:
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        - name: MAX_CONCURRENT_TASKS
          value: "10"
---
apiVersion: v1
kind: Service
metadata:
  name: llm-json-processor-service
spec:
  selector:
    app: llm-json-processor
  ports:
  - port: 8080
    targetPort: 8080
```

### 3. ç›£æ§é…ç½®

```yaml
# prometheus-config.yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'llm-json-processor'
    static_configs:
      - targets: ['llm-json-processor-service:8080']
    metrics_path: '/metrics'
    scrape_interval: 10s
```

## ğŸ“Š æ€§èƒ½åŸºæº–å’Œæ“´å±•å»ºè­°

### 1. æ€§èƒ½ç›®æ¨™

| æŒ‡æ¨™ | ç›®æ¨™å€¼ | ç›£æ§æ–¹å¼ |
|------|--------|----------|
| å¹³å‡éŸ¿æ‡‰æ™‚é–“ | < 2ç§’ | Prometheus + Grafana |
| 99%éŸ¿æ‡‰æ™‚é–“ | < 5ç§’ | æ‡‰ç”¨æ—¥èªŒåˆ†æ |
| ç³»çµ±å¯ç”¨æ€§ | > 99.9% | å¥åº·æª¢æŸ¥ + å‘Šè­¦ |
| éŒ¯èª¤ç‡ | < 1% | éŒ¯èª¤æ—¥èªŒçµ±è¨ˆ |
| ä¸¦ç™¼è™•ç†èƒ½åŠ› | 100 req/s | è² è¼‰æ¸¬è©¦ |

### 2. æ“´å±•ç­–ç•¥

#### æ°´å¹³æ“´å±•
- å¢åŠ è™•ç†ç¯€é»æ•¸é‡
- ä½¿ç”¨Kubernetes HPAè‡ªå‹•æ“´å±•
- å¯¦æ–½å¾®æœå‹™æ¶æ§‹

#### å‚ç›´æ“´å±•
- å¢åŠ å–®ç¯€é»è³‡æºé…ç½®
- å„ªåŒ–ç®—æ³•å’Œæ•¸æ“šçµæ§‹
- ä½¿ç”¨æ›´é«˜æ€§èƒ½çš„ç¡¬ä»¶

### 3. æˆæœ¬å„ªåŒ–

```python
class CostOptimizer:
    """
    æˆæœ¬å„ªåŒ–å™¨
    æ™ºèƒ½ç®¡ç†è³‡æºä½¿ç”¨ï¼Œé™ä½é‹ç‡Ÿæˆæœ¬
    """
    
    def __init__(self):
        self.usage_patterns = {}
        self.cost_thresholds = {
            'cpu_utilization': 0.7,
            'memory_utilization': 0.8,
            'storage_utilization': 0.9
        }
    
    async def optimize_resources(self):
        """å„ªåŒ–è³‡æºé…ç½®"""
        
        # åˆ†æä½¿ç”¨æ¨¡å¼
        patterns = await self._analyze_usage_patterns()
        
        # é æ¸¬è³‡æºéœ€æ±‚
        predicted_demand = await self._predict_demand(patterns)
        
        # èª¿æ•´è³‡æºé…ç½®
        await self._adjust_resources(predicted_demand)
    
    async def _adjust_resources(self, demand: Dict):
        """èª¿æ•´è³‡æºé…ç½®"""
        
        # ä½å³°æœŸç¸®æ¸›è³‡æº
        if demand['level'] == 'low':
            await self._scale_down_resources()
        
        # é«˜å³°æœŸæ“´å±•è³‡æº
        elif demand['level'] == 'high':
            await self._scale_up_resources()
```

## ğŸ”’ å®‰å…¨æ€§è€ƒæ…®

### 1. æ•¸æ“šå®‰å…¨

```python
class SecurityManager:
    """
    å®‰å…¨ç®¡ç†å™¨
    ç¢ºä¿æ•¸æ“šå’Œç³»çµ±å®‰å…¨
    """
    
    def __init__(self):
        self.encryption_key = self._load_encryption_key()
        self.access_control = AccessController()
        self.audit_logger = AuditLogger()
    
    async def secure_file_processing(self, file_path: str, user_context: Dict):
        """å®‰å…¨çš„æª”æ¡ˆè™•ç†"""
        
        # 1. é©—è­‰ç”¨æˆ¶æ¬Šé™
        if not await self.access_control.check_permission(user_context, 'file_read'):
            raise PermissionError("ç”¨æˆ¶ç„¡æ¬Šé™è¨ªå•æ­¤æª”æ¡ˆ")
        
        # 2. æª”æ¡ˆå®Œæ•´æ€§æª¢æŸ¥
        if not await self._verify_file_integrity(file_path):
            raise SecurityError("æª”æ¡ˆå®Œæ•´æ€§é©—è­‰å¤±æ•—")
        
        # 3. æ•æ„Ÿæ•¸æ“šæª¢æ¸¬
        sensitive_data = await self._detect_sensitive_data(file_path)
        if sensitive_data:
            await self._handle_sensitive_data(sensitive_data)
        
        # 4. è¨˜éŒ„å¯©è¨ˆæ—¥èªŒ
        await self.audit_logger.log_file_access(user_context, file_path)
```

### 2. APIå®‰å…¨

```python
class APISecurityMiddleware:
    """
    APIå®‰å…¨ä¸­é–“ä»¶
    ä¿è­·APIç«¯é»å®‰å…¨
    """
    
    async def __call__(self, request, call_next):
        # 1. é€Ÿç‡é™åˆ¶
        await self._check_rate_limit(request)
        
        # 2. èº«ä»½é©—è­‰
        user = await self._authenticate_user(request)
        
        # 3. æˆæ¬Šæª¢æŸ¥
        await self._authorize_request(user, request)
        
        # 4. è¼¸å…¥é©—è­‰
        await self._validate_input(request)
        
        # 5. è™•ç†è«‹æ±‚
        response = await call_next(request)
        
        # 6. è¼¸å‡ºéæ¿¾
        return await self._filter_response(response, user)
```

## ğŸ“ˆ æœªä¾†ç™¼å±•è¦åŠƒ

### 1. AIå¢å¼·åŠŸèƒ½
- æ™ºèƒ½ä»»å‹™å„ªå…ˆç´šé æ¸¬
- è‡ªå‹•åŒ–æ€§èƒ½èª¿å„ª
- ç•°å¸¸æª¢æ¸¬å’Œè‡ªå‹•æ¢å¾©

### 2. å¤šé›²éƒ¨ç½²
- è·¨é›²å¹³å°è² è¼‰å‡è¡¡
- ç½é›£æ¢å¾©æ©Ÿåˆ¶
- æˆæœ¬å„ªåŒ–ç­–ç•¥

### 3. å¯¦æ™‚åˆ†æ
- æµå¼æ•¸æ“šè™•ç†
- å¯¦æ™‚å„€è¡¨æ¿
- é æ¸¬æ€§ç¶­è­·

---

## ğŸ“ è¯ç¹«ä¿¡æ¯

å¦‚æœ‰ä»»ä½•å•é¡Œæˆ–å»ºè­°ï¼Œè«‹è¯ç¹«ï¼š
- æŠ€è¡“åœ˜éšŠï¼štech@jobspy.com
- æ–‡æª”ç¶­è­·ï¼šdocs@jobspy.com
- GitHub Issuesï¼šhttps://github.com/jobspy/jobspy/issues

---

*æœ¬æ–‡æª”æœ€å¾Œæ›´æ–°ï¼š2025-01-27*
*ç‰ˆæœ¬ï¼šv1.0*