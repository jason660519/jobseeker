#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªå‹•åŒ–LLMæ¸¬è©¦åŸ·è¡Œå™¨
ç”¨æ–¼æ‰¹é‡é‹è¡Œä¸åŒçš„LLMæ¸¬è©¦æ¡ˆä¾‹ï¼Œæ”¶é›†çµæœä¸¦ç”Ÿæˆå°æ¯”å ±å‘Š

Author: JobSpy Team
Date: 2025-01-27
"""

import sys
import os
import json
import time
import asyncio
import subprocess
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Callable
from dataclasses import dataclass, asdict, field
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import logging
from enum import Enum
import psutil
import signal
from contextlib import contextmanager

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from jobseeker.llm_intent_analyzer import LLMProvider
from llm_config import LLMConfig

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('automated_test_runner.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class TestType(Enum):
    """æ¸¬è©¦é¡å‹æšèˆ‰"""
    COMPREHENSIVE = "comprehensive"  # å…¨é¢æ¸¬è©¦
    BENCHMARK = "benchmark"  # åŸºæº–æ¸¬è©¦
    STRESS = "stress"  # å£“åŠ›æ¸¬è©¦
    PERFORMANCE = "performance"  # æ€§èƒ½æ¸¬è©¦
    COMPARISON = "comparison"  # å°æ¯”æ¸¬è©¦
    CUSTOM = "custom"  # è‡ªå®šç¾©æ¸¬è©¦


class TestStatus(Enum):
    """æ¸¬è©¦ç‹€æ…‹æšèˆ‰"""
    PENDING = "pending"  # ç­‰å¾…ä¸­
    RUNNING = "running"  # åŸ·è¡Œä¸­
    COMPLETED = "completed"  # å·²å®Œæˆ
    FAILED = "failed"  # å¤±æ•—
    TIMEOUT = "timeout"  # è¶…æ™‚
    CANCELLED = "cancelled"  # å·²å–æ¶ˆ


@dataclass
class TestTask:
    """æ¸¬è©¦ä»»å‹™"""
    task_id: str
    test_type: TestType
    test_script: str
    providers: List[str]
    config: Dict[str, Any]
    priority: int = 1  # 1-5, 5æœ€é«˜
    timeout: int = 3600  # è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
    retry_count: int = 0  # é‡è©¦æ¬¡æ•¸
    max_retries: int = 2  # æœ€å¤§é‡è©¦æ¬¡æ•¸
    dependencies: List[str] = field(default_factory=list)  # ä¾è³´çš„ä»»å‹™ID
    status: TestStatus = TestStatus.PENDING
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    result_file: Optional[str] = None
    error_message: Optional[str] = None
    resource_usage: Dict[str, float] = field(default_factory=dict)


@dataclass
class TestSuite:
    """æ¸¬è©¦å¥—ä»¶"""
    suite_id: str
    name: str
    description: str
    tasks: List[TestTask]
    parallel_limit: int = 3  # ä¸¦è¡ŒåŸ·è¡Œé™åˆ¶
    total_timeout: int = 7200  # ç¸½è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
    created_time: datetime = field(default_factory=datetime.now)
    status: TestStatus = TestStatus.PENDING


@dataclass
class TestResult:
    """æ¸¬è©¦çµæœ"""
    task_id: str
    test_type: TestType
    provider: str
    success: bool
    execution_time: float
    result_data: Dict[str, Any]
    error_details: Optional[str] = None
    resource_usage: Dict[str, float] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)


class ResourceMonitor:
    """è³‡æºç›£æ§å™¨"""
    
    def __init__(self):
        self.monitoring = False
        self.data = []
        self.monitor_thread = None
    
    def start_monitoring(self, interval: float = 1.0):
        """é–‹å§‹ç›£æ§"""
        if self.monitoring:
            return
        
        self.monitoring = True
        self.data = []
        self.monitor_thread = threading.Thread(
            target=self._monitor_loop,
            args=(interval,),
            daemon=True
        )
        self.monitor_thread.start()
        logger.info("è³‡æºç›£æ§å·²å•Ÿå‹•")
    
    def stop_monitoring(self) -> Dict[str, float]:
        """åœæ­¢ç›£æ§ä¸¦è¿”å›çµ±è¨ˆæ•¸æ“š"""
        if not self.monitoring:
            return {}
        
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=2.0)
        
        if not self.data:
            return {}
        
        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        cpu_values = [d['cpu'] for d in self.data]
        memory_values = [d['memory'] for d in self.data]
        
        stats = {
            'avg_cpu_percent': sum(cpu_values) / len(cpu_values),
            'max_cpu_percent': max(cpu_values),
            'avg_memory_mb': sum(memory_values) / len(memory_values),
            'max_memory_mb': max(memory_values),
            'sample_count': len(self.data)
        }
        
        logger.info(f"è³‡æºç›£æ§å·²åœæ­¢ï¼Œæ”¶é›†äº† {len(self.data)} å€‹æ¨£æœ¬")
        return stats
    
    def _monitor_loop(self, interval: float):
        """ç›£æ§å¾ªç’°"""
        process = psutil.Process()
        
        while self.monitoring:
            try:
                cpu_percent = process.cpu_percent()
                memory_info = process.memory_info()
                memory_mb = memory_info.rss / 1024 / 1024
                
                self.data.append({
                    'timestamp': time.time(),
                    'cpu': cpu_percent,
                    'memory': memory_mb
                })
                
                time.sleep(interval)
            except Exception as e:
                logger.warning(f"è³‡æºç›£æ§éŒ¯èª¤: {e}")
                break


class AutomatedLLMTestRunner:
    """è‡ªå‹•åŒ–LLMæ¸¬è©¦åŸ·è¡Œå™¨"""
    
    def __init__(self, output_dir: str = "test_results", max_workers: int = 3):
        """åˆå§‹åŒ–æ¸¬è©¦åŸ·è¡Œå™¨"""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # å‰µå»ºå­ç›®éŒ„
        (self.output_dir / "logs").mkdir(exist_ok=True)
        (self.output_dir / "results").mkdir(exist_ok=True)
        (self.output_dir / "reports").mkdir(exist_ok=True)
        
        self.max_workers = max_workers
        self.test_suites: Dict[str, TestSuite] = {}
        self.running_tasks: Dict[str, subprocess.Popen] = {}
        self.completed_results: List[TestResult] = []
        
        # è³‡æºç›£æ§
        self.resource_monitor = ResourceMonitor()
        
        # åŸ·è¡Œå™¨
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        
        # å¯ç”¨çš„æ¸¬è©¦è…³æœ¬
        self.available_scripts = {
            TestType.COMPREHENSIVE: "comprehensive_llm_test.py",
            TestType.BENCHMARK: "llm_benchmark_test_suite.py",
            TestType.STRESS: "llm_stress_test_tool.py",
            TestType.PERFORMANCE: "advanced_llm_performance_analyzer.py",
            TestType.COMPARISON: "enhanced_llm_model_comparison_test.py"
        }
        
        # ä¿¡è™Ÿè™•ç†
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def create_test_suite(self, suite_id: str, name: str, description: str, 
                         parallel_limit: int = 3) -> TestSuite:
        """å‰µå»ºæ¸¬è©¦å¥—ä»¶"""
        suite = TestSuite(
            suite_id=suite_id,
            name=name,
            description=description,
            tasks=[],
            parallel_limit=parallel_limit
        )
        
        self.test_suites[suite_id] = suite
        logger.info(f"å‰µå»ºæ¸¬è©¦å¥—ä»¶: {name} (ID: {suite_id})")
        return suite
    
    def add_test_task(self, suite_id: str, task_id: str, test_type: TestType,
                     providers: List[str], config: Dict[str, Any] = None,
                     priority: int = 1, timeout: int = 3600,
                     dependencies: List[str] = None) -> TestTask:
        """æ·»åŠ æ¸¬è©¦ä»»å‹™"""
        if suite_id not in self.test_suites:
            raise ValueError(f"æ¸¬è©¦å¥—ä»¶ {suite_id} ä¸å­˜åœ¨")
        
        if test_type not in self.available_scripts:
            raise ValueError(f"ä¸æ”¯æ´çš„æ¸¬è©¦é¡å‹: {test_type}")
        
        task = TestTask(
            task_id=task_id,
            test_type=test_type,
            test_script=self.available_scripts[test_type],
            providers=providers,
            config=config or {},
            priority=priority,
            timeout=timeout,
            dependencies=dependencies or []
        )
        
        self.test_suites[suite_id].tasks.append(task)
        logger.info(f"æ·»åŠ æ¸¬è©¦ä»»å‹™: {task_id} åˆ°å¥—ä»¶ {suite_id}")
        return task
    
    def create_comprehensive_test_suite(self, providers: List[str]) -> str:
        """å‰µå»ºå…¨é¢æ¸¬è©¦å¥—ä»¶"""
        suite_id = f"comprehensive_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        suite = self.create_test_suite(
            suite_id=suite_id,
            name="å…¨é¢LLMæ¸¬è©¦å¥—ä»¶",
            description="åŒ…å«æ‰€æœ‰é¡å‹çš„LLMæ¸¬è©¦ï¼Œç”¨æ–¼å…¨é¢è©•ä¼°æ¨¡å‹æ€§èƒ½",
            parallel_limit=2
        )
        
        # åŸºç¤åŠŸèƒ½æ¸¬è©¦
        self.add_test_task(
            suite_id=suite_id,
            task_id=f"{suite_id}_comprehensive",
            test_type=TestType.COMPREHENSIVE,
            providers=providers,
            config={"test_mode": "full", "iterations": 3},
            priority=5,
            timeout=1800
        )
        
        # åŸºæº–æ¸¬è©¦
        self.add_test_task(
            suite_id=suite_id,
            task_id=f"{suite_id}_benchmark",
            test_type=TestType.BENCHMARK,
            providers=providers,
            config={"test_mode": "standard", "categories": "all"},
            priority=4,
            timeout=2400,
            dependencies=[f"{suite_id}_comprehensive"]
        )
        
        # æ€§èƒ½åˆ†æ
        self.add_test_task(
            suite_id=suite_id,
            task_id=f"{suite_id}_performance",
            test_type=TestType.PERFORMANCE,
            providers=providers,
            config={"analysis_mode": "standard", "include_charts": True},
            priority=3,
            timeout=1800,
            dependencies=[f"{suite_id}_benchmark"]
        )
        
        # å£“åŠ›æ¸¬è©¦
        self.add_test_task(
            suite_id=suite_id,
            task_id=f"{suite_id}_stress",
            test_type=TestType.STRESS,
            providers=providers,
            config={"stress_type": "moderate", "duration": 300},
            priority=2,
            timeout=900
        )
        
        # å°æ¯”æ¸¬è©¦
        self.add_test_task(
            suite_id=suite_id,
            task_id=f"{suite_id}_comparison",
            test_type=TestType.COMPARISON,
            providers=providers,
            config={"comparison_mode": "detailed", "generate_report": True},
            priority=1,
            timeout=1200,
            dependencies=[f"{suite_id}_performance", f"{suite_id}_stress"]
        )
        
        logger.info(f"å‰µå»ºå…¨é¢æ¸¬è©¦å¥—ä»¶: {suite_id}ï¼ŒåŒ…å« {len(suite.tasks)} å€‹ä»»å‹™")
        return suite_id
    
    def create_quick_test_suite(self, providers: List[str]) -> str:
        """å‰µå»ºå¿«é€Ÿæ¸¬è©¦å¥—ä»¶"""
        suite_id = f"quick_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        suite = self.create_test_suite(
            suite_id=suite_id,
            name="å¿«é€ŸLLMæ¸¬è©¦å¥—ä»¶",
            description="å¿«é€Ÿè©•ä¼°LLMæ¨¡å‹åŸºæœ¬æ€§èƒ½",
            parallel_limit=3
        )
        
        # åŸºç¤æ¸¬è©¦
        self.add_test_task(
            suite_id=suite_id,
            task_id=f"{suite_id}_basic",
            test_type=TestType.COMPREHENSIVE,
            providers=providers,
            config={"test_mode": "quick", "iterations": 1},
            priority=3,
            timeout=600
        )
        
        # ç°¡å–®å°æ¯”
        self.add_test_task(
            suite_id=suite_id,
            task_id=f"{suite_id}_comparison",
            test_type=TestType.COMPARISON,
            providers=providers,
            config={"comparison_mode": "basic"},
            priority=2,
            timeout=600,
            dependencies=[f"{suite_id}_basic"]
        )
        
        logger.info(f"å‰µå»ºå¿«é€Ÿæ¸¬è©¦å¥—ä»¶: {suite_id}ï¼ŒåŒ…å« {len(suite.tasks)} å€‹ä»»å‹™")
        return suite_id
    
    def create_stress_test_suite(self, providers: List[str]) -> str:
        """å‰µå»ºå£“åŠ›æ¸¬è©¦å¥—ä»¶"""
        suite_id = f"stress_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        suite = self.create_test_suite(
            suite_id=suite_id,
            name="LLMå£“åŠ›æ¸¬è©¦å¥—ä»¶",
            description="æ¸¬è©¦LLMæ¨¡å‹åœ¨é«˜è² è¼‰ä¸‹çš„è¡¨ç¾",
            parallel_limit=1  # å£“åŠ›æ¸¬è©¦é€šå¸¸ä¸ä¸¦è¡Œ
        )
        
        stress_types = ["concurrent", "sustained", "burst", "memory"]
        
        for i, stress_type in enumerate(stress_types):
            self.add_test_task(
                suite_id=suite_id,
                task_id=f"{suite_id}_{stress_type}",
                test_type=TestType.STRESS,
                providers=providers,
                config={
                    "stress_type": stress_type,
                    "duration": 600,
                    "intensity": "high"
                },
                priority=4 - i,
                timeout=1200
            )
        
        logger.info(f"å‰µå»ºå£“åŠ›æ¸¬è©¦å¥—ä»¶: {suite_id}ï¼ŒåŒ…å« {len(suite.tasks)} å€‹ä»»å‹™")
        return suite_id
    
    async def run_test_suite(self, suite_id: str) -> Dict[str, Any]:
        """é‹è¡Œæ¸¬è©¦å¥—ä»¶"""
        if suite_id not in self.test_suites:
            raise ValueError(f"æ¸¬è©¦å¥—ä»¶ {suite_id} ä¸å­˜åœ¨")
        
        suite = self.test_suites[suite_id]
        suite.status = TestStatus.RUNNING
        
        logger.info(f"é–‹å§‹é‹è¡Œæ¸¬è©¦å¥—ä»¶: {suite.name} (ID: {suite_id})")
        logger.info(f"åŒ…å« {len(suite.tasks)} å€‹ä»»å‹™ï¼Œä¸¦è¡Œé™åˆ¶: {suite.parallel_limit}")
        
        # å•Ÿå‹•è³‡æºç›£æ§
        self.resource_monitor.start_monitoring()
        
        start_time = time.time()
        
        try:
            # æŒ‰ä¾è³´é—œä¿‚å’Œå„ªå…ˆç´šæ’åºä»»å‹™
            sorted_tasks = self._sort_tasks_by_dependencies(suite.tasks)
            
            # åŸ·è¡Œä»»å‹™
            results = await self._execute_tasks_parallel(sorted_tasks, suite.parallel_limit)
            
            # åœæ­¢è³‡æºç›£æ§
            resource_stats = self.resource_monitor.stop_monitoring()
            
            execution_time = time.time() - start_time
            
            # çµ±è¨ˆçµæœ
            successful_tasks = sum(1 for r in results if r.success)
            failed_tasks = len(results) - successful_tasks
            
            suite.status = TestStatus.COMPLETED if failed_tasks == 0 else TestStatus.FAILED
            
            # ç”Ÿæˆå¥—ä»¶çµæœ
            suite_result = {
                'suite_id': suite_id,
                'suite_name': suite.name,
                'status': suite.status.value,
                'execution_time': execution_time,
                'total_tasks': len(suite.tasks),
                'successful_tasks': successful_tasks,
                'failed_tasks': failed_tasks,
                'success_rate': successful_tasks / len(suite.tasks) if suite.tasks else 0,
                'resource_usage': resource_stats,
                'task_results': [asdict(r) for r in results],
                'timestamp': datetime.now().isoformat()
            }
            
            # ä¿å­˜çµæœ
            result_file = self._save_suite_result(suite_result)
            
            logger.info(f"æ¸¬è©¦å¥—ä»¶ {suite_id} åŸ·è¡Œå®Œæˆ")
            logger.info(f"æˆåŠŸ: {successful_tasks}/{len(suite.tasks)}, åŸ·è¡Œæ™‚é–“: {execution_time:.2f}ç§’")
            logger.info(f"çµæœå·²ä¿å­˜åˆ°: {result_file}")
            
            return suite_result
            
        except Exception as e:
            suite.status = TestStatus.FAILED
            self.resource_monitor.stop_monitoring()
            logger.error(f"æ¸¬è©¦å¥—ä»¶ {suite_id} åŸ·è¡Œå¤±æ•—: {str(e)}")
            raise
    
    def _sort_tasks_by_dependencies(self, tasks: List[TestTask]) -> List[TestTask]:
        """æŒ‰ä¾è³´é—œä¿‚å’Œå„ªå…ˆç´šæ’åºä»»å‹™"""
        # å‰µå»ºä»»å‹™æ˜ å°„
        task_map = {task.task_id: task for task in tasks}
        
        # æ‹“æ’²æ’åº
        sorted_tasks = []
        visited = set()
        visiting = set()
        
        def visit(task_id: str):
            if task_id in visiting:
                raise ValueError(f"æª¢æ¸¬åˆ°å¾ªç’°ä¾è³´: {task_id}")
            if task_id in visited:
                return
            
            visiting.add(task_id)
            
            task = task_map.get(task_id)
            if task:
                for dep_id in task.dependencies:
                    if dep_id in task_map:
                        visit(dep_id)
                
                sorted_tasks.append(task)
                visited.add(task_id)
            
            visiting.remove(task_id)
        
        # è¨ªå•æ‰€æœ‰ä»»å‹™
        for task in tasks:
            if task.task_id not in visited:
                visit(task.task_id)
        
        # æŒ‰å„ªå…ˆç´šæ’åºï¼ˆç›¸åŒä¾è³´å±¤ç´šå…§ï¼‰
        return sorted(sorted_tasks, key=lambda t: (-t.priority, t.task_id))
    
    async def _execute_tasks_parallel(self, tasks: List[TestTask], 
                                    parallel_limit: int) -> List[TestResult]:
        """ä¸¦è¡ŒåŸ·è¡Œä»»å‹™"""
        results = []
        semaphore = asyncio.Semaphore(parallel_limit)
        
        async def execute_single_task(task: TestTask) -> TestResult:
            async with semaphore:
                return await self._execute_task(task)
        
        # å‰µå»ºä»»å‹™å”ç¨‹
        task_coroutines = [execute_single_task(task) for task in tasks]
        
        # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
        results = await asyncio.gather(*task_coroutines, return_exceptions=True)
        
        # è™•ç†ç•°å¸¸
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                # å‰µå»ºå¤±æ•—çµæœ
                task = tasks[i]
                failed_result = TestResult(
                    task_id=task.task_id,
                    test_type=task.test_type,
                    provider="unknown",
                    success=False,
                    execution_time=0.0,
                    result_data={},
                    error_details=str(result)
                )
                processed_results.append(failed_result)
                logger.error(f"ä»»å‹™ {task.task_id} åŸ·è¡Œç•°å¸¸: {str(result)}")
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _execute_task(self, task: TestTask) -> TestResult:
        """åŸ·è¡Œå–®å€‹æ¸¬è©¦ä»»å‹™"""
        logger.info(f"é–‹å§‹åŸ·è¡Œä»»å‹™: {task.task_id} ({task.test_type.value})")
        
        task.status = TestStatus.RUNNING
        task.start_time = datetime.now()
        
        # æº–å‚™å‘½ä»¤
        cmd = self._prepare_command(task)
        
        # å•Ÿå‹•ä»»å‹™è³‡æºç›£æ§
        task_monitor = ResourceMonitor()
        task_monitor.start_monitoring()
        
        try:
            # åŸ·è¡Œå‘½ä»¤
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=os.getcwd()
            )
            
            self.running_tasks[task.task_id] = process
            
            # ç­‰å¾…å®Œæˆæˆ–è¶…æ™‚
            try:
                stdout, stderr = await asyncio.wait_for(
                    process.communicate(),
                    timeout=task.timeout
                )
                
                task.end_time = datetime.now()
                execution_time = (task.end_time - task.start_time).total_seconds()
                
                # åœæ­¢è³‡æºç›£æ§
                resource_usage = task_monitor.stop_monitoring()
                task.resource_usage = resource_usage
                
                # æª¢æŸ¥åŸ·è¡Œçµæœ
                if process.returncode == 0:
                    task.status = TestStatus.COMPLETED
                    
                    # æŸ¥æ‰¾çµæœæ–‡ä»¶
                    result_file = self._find_result_file(task)
                    task.result_file = result_file
                    
                    # è§£æçµæœ
                    result_data = self._parse_result_file(result_file) if result_file else {}
                    
                    result = TestResult(
                        task_id=task.task_id,
                        test_type=task.test_type,
                        provider="multiple" if len(task.providers) > 1 else task.providers[0],
                        success=True,
                        execution_time=execution_time,
                        result_data=result_data,
                        resource_usage=resource_usage
                    )
                    
                    logger.info(f"ä»»å‹™ {task.task_id} åŸ·è¡ŒæˆåŠŸï¼Œè€—æ™‚ {execution_time:.2f}ç§’")
                    
                else:
                    task.status = TestStatus.FAILED
                    error_msg = stderr.decode('utf-8', errors='ignore')
                    task.error_message = error_msg
                    
                    result = TestResult(
                        task_id=task.task_id,
                        test_type=task.test_type,
                        provider="multiple" if len(task.providers) > 1 else task.providers[0],
                        success=False,
                        execution_time=execution_time,
                        result_data={},
                        error_details=error_msg,
                        resource_usage=resource_usage
                    )
                    
                    logger.error(f"ä»»å‹™ {task.task_id} åŸ·è¡Œå¤±æ•—: {error_msg}")
                
            except asyncio.TimeoutError:
                task.status = TestStatus.TIMEOUT
                task.end_time = datetime.now()
                
                # çµ‚æ­¢é€²ç¨‹
                try:
                    process.terminate()
                    await asyncio.wait_for(process.wait(), timeout=5.0)
                except:
                    process.kill()
                
                resource_usage = task_monitor.stop_monitoring()
                task.resource_usage = resource_usage
                
                result = TestResult(
                    task_id=task.task_id,
                    test_type=task.test_type,
                    provider="multiple" if len(task.providers) > 1 else task.providers[0],
                    success=False,
                    execution_time=task.timeout,
                    result_data={},
                    error_details=f"ä»»å‹™è¶…æ™‚ ({task.timeout}ç§’)",
                    resource_usage=resource_usage
                )
                
                logger.warning(f"ä»»å‹™ {task.task_id} åŸ·è¡Œè¶…æ™‚")
            
            finally:
                # æ¸…ç†
                if task.task_id in self.running_tasks:
                    del self.running_tasks[task.task_id]
            
            return result
            
        except Exception as e:
            task.status = TestStatus.FAILED
            task.end_time = datetime.now()
            task.error_message = str(e)
            
            resource_usage = task_monitor.stop_monitoring()
            task.resource_usage = resource_usage
            
            result = TestResult(
                task_id=task.task_id,
                test_type=task.test_type,
                provider="multiple" if len(task.providers) > 1 else task.providers[0],
                success=False,
                execution_time=0.0,
                result_data={},
                error_details=str(e),
                resource_usage=resource_usage
            )
            
            logger.error(f"ä»»å‹™ {task.task_id} åŸ·è¡Œç•°å¸¸: {str(e)}")
            return result
    
    def _prepare_command(self, task: TestTask) -> List[str]:
        """æº–å‚™åŸ·è¡Œå‘½ä»¤"""
        cmd = ["python", task.test_script]
        
        # æ·»åŠ æä¾›å•†åƒæ•¸
        if task.providers:
            cmd.extend(["--providers"] + task.providers)
        
        # æ·»åŠ é…ç½®åƒæ•¸
        for key, value in task.config.items():
            if isinstance(value, bool):
                if value:
                    cmd.append(f"--{key}")
            else:
                cmd.extend([f"--{key}", str(value)])
        
        # æ·»åŠ è¼¸å‡ºç›®éŒ„
        output_file = self.output_dir / "results" / f"{task.task_id}_result.json"
        cmd.extend(["--output", str(output_file)])
        
        return cmd
    
    def _find_result_file(self, task: TestTask) -> Optional[str]:
        """æŸ¥æ‰¾çµæœæ–‡ä»¶"""
        # å¯èƒ½çš„çµæœæ–‡ä»¶ä½ç½®
        possible_files = [
            self.output_dir / "results" / f"{task.task_id}_result.json",
            Path(f"{task.task_id}_result.json"),
            Path(f"test_results_{task.task_id}.json"),
            Path(f"{task.test_type.value}_results.json")
        ]
        
        for file_path in possible_files:
            if file_path.exists():
                return str(file_path)
        
        # æœå°‹æœ€è¿‘å‰µå»ºçš„çµæœæ–‡ä»¶
        result_pattern = f"*{task.task_id}*.json"
        recent_files = list(Path(".").glob(result_pattern))
        
        if recent_files:
            # è¿”å›æœ€æ–°çš„æ–‡ä»¶
            latest_file = max(recent_files, key=lambda f: f.stat().st_mtime)
            return str(latest_file)
        
        return None
    
    def _parse_result_file(self, file_path: str) -> Dict[str, Any]:
        """è§£æçµæœæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"è§£æçµæœæ–‡ä»¶å¤±æ•— {file_path}: {str(e)}")
            return {}
    
    def _save_suite_result(self, result: Dict[str, Any]) -> str:
        """ä¿å­˜å¥—ä»¶çµæœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = self.output_dir / "results" / f"suite_result_{result['suite_id']}_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, default=str)
        
        return str(filename)
    
    def _signal_handler(self, signum, frame):
        """ä¿¡è™Ÿè™•ç†å™¨"""
        logger.info(f"æ”¶åˆ°ä¿¡è™Ÿ {signum}ï¼Œæ­£åœ¨æ¸…ç†...")
        
        # çµ‚æ­¢æ‰€æœ‰é‹è¡Œä¸­çš„ä»»å‹™
        for task_id, process in self.running_tasks.items():
            try:
                process.terminate()
                logger.info(f"å·²çµ‚æ­¢ä»»å‹™: {task_id}")
            except:
                pass
        
        # åœæ­¢è³‡æºç›£æ§
        self.resource_monitor.stop_monitoring()
        
        # é—œé–‰åŸ·è¡Œå™¨
        self.executor.shutdown(wait=False)
        
        logger.info("æ¸…ç†å®Œæˆ")
        sys.exit(0)
    
    def get_suite_status(self, suite_id: str) -> Dict[str, Any]:
        """ç²å–å¥—ä»¶ç‹€æ…‹"""
        if suite_id not in self.test_suites:
            return {"error": f"å¥—ä»¶ {suite_id} ä¸å­˜åœ¨"}
        
        suite = self.test_suites[suite_id]
        
        task_statuses = {}
        for task in suite.tasks:
            task_statuses[task.task_id] = {
                'status': task.status.value,
                'start_time': task.start_time.isoformat() if task.start_time else None,
                'end_time': task.end_time.isoformat() if task.end_time else None,
                'error_message': task.error_message
            }
        
        return {
            'suite_id': suite_id,
            'name': suite.name,
            'status': suite.status.value,
            'total_tasks': len(suite.tasks),
            'task_statuses': task_statuses,
            'created_time': suite.created_time.isoformat()
        }
    
    def list_available_providers(self) -> List[str]:
        """åˆ—å‡ºå¯ç”¨çš„æä¾›å•†"""
        return [provider.value for provider in LLMProvider]
    
    def cleanup(self):
        """æ¸…ç†è³‡æº"""
        logger.info("é–‹å§‹æ¸…ç†è³‡æº...")
        
        # åœæ­¢æ‰€æœ‰é‹è¡Œä¸­çš„ä»»å‹™
        for task_id, process in self.running_tasks.items():
            try:
                process.terminate()
                logger.info(f"å·²çµ‚æ­¢ä»»å‹™: {task_id}")
            except:
                pass
        
        # åœæ­¢è³‡æºç›£æ§
        self.resource_monitor.stop_monitoring()
        
        # é—œé–‰åŸ·è¡Œå™¨
        self.executor.shutdown(wait=True)
        
        logger.info("è³‡æºæ¸…ç†å®Œæˆ")


def main():
    """ä¸»å‡½æ•¸ - è‡ªå‹•åŒ–æ¸¬è©¦åŸ·è¡Œå™¨å…¥å£é»"""
    print("ğŸ¤– è‡ªå‹•åŒ–LLMæ¸¬è©¦åŸ·è¡Œå™¨")
    print("=" * 60)
    
    # å‰µå»ºæ¸¬è©¦åŸ·è¡Œå™¨
    runner = AutomatedLLMTestRunner()
    
    try:
        # ç²å–å¯ç”¨æä¾›å•†
        available_providers = runner.list_available_providers()
        print(f"\nğŸ“‹ å¯ç”¨çš„LLMæä¾›å•†: {', '.join(available_providers)}")
        
        # é¸æ“‡æ¸¬è©¦æ¨¡å¼
        print("\nğŸ¯ é¸æ“‡æ¸¬è©¦æ¨¡å¼:")
        print("1. å¿«é€Ÿæ¸¬è©¦ (åŸºç¤åŠŸèƒ½é©—è­‰)")
        print("2. å…¨é¢æ¸¬è©¦ (å®Œæ•´æ€§èƒ½è©•ä¼°)")
        print("3. å£“åŠ›æ¸¬è©¦ (é«˜è² è¼‰æ¸¬è©¦)")
        print("4. è‡ªå®šç¾©æ¸¬è©¦")
        
        choice = input("\nè«‹é¸æ“‡ (1-4): ").strip()
        
        # é¸æ“‡æä¾›å•†
        print(f"\nğŸ“ é¸æ“‡è¦æ¸¬è©¦çš„æä¾›å•† (å¯å¤šé¸ï¼Œç”¨é€—è™Ÿåˆ†éš”):")
        print(f"å¯é¸: {', '.join(available_providers)}")
        provider_input = input("æä¾›å•†: ").strip()
        
        if not provider_input:
            providers = available_providers[:2]  # é»˜èªé¸æ“‡å‰å…©å€‹
        else:
            providers = [p.strip() for p in provider_input.split(',') if p.strip() in available_providers]
        
        if not providers:
            print("âŒ æ²’æœ‰é¸æ“‡æœ‰æ•ˆçš„æä¾›å•†")
            return
        
        print(f"\nâœ… å·²é¸æ“‡æä¾›å•†: {', '.join(providers)}")
        
        # å‰µå»ºæ¸¬è©¦å¥—ä»¶
        if choice == "1":
            suite_id = runner.create_quick_test_suite(providers)
        elif choice == "2":
            suite_id = runner.create_comprehensive_test_suite(providers)
        elif choice == "3":
            suite_id = runner.create_stress_test_suite(providers)
        else:
            print("âŒ è‡ªå®šç¾©æ¸¬è©¦æ¨¡å¼å°šæœªå¯¦ç¾")
            return
        
        # é‹è¡Œæ¸¬è©¦å¥—ä»¶
        print(f"\nğŸš€ é–‹å§‹åŸ·è¡Œæ¸¬è©¦å¥—ä»¶: {suite_id}")
        
        async def run_tests():
            result = await runner.run_test_suite(suite_id)
            return result
        
        # åŸ·è¡Œæ¸¬è©¦
        result = asyncio.run(run_tests())
        
        # é¡¯ç¤ºçµæœæ‘˜è¦
        print("\nğŸ“Š æ¸¬è©¦çµæœæ‘˜è¦:")
        print(f"   å¥—ä»¶: {result['suite_name']}")
        print(f"   ç‹€æ…‹: {result['status']}")
        print(f"   ç¸½ä»»å‹™: {result['total_tasks']}")
        print(f"   æˆåŠŸ: {result['successful_tasks']}")
        print(f"   å¤±æ•—: {result['failed_tasks']}")
        print(f"   æˆåŠŸç‡: {result['success_rate']:.1%}")
        print(f"   åŸ·è¡Œæ™‚é–“: {result['execution_time']:.2f}ç§’")
        
        if result['resource_usage']:
            print(f"   å¹³å‡CPU: {result['resource_usage'].get('avg_cpu_percent', 0):.1f}%")
            print(f"   å¹³å‡è¨˜æ†¶é«”: {result['resource_usage'].get('avg_memory_mb', 0):.1f}MB")
        
        print(f"\nğŸ’¾ è©³ç´°çµæœå·²ä¿å­˜åˆ°: {runner.output_dir / 'results'}")
        
        # ç”Ÿæˆå ±å‘Š
        print("\nğŸ“„ æ˜¯å¦ç”Ÿæˆå°æ¯”å ±å‘Š? (y/n): ", end="")
        if input().strip().lower() == 'y':
            print("æ­£åœ¨ç”Ÿæˆå ±å‘Š...")
            # é€™è£¡å¯ä»¥èª¿ç”¨å ±å‘Šç”Ÿæˆå™¨
            subprocess.run(["python", "llm_comparison_report_generator.py"], check=False)
        
    except KeyboardInterrupt:
        print("\nâŒ æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†è³‡æº
        runner.cleanup()


if __name__ == "__main__":
    main()