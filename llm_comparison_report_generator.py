#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMæ¨¡å‹å°æ¯”åˆ†æå ±å‘Šç”Ÿæˆå™¨
ç”¨æ–¼ç”Ÿæˆè©³ç´°çš„LLMæ¨¡å‹æ¸¬è©¦çµæœåˆ†æå ±å‘Šï¼ŒåŒ…å«åœ–è¡¨ã€çµ±è¨ˆåˆ†æå’Œå»ºè­°

Author: JobSpy Team
Date: 2025-01-27
"""

import sys
import os
import json
import time
import statistics
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict, field
from collections import defaultdict, Counter
from pathlib import Path
import warnings
from jinja2 import Template
import base64
from io import BytesIO

# è¨­ç½®ä¸­æ–‡å­—é«”å’Œæ¨£å¼
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
sns.set_palette("husl")
warnings.filterwarnings('ignore')

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from jobseeker.llm_intent_analyzer import LLMProvider


@dataclass
class ComparisonMetrics:
    """å°æ¯”æŒ‡æ¨™"""
    provider: str
    total_tests: int
    success_rate: float
    avg_response_time: float
    median_response_time: float
    p95_response_time: float
    p99_response_time: float
    throughput: float
    accuracy_score: float
    consistency_score: float
    robustness_score: float
    cost_efficiency: float
    overall_score: float
    strengths: List[str] = field(default_factory=list)
    weaknesses: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)


@dataclass
class CategoryAnalysis:
    """é¡åˆ¥åˆ†æ"""
    category: str
    provider_scores: Dict[str, float]
    best_provider: str
    worst_provider: str
    score_range: float
    insights: List[str] = field(default_factory=list)


@dataclass
class TrendAnalysis:
    """è¶¨å‹¢åˆ†æ"""
    provider: str
    time_series: List[Tuple[float, float]]  # (timestamp, metric_value)
    trend_direction: str  # 'improving', 'declining', 'stable'
    trend_strength: float  # 0-1
    seasonal_patterns: Dict[str, float]
    anomalies: List[Tuple[float, float, str]]  # (timestamp, value, description)


class LLMComparisonReportGenerator:
    """LLMæ¨¡å‹å°æ¯”åˆ†æå ±å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: str = "reports"):
        """åˆå§‹åŒ–å ±å‘Šç”Ÿæˆå™¨"""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # å‰µå»ºå­ç›®éŒ„
        (self.output_dir / "charts").mkdir(exist_ok=True)
        (self.output_dir / "data").mkdir(exist_ok=True)
        (self.output_dir / "html").mkdir(exist_ok=True)
        
        self.comparison_data: List[Dict[str, Any]] = []
        self.metrics: List[ComparisonMetrics] = []
        self.category_analyses: List[CategoryAnalysis] = []
        self.trend_analyses: List[TrendAnalysis] = []
        
        # åœ–è¡¨é…ç½®
        self.chart_config = {
            'figsize': (12, 8),
            'dpi': 300,
            'style': 'whitegrid',
            'palette': 'husl',
            'font_size': 12
        }
    
    def load_test_results(self, result_files: List[str]) -> None:
        """è¼‰å…¥æ¸¬è©¦çµæœæ–‡ä»¶"""
        print("ğŸ“‚ è¼‰å…¥æ¸¬è©¦çµæœæ–‡ä»¶...")
        
        for file_path in result_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.comparison_data.append({
                        'file': file_path,
                        'data': data
                    })
                print(f"   âœ… å·²è¼‰å…¥: {file_path}")
            except Exception as e:
                print(f"   âŒ è¼‰å…¥å¤±æ•— {file_path}: {str(e)}")
        
        print(f"ğŸ“Š ç¸½å…±è¼‰å…¥ {len(self.comparison_data)} å€‹çµæœæ–‡ä»¶")
    
    def analyze_comparison_metrics(self) -> None:
        """åˆ†æå°æ¯”æŒ‡æ¨™"""
        print("ğŸ” åˆ†æå°æ¯”æŒ‡æ¨™...")
        
        provider_data = defaultdict(list)
        
        # æ”¶é›†å„æä¾›å•†çš„æ•¸æ“š
        for file_data in self.comparison_data:
            results = file_data['data'].get('results', [])
            
            for result in results:
                provider = result.get('provider', 'unknown')
                provider_data[provider].append(result)
        
        # è¨ˆç®—æ¯å€‹æä¾›å•†çš„æŒ‡æ¨™
        for provider, results in provider_data.items():
            metrics = self._calculate_provider_metrics(provider, results)
            self.metrics.append(metrics)
        
        # æ’åºï¼ˆæŒ‰ç¸½åˆ†ï¼‰
        self.metrics.sort(key=lambda x: x.overall_score, reverse=True)
        
        print(f"   ğŸ“ˆ å·²åˆ†æ {len(self.metrics)} å€‹æä¾›å•†çš„æŒ‡æ¨™")
    
    def _calculate_provider_metrics(self, provider: str, results: List[Dict[str, Any]]) -> ComparisonMetrics:
        """è¨ˆç®—æä¾›å•†æŒ‡æ¨™"""
        if not results:
            return ComparisonMetrics(
                provider=provider,
                total_tests=0,
                success_rate=0.0,
                avg_response_time=0.0,
                median_response_time=0.0,
                p95_response_time=0.0,
                p99_response_time=0.0,
                throughput=0.0,
                accuracy_score=0.0,
                consistency_score=0.0,
                robustness_score=0.0,
                cost_efficiency=0.0,
                overall_score=0.0
            )
        
        # åŸºæœ¬çµ±è¨ˆ
        total_tests = sum(r.get('total_requests', 0) for r in results)
        success_rates = [r.get('success_rate', 0.0) for r in results]
        response_times = [r.get('avg_response_time', 0.0) for r in results]
        throughputs = [r.get('throughput', 0.0) for r in results]
        
        # è¨ˆç®—å¹³å‡å€¼
        avg_success_rate = statistics.mean(success_rates) if success_rates else 0.0
        avg_response_time = statistics.mean(response_times) if response_times else 0.0
        avg_throughput = statistics.mean(throughputs) if throughputs else 0.0
        
        # è¨ˆç®—ç™¾åˆ†ä½æ•¸
        all_response_times = []
        for result in results:
            rt_dist = result.get('response_time_distribution', [])
            all_response_times.extend(rt_dist)
        
        if all_response_times:
            median_response_time = np.percentile(all_response_times, 50)
            p95_response_time = np.percentile(all_response_times, 95)
            p99_response_time = np.percentile(all_response_times, 99)
        else:
            median_response_time = p95_response_time = p99_response_time = 0.0
        
        # è¨ˆç®—é«˜ç´šæŒ‡æ¨™
        accuracy_score = self._calculate_accuracy_score(results)
        consistency_score = self._calculate_consistency_score(results)
        robustness_score = self._calculate_robustness_score(results)
        cost_efficiency = self._calculate_cost_efficiency(results)
        
        # è¨ˆç®—ç¸½åˆ†
        overall_score = self._calculate_overall_score(
            avg_success_rate, avg_response_time, avg_throughput,
            accuracy_score, consistency_score, robustness_score, cost_efficiency
        )
        
        # ç”Ÿæˆå„ªç¼ºé»å’Œå»ºè­°
        strengths, weaknesses, recommendations = self._generate_insights(
            provider, avg_success_rate, avg_response_time, avg_throughput,
            accuracy_score, consistency_score, robustness_score, cost_efficiency
        )
        
        return ComparisonMetrics(
            provider=provider,
            total_tests=total_tests,
            success_rate=avg_success_rate,
            avg_response_time=avg_response_time,
            median_response_time=median_response_time,
            p95_response_time=p95_response_time,
            p99_response_time=p99_response_time,
            throughput=avg_throughput,
            accuracy_score=accuracy_score,
            consistency_score=consistency_score,
            robustness_score=robustness_score,
            cost_efficiency=cost_efficiency,
            overall_score=overall_score,
            strengths=strengths,
            weaknesses=weaknesses,
            recommendations=recommendations
        )
    
    def _calculate_accuracy_score(self, results: List[Dict[str, Any]]) -> float:
        """è¨ˆç®—æº–ç¢ºæ€§åˆ†æ•¸"""
        # åŸºæ–¼æˆåŠŸç‡å’ŒéŒ¯èª¤åˆ†ä½ˆè¨ˆç®—
        success_rates = [r.get('success_rate', 0.0) for r in results]
        
        if not success_rates:
            return 0.0
        
        # è€ƒæ…®æˆåŠŸç‡çš„ç©©å®šæ€§
        avg_success_rate = statistics.mean(success_rates)
        success_rate_std = statistics.stdev(success_rates) if len(success_rates) > 1 else 0.0
        
        # ç©©å®šæ€§æ‡²ç½°
        stability_penalty = min(success_rate_std * 2, 0.2)
        
        return max(0.0, avg_success_rate - stability_penalty)
    
    def _calculate_consistency_score(self, results: List[Dict[str, Any]]) -> float:
        """è¨ˆç®—ä¸€è‡´æ€§åˆ†æ•¸"""
        # åŸºæ–¼éŸ¿æ‡‰æ™‚é–“è®Šç•°æ€§è¨ˆç®—
        response_times = [r.get('avg_response_time', 0.0) for r in results]
        
        if len(response_times) < 2:
            return 1.0 if response_times else 0.0
        
        # è¨ˆç®—è®Šç•°ä¿‚æ•¸
        mean_rt = statistics.mean(response_times)
        std_rt = statistics.stdev(response_times)
        
        if mean_rt == 0:
            return 0.0
        
        cv = std_rt / mean_rt
        
        # è®Šç•°ä¿‚æ•¸è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šé«˜
        return max(0.0, 1.0 - cv)
    
    def _calculate_robustness_score(self, results: List[Dict[str, Any]]) -> float:
        """è¨ˆç®—é­¯æ£’æ€§åˆ†æ•¸"""
        # åŸºæ–¼éŒ¯èª¤è™•ç†å’Œæ¢å¾©èƒ½åŠ›è¨ˆç®—
        error_rates = []
        timeout_rates = []
        
        for result in results:
            total_requests = result.get('total_requests', 0)
            if total_requests > 0:
                error_rate = result.get('error_requests', 0) / total_requests
                timeout_rate = result.get('timeout_requests', 0) / total_requests
                error_rates.append(error_rate)
                timeout_rates.append(timeout_rate)
        
        if not error_rates:
            return 0.0
        
        avg_error_rate = statistics.mean(error_rates)
        avg_timeout_rate = statistics.mean(timeout_rates)
        
        # éŒ¯èª¤ç‡å’Œè¶…æ™‚ç‡è¶Šä½ï¼Œé­¯æ£’æ€§è¶Šé«˜
        robustness = 1.0 - (avg_error_rate + avg_timeout_rate)
        
        return max(0.0, robustness)
    
    def _calculate_cost_efficiency(self, results: List[Dict[str, Any]]) -> float:
        """è¨ˆç®—æˆæœ¬æ•ˆç‡"""
        # åŸºæ–¼ååé‡å’Œè³‡æºä½¿ç”¨è¨ˆç®—
        throughputs = [r.get('throughput', 0.0) for r in results]
        memory_usages = [r.get('avg_memory_usage', 0.0) for r in results]
        
        if not throughputs or not memory_usages:
            return 0.0
        
        avg_throughput = statistics.mean(throughputs)
        avg_memory = statistics.mean(memory_usages)
        
        if avg_memory == 0:
            return 0.0
        
        # ååé‡/è¨˜æ†¶é«”ä½¿ç”¨é‡ = æ•ˆç‡
        efficiency = avg_throughput / avg_memory
        
        # æ­£è¦åŒ–åˆ°0-1ç¯„åœ
        return min(1.0, efficiency / 10.0)  # å‡è¨­10æ˜¯ä¸€å€‹åˆç†çš„ä¸Šé™
    
    def _calculate_overall_score(self, success_rate: float, response_time: float, 
                               throughput: float, accuracy: float, consistency: float, 
                               robustness: float, cost_efficiency: float) -> float:
        """è¨ˆç®—ç¸½åˆ†"""
        # æ¬Šé‡é…ç½®
        weights = {
            'success_rate': 0.25,
            'response_time': 0.15,  # éŸ¿æ‡‰æ™‚é–“è¶Šä½è¶Šå¥½
            'throughput': 0.15,
            'accuracy': 0.20,
            'consistency': 0.10,
            'robustness': 0.10,
            'cost_efficiency': 0.05
        }
        
        # æ­£è¦åŒ–éŸ¿æ‡‰æ™‚é–“ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
        normalized_response_time = max(0.0, 1.0 - min(response_time / 30.0, 1.0))
        
        # æ­£è¦åŒ–ååé‡
        normalized_throughput = min(throughput / 10.0, 1.0)
        
        # è¨ˆç®—åŠ æ¬Šç¸½åˆ†
        total_score = (
            weights['success_rate'] * success_rate +
            weights['response_time'] * normalized_response_time +
            weights['throughput'] * normalized_throughput +
            weights['accuracy'] * accuracy +
            weights['consistency'] * consistency +
            weights['robustness'] * robustness +
            weights['cost_efficiency'] * cost_efficiency
        )
        
        return total_score
    
    def _generate_insights(self, provider: str, success_rate: float, response_time: float,
                         throughput: float, accuracy: float, consistency: float,
                         robustness: float, cost_efficiency: float) -> Tuple[List[str], List[str], List[str]]:
        """ç”Ÿæˆæ´å¯Ÿã€å„ªç¼ºé»å’Œå»ºè­°"""
        strengths = []
        weaknesses = []
        recommendations = []
        
        # åˆ†æå„ªå‹¢
        if success_rate > 0.95:
            strengths.append("æ¥µé«˜çš„æˆåŠŸç‡")
        elif success_rate > 0.90:
            strengths.append("é«˜æˆåŠŸç‡")
        
        if response_time < 2.0:
            strengths.append("å¿«é€ŸéŸ¿æ‡‰æ™‚é–“")
        elif response_time < 5.0:
            strengths.append("è‰¯å¥½çš„éŸ¿æ‡‰æ™‚é–“")
        
        if throughput > 5.0:
            strengths.append("é«˜ååé‡")
        elif throughput > 2.0:
            strengths.append("è‰¯å¥½çš„ååé‡")
        
        if accuracy > 0.90:
            strengths.append("é«˜æº–ç¢ºæ€§")
        
        if consistency > 0.85:
            strengths.append("è‰¯å¥½çš„ä¸€è‡´æ€§")
        
        if robustness > 0.85:
            strengths.append("å¼·é­¯æ£’æ€§")
        
        if cost_efficiency > 0.7:
            strengths.append("é«˜æˆæœ¬æ•ˆç‡")
        
        # åˆ†æå¼±é»
        if success_rate < 0.80:
            weaknesses.append("æˆåŠŸç‡åä½")
            recommendations.append("æª¢æŸ¥APIé…ç½®å’Œç¶²è·¯é€£æ¥")
        
        if response_time > 10.0:
            weaknesses.append("éŸ¿æ‡‰æ™‚é–“éé•·")
            recommendations.append("å„ªåŒ–è«‹æ±‚åƒæ•¸æˆ–è€ƒæ…®ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹")
        
        if throughput < 1.0:
            weaknesses.append("ååé‡è¼ƒä½")
            recommendations.append("å¢åŠ ä¸¦ç™¼æ•¸æˆ–å„ªåŒ–è«‹æ±‚è™•ç†")
        
        if accuracy < 0.80:
            weaknesses.append("æº–ç¢ºæ€§æœ‰å¾…æå‡")
            recommendations.append("èª¿æ•´æ¨¡å‹åƒæ•¸æˆ–ä½¿ç”¨æ›´é©åˆçš„æ¨¡å‹")
        
        if consistency < 0.70:
            weaknesses.append("ä¸€è‡´æ€§ä¸ç©©å®š")
            recommendations.append("æª¢æŸ¥æ¨¡å‹é…ç½®çš„ç©©å®šæ€§")
        
        if robustness < 0.70:
            weaknesses.append("éŒ¯èª¤è™•ç†èƒ½åŠ›ä¸è¶³")
            recommendations.append("å¢å¼·éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶")
        
        if cost_efficiency < 0.5:
            weaknesses.append("æˆæœ¬æ•ˆç‡è¼ƒä½")
            recommendations.append("å„ªåŒ–è³‡æºä½¿ç”¨æˆ–è€ƒæ…®æ›´ç¶“æ¿Ÿçš„æ–¹æ¡ˆ")
        
        # é€šç”¨å»ºè­°
        if not recommendations:
            recommendations.append("ç¹¼çºŒç›£æ§æ€§èƒ½ä¸¦å®šæœŸè©•ä¼°")
        
        return strengths, weaknesses, recommendations
    
    def analyze_categories(self) -> None:
        """åˆ†æå„é¡åˆ¥è¡¨ç¾"""
        print("ğŸ“Š åˆ†æå„é¡åˆ¥è¡¨ç¾...")
        
        categories = [
            ('æˆåŠŸç‡', 'success_rate'),
            ('éŸ¿æ‡‰æ™‚é–“', 'avg_response_time'),
            ('ååé‡', 'throughput'),
            ('æº–ç¢ºæ€§', 'accuracy_score'),
            ('ä¸€è‡´æ€§', 'consistency_score'),
            ('é­¯æ£’æ€§', 'robustness_score'),
            ('æˆæœ¬æ•ˆç‡', 'cost_efficiency')
        ]
        
        for category_name, metric_key in categories:
            provider_scores = {}
            
            for metric in self.metrics:
                score = getattr(metric, metric_key)
                # éŸ¿æ‡‰æ™‚é–“éœ€è¦åè½‰ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
                if metric_key == 'avg_response_time':
                    score = max(0.0, 1.0 - min(score / 30.0, 1.0))
                provider_scores[metric.provider] = score
            
            if provider_scores:
                best_provider = max(provider_scores.keys(), key=lambda k: provider_scores[k])
                worst_provider = min(provider_scores.keys(), key=lambda k: provider_scores[k])
                score_range = max(provider_scores.values()) - min(provider_scores.values())
                
                insights = self._generate_category_insights(category_name, provider_scores)
                
                analysis = CategoryAnalysis(
                    category=category_name,
                    provider_scores=provider_scores,
                    best_provider=best_provider,
                    worst_provider=worst_provider,
                    score_range=score_range,
                    insights=insights
                )
                
                self.category_analyses.append(analysis)
        
        print(f"   ğŸ“ˆ å·²åˆ†æ {len(self.category_analyses)} å€‹é¡åˆ¥")
    
    def _generate_category_insights(self, category: str, scores: Dict[str, float]) -> List[str]:
        """ç”Ÿæˆé¡åˆ¥æ´å¯Ÿ"""
        insights = []
        
        if not scores:
            return insights
        
        values = list(scores.values())
        avg_score = statistics.mean(values)
        std_score = statistics.stdev(values) if len(values) > 1 else 0.0
        
        # åˆ†æåˆ†æ•¸åˆ†ä½ˆ
        if std_score < 0.1:
            insights.append(f"{category}æ–¹é¢å„æä¾›å•†è¡¨ç¾ç›¸è¿‘")
        elif std_score > 0.3:
            insights.append(f"{category}æ–¹é¢æä¾›å•†é–“å·®ç•°é¡¯è‘—")
        
        # åˆ†ææ•´é«”æ°´å¹³
        if avg_score > 0.8:
            insights.append(f"æ•´é«”{category}æ°´å¹³è¼ƒé«˜")
        elif avg_score < 0.5:
            insights.append(f"æ•´é«”{category}æ°´å¹³æœ‰å¾…æå‡")
        
        return insights
    
    def generate_charts(self) -> Dict[str, str]:
        """ç”Ÿæˆåœ–è¡¨"""
        print("ğŸ“ˆ ç”Ÿæˆå°æ¯”åœ–è¡¨...")
        
        chart_files = {}
        
        if not self.metrics:
            print("   âŒ æ²’æœ‰æ•¸æ“šå¯ç”Ÿæˆåœ–è¡¨")
            return chart_files
        
        # 1. ç¸½åˆ†å°æ¯”é›·é”åœ–
        chart_files['radar'] = self._create_radar_chart()
        
        # 2. éŸ¿æ‡‰æ™‚é–“åˆ†ä½ˆåœ–
        chart_files['response_time'] = self._create_response_time_chart()
        
        # 3. æˆåŠŸç‡å°æ¯”æŸ±ç‹€åœ–
        chart_files['success_rate'] = self._create_success_rate_chart()
        
        # 4. ååé‡å°æ¯”åœ–
        chart_files['throughput'] = self._create_throughput_chart()
        
        # 5. ç¶œåˆæ€§èƒ½ç†±åŠ›åœ–
        chart_files['heatmap'] = self._create_performance_heatmap()
        
        # 6. æˆæœ¬æ•ˆç‡æ•£é»åœ–
        chart_files['cost_efficiency'] = self._create_cost_efficiency_chart()
        
        print(f"   ğŸ“Š å·²ç”Ÿæˆ {len(chart_files)} å€‹åœ–è¡¨")
        return chart_files
    
    def _create_radar_chart(self) -> str:
        """å‰µå»ºé›·é”åœ–"""
        fig, ax = plt.subplots(figsize=self.chart_config['figsize'], 
                              subplot_kw=dict(projection='polar'))
        
        # æº–å‚™æ•¸æ“š
        categories = ['æˆåŠŸç‡', 'éŸ¿æ‡‰æ™‚é–“', 'ååé‡', 'æº–ç¢ºæ€§', 'ä¸€è‡´æ€§', 'é­¯æ£’æ€§', 'æˆæœ¬æ•ˆç‡']
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        angles += angles[:1]  # é–‰åˆåœ–å½¢
        
        # ç‚ºæ¯å€‹æä¾›å•†ç¹ªè£½é›·é”åœ–
        colors = plt.cm.Set3(np.linspace(0, 1, len(self.metrics)))
        
        for i, metric in enumerate(self.metrics):
            values = [
                metric.success_rate,
                max(0.0, 1.0 - min(metric.avg_response_time / 30.0, 1.0)),  # éŸ¿æ‡‰æ™‚é–“åè½‰
                min(metric.throughput / 10.0, 1.0),  # æ­£è¦åŒ–ååé‡
                metric.accuracy_score,
                metric.consistency_score,
                metric.robustness_score,
                metric.cost_efficiency
            ]
            values += values[:1]  # é–‰åˆåœ–å½¢
            
            ax.plot(angles, values, 'o-', linewidth=2, label=metric.provider, color=colors[i])
            ax.fill(angles, values, alpha=0.25, color=colors[i])
        
        # è¨­ç½®æ¨™ç±¤
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories)
        ax.set_ylim(0, 1)
        ax.set_title('LLMæä¾›å•†ç¶œåˆæ€§èƒ½å°æ¯”', size=16, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax.grid(True)
        
        # ä¿å­˜åœ–è¡¨
        filename = self.output_dir / "charts" / "radar_comparison.png"
        plt.tight_layout()
        plt.savefig(filename, dpi=self.chart_config['dpi'], bbox_inches='tight')
        plt.close()
        
        return str(filename)
    
    def _create_response_time_chart(self) -> str:
        """å‰µå»ºéŸ¿æ‡‰æ™‚é–“åˆ†ä½ˆåœ–"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # æº–å‚™æ•¸æ“š
        providers = [m.provider for m in self.metrics]
        avg_times = [m.avg_response_time for m in self.metrics]
        p95_times = [m.p95_response_time for m in self.metrics]
        p99_times = [m.p99_response_time for m in self.metrics]
        
        # å·¦åœ–ï¼šå¹³å‡éŸ¿æ‡‰æ™‚é–“æŸ±ç‹€åœ–
        bars = ax1.bar(providers, avg_times, color='skyblue', alpha=0.7)
        ax1.set_title('å¹³å‡éŸ¿æ‡‰æ™‚é–“å°æ¯”', fontsize=14, fontweight='bold')
        ax1.set_ylabel('éŸ¿æ‡‰æ™‚é–“ (ç§’)')
        ax1.tick_params(axis='x', rotation=45)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar, time in zip(bars, avg_times):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                    f'{time:.2f}s', ha='center', va='bottom')
        
        # å³åœ–ï¼šç™¾åˆ†ä½æ•¸å°æ¯”
        x = np.arange(len(providers))
        width = 0.25
        
        ax2.bar(x - width, avg_times, width, label='å¹³å‡', alpha=0.8)
        ax2.bar(x, p95_times, width, label='P95', alpha=0.8)
        ax2.bar(x + width, p99_times, width, label='P99', alpha=0.8)
        
        ax2.set_title('éŸ¿æ‡‰æ™‚é–“ç™¾åˆ†ä½æ•¸å°æ¯”', fontsize=14, fontweight='bold')
        ax2.set_ylabel('éŸ¿æ‡‰æ™‚é–“ (ç§’)')
        ax2.set_xticks(x)
        ax2.set_xticklabels(providers, rotation=45)
        ax2.legend()
        
        # ä¿å­˜åœ–è¡¨
        filename = self.output_dir / "charts" / "response_time_comparison.png"
        plt.tight_layout()
        plt.savefig(filename, dpi=self.chart_config['dpi'], bbox_inches='tight')
        plt.close()
        
        return str(filename)
    
    def _create_success_rate_chart(self) -> str:
        """å‰µå»ºæˆåŠŸç‡å°æ¯”åœ–"""
        fig, ax = plt.subplots(figsize=self.chart_config['figsize'])
        
        # æº–å‚™æ•¸æ“š
        providers = [m.provider for m in self.metrics]
        success_rates = [m.success_rate * 100 for m in self.metrics]  # è½‰æ›ç‚ºç™¾åˆ†æ¯”
        
        # å‰µå»ºæŸ±ç‹€åœ–
        colors = ['green' if sr >= 95 else 'orange' if sr >= 90 else 'red' for sr in success_rates]
        bars = ax.bar(providers, success_rates, color=colors, alpha=0.7)
        
        # è¨­ç½®æ¨™ç±¤å’Œæ¨™é¡Œ
        ax.set_title('æˆåŠŸç‡å°æ¯”', fontsize=16, fontweight='bold')
        ax.set_ylabel('æˆåŠŸç‡ (%)')
        ax.set_ylim(0, 100)
        ax.tick_params(axis='x', rotation=45)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar, rate in zip(bars, success_rates):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                   f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        # æ·»åŠ åƒè€ƒç·š
        ax.axhline(y=95, color='green', linestyle='--', alpha=0.5, label='å„ªç§€ (95%)')
        ax.axhline(y=90, color='orange', linestyle='--', alpha=0.5, label='è‰¯å¥½ (90%)')
        ax.legend()
        
        # ä¿å­˜åœ–è¡¨
        filename = self.output_dir / "charts" / "success_rate_comparison.png"
        plt.tight_layout()
        plt.savefig(filename, dpi=self.chart_config['dpi'], bbox_inches='tight')
        plt.close()
        
        return str(filename)
    
    def _create_throughput_chart(self) -> str:
        """å‰µå»ºååé‡å°æ¯”åœ–"""
        fig, ax = plt.subplots(figsize=self.chart_config['figsize'])
        
        # æº–å‚™æ•¸æ“š
        providers = [m.provider for m in self.metrics]
        throughputs = [m.throughput for m in self.metrics]
        
        # å‰µå»ºæŸ±ç‹€åœ–
        bars = ax.bar(providers, throughputs, color='lightcoral', alpha=0.7)
        
        # è¨­ç½®æ¨™ç±¤å’Œæ¨™é¡Œ
        ax.set_title('ååé‡å°æ¯”', fontsize=16, fontweight='bold')
        ax.set_ylabel('ååé‡ (è«‹æ±‚/ç§’)')
        ax.tick_params(axis='x', rotation=45)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar, throughput in zip(bars, throughputs):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,
                   f'{throughput:.2f}', ha='center', va='bottom', fontweight='bold')
        
        # ä¿å­˜åœ–è¡¨
        filename = self.output_dir / "charts" / "throughput_comparison.png"
        plt.tight_layout()
        plt.savefig(filename, dpi=self.chart_config['dpi'], bbox_inches='tight')
        plt.close()
        
        return str(filename)
    
    def _create_performance_heatmap(self) -> str:
        """å‰µå»ºæ€§èƒ½ç†±åŠ›åœ–"""
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # æº–å‚™æ•¸æ“š
        providers = [m.provider for m in self.metrics]
        metrics_data = []
        metric_names = ['æˆåŠŸç‡', 'éŸ¿æ‡‰æ™‚é–“', 'ååé‡', 'æº–ç¢ºæ€§', 'ä¸€è‡´æ€§', 'é­¯æ£’æ€§', 'æˆæœ¬æ•ˆç‡']
        
        for metric in self.metrics:
            row = [
                metric.success_rate,
                max(0.0, 1.0 - min(metric.avg_response_time / 30.0, 1.0)),  # éŸ¿æ‡‰æ™‚é–“åè½‰
                min(metric.throughput / 10.0, 1.0),  # æ­£è¦åŒ–ååé‡
                metric.accuracy_score,
                metric.consistency_score,
                metric.robustness_score,
                metric.cost_efficiency
            ]
            metrics_data.append(row)
        
        # å‰µå»ºç†±åŠ›åœ–
        im = ax.imshow(metrics_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)
        
        # è¨­ç½®æ¨™ç±¤
        ax.set_xticks(range(len(metric_names)))
        ax.set_xticklabels(metric_names, rotation=45, ha='right')
        ax.set_yticks(range(len(providers)))
        ax.set_yticklabels(providers)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for i in range(len(providers)):
            for j in range(len(metric_names)):
                text = ax.text(j, i, f'{metrics_data[i][j]:.2f}',
                             ha="center", va="center", color="black", fontweight='bold')
        
        # æ·»åŠ é¡è‰²æ¢
        cbar = plt.colorbar(im, ax=ax)
        cbar.set_label('æ€§èƒ½åˆ†æ•¸', rotation=270, labelpad=20)
        
        ax.set_title('LLMæä¾›å•†ç¶œåˆæ€§èƒ½ç†±åŠ›åœ–', fontsize=16, fontweight='bold', pad=20)
        
        # ä¿å­˜åœ–è¡¨
        filename = self.output_dir / "charts" / "performance_heatmap.png"
        plt.tight_layout()
        plt.savefig(filename, dpi=self.chart_config['dpi'], bbox_inches='tight')
        plt.close()
        
        return str(filename)
    
    def _create_cost_efficiency_chart(self) -> str:
        """å‰µå»ºæˆæœ¬æ•ˆç‡æ•£é»åœ–"""
        fig, ax = plt.subplots(figsize=self.chart_config['figsize'])
        
        # æº–å‚™æ•¸æ“š
        throughputs = [m.throughput for m in self.metrics]
        cost_efficiencies = [m.cost_efficiency for m in self.metrics]
        providers = [m.provider for m in self.metrics]
        
        # å‰µå»ºæ•£é»åœ–
        scatter = ax.scatter(throughputs, cost_efficiencies, s=100, alpha=0.7, c=range(len(providers)), cmap='viridis')
        
        # æ·»åŠ æä¾›å•†æ¨™ç±¤
        for i, provider in enumerate(providers):
            ax.annotate(provider, (throughputs[i], cost_efficiencies[i]), 
                       xytext=(5, 5), textcoords='offset points', fontsize=10)
        
        # è¨­ç½®æ¨™ç±¤å’Œæ¨™é¡Œ
        ax.set_xlabel('ååé‡ (è«‹æ±‚/ç§’)')
        ax.set_ylabel('æˆæœ¬æ•ˆç‡')
        ax.set_title('ååé‡ vs æˆæœ¬æ•ˆç‡', fontsize=16, fontweight='bold')
        ax.grid(True, alpha=0.3)
        
        # ä¿å­˜åœ–è¡¨
        filename = self.output_dir / "charts" / "cost_efficiency_scatter.png"
        plt.tight_layout()
        plt.savefig(filename, dpi=self.chart_config['dpi'], bbox_inches='tight')
        plt.close()
        
        return str(filename)
    
    def generate_html_report(self, chart_files: Dict[str, str]) -> str:
        """ç”ŸæˆHTMLå ±å‘Š"""
        print("ğŸ“„ ç”ŸæˆHTMLå ±å‘Š...")
        
        # HTMLæ¨¡æ¿
        html_template = """
<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLMæ¨¡å‹å°æ¯”åˆ†æå ±å‘Š</title>
    <style>
        body {
            font-family: 'Microsoft YaHei', Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background-color: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
        }
        h1, h2, h3 {
            color: #333;
            border-bottom: 2px solid #4CAF50;
            padding-bottom: 10px;
        }
        .metric-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            margin: 15px 0;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        .metric-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        .chart-container {
            text-align: center;
            margin: 30px 0;
            padding: 20px;
            background-color: #fafafa;
            border-radius: 10px;
        }
        .chart-container img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        .insights {
            background-color: #e8f5e8;
            padding: 15px;
            border-left: 4px solid #4CAF50;
            margin: 15px 0;
            border-radius: 5px;
        }
        .strengths {
            color: #2e7d32;
        }
        .weaknesses {
            color: #d32f2f;
        }
        .recommendations {
            color: #1976d2;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        th {
            background-color: #4CAF50;
            color: white;
        }
        tr:nth-child(even) {
            background-color: #f2f2f2;
        }
        .score-high { color: #2e7d32; font-weight: bold; }
        .score-medium { color: #f57c00; font-weight: bold; }
        .score-low { color: #d32f2f; font-weight: bold; }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ¤– LLMæ¨¡å‹å°æ¯”åˆ†æå ±å‘Š</h1>
        <p><strong>ç”Ÿæˆæ™‚é–“:</strong> {{ timestamp }}</p>
        <p><strong>æ¸¬è©¦æä¾›å•†æ•¸é‡:</strong> {{ provider_count }}</p>
        <p><strong>ç¸½æ¸¬è©¦æ¬¡æ•¸:</strong> {{ total_tests }}</p>
        
        <h2>ğŸ“Š ç¶œåˆæ’å</h2>
        <div class="metric-grid">
            {% for metric in metrics %}
            <div class="metric-card">
                <h3>{{ loop.index }}. {{ metric.provider }}</h3>
                <p><strong>ç¸½åˆ†:</strong> {{ "%.2f"|format(metric.overall_score) }}</p>
                <p><strong>æˆåŠŸç‡:</strong> {{ "%.1f%%"|format(metric.success_rate * 100) }}</p>
                <p><strong>å¹³å‡éŸ¿æ‡‰æ™‚é–“:</strong> {{ "%.2fç§’"|format(metric.avg_response_time) }}</p>
                <p><strong>ååé‡:</strong> {{ "%.2f è«‹æ±‚/ç§’"|format(metric.throughput) }}</p>
            </div>
            {% endfor %}
        </div>
        
        <h2>ğŸ“ˆ æ€§èƒ½åœ–è¡¨</h2>
        {% for chart_name, chart_file in charts.items() %}
        <div class="chart-container">
            <h3>{{ chart_titles[chart_name] }}</h3>
            <img src="{{ chart_file }}" alt="{{ chart_titles[chart_name] }}">
        </div>
        {% endfor %}
        
        <h2>ğŸ“‹ è©³ç´°æŒ‡æ¨™å°æ¯”</h2>
        <table>
            <thead>
                <tr>
                    <th>æä¾›å•†</th>
                    <th>ç¸½åˆ†</th>
                    <th>æˆåŠŸç‡</th>
                    <th>éŸ¿æ‡‰æ™‚é–“</th>
                    <th>ååé‡</th>
                    <th>æº–ç¢ºæ€§</th>
                    <th>ä¸€è‡´æ€§</th>
                    <th>é­¯æ£’æ€§</th>
                    <th>æˆæœ¬æ•ˆç‡</th>
                </tr>
            </thead>
            <tbody>
                {% for metric in metrics %}
                <tr>
                    <td><strong>{{ metric.provider }}</strong></td>
                    <td class="{{ 'score-high' if metric.overall_score > 0.8 else 'score-medium' if metric.overall_score > 0.6 else 'score-low' }}">{{ "%.3f"|format(metric.overall_score) }}</td>
                    <td class="{{ 'score-high' if metric.success_rate > 0.95 else 'score-medium' if metric.success_rate > 0.9 else 'score-low' }}">{{ "%.1f%%"|format(metric.success_rate * 100) }}</td>
                    <td class="{{ 'score-high' if metric.avg_response_time < 2 else 'score-medium' if metric.avg_response_time < 5 else 'score-low' }}">{{ "%.2fç§’"|format(metric.avg_response_time) }}</td>
                    <td class="{{ 'score-high' if metric.throughput > 5 else 'score-medium' if metric.throughput > 2 else 'score-low' }}">{{ "%.2f"|format(metric.throughput) }}</td>
                    <td class="{{ 'score-high' if metric.accuracy_score > 0.9 else 'score-medium' if metric.accuracy_score > 0.8 else 'score-low' }}">{{ "%.3f"|format(metric.accuracy_score) }}</td>
                    <td class="{{ 'score-high' if metric.consistency_score > 0.85 else 'score-medium' if metric.consistency_score > 0.7 else 'score-low' }}">{{ "%.3f"|format(metric.consistency_score) }}</td>
                    <td class="{{ 'score-high' if metric.robustness_score > 0.85 else 'score-medium' if metric.robustness_score > 0.7 else 'score-low' }}">{{ "%.3f"|format(metric.robustness_score) }}</td>
                    <td class="{{ 'score-high' if metric.cost_efficiency > 0.7 else 'score-medium' if metric.cost_efficiency > 0.5 else 'score-low' }}">{{ "%.3f"|format(metric.cost_efficiency) }}</td>
                </tr>
                {% endfor %}
            </tbody>
        </table>
        
        <h2>ğŸ’¡ åˆ†ææ´å¯Ÿ</h2>
        {% for metric in metrics %}
        <div class="insights">
            <h3>{{ metric.provider }}</h3>
            {% if metric.strengths %}
            <div class="strengths">
                <strong>âœ… å„ªå‹¢:</strong>
                <ul>
                    {% for strength in metric.strengths %}
                    <li>{{ strength }}</li>
                    {% endfor %}
                </ul>
            </div>
            {% endif %}
            
            {% if metric.weaknesses %}
            <div class="weaknesses">
                <strong>âŒ å¼±é»:</strong>
                <ul>
                    {% for weakness in metric.weaknesses %}
                    <li>{{ weakness }}</li>
                    {% endfor %}
                </ul>
            </div>
            {% endif %}
            
            {% if metric.recommendations %}
            <div class="recommendations">
                <strong>ğŸ’¡ å»ºè­°:</strong>
                <ul>
                    {% for recommendation in metric.recommendations %}
                    <li>{{ recommendation }}</li>
                    {% endfor %}
                </ul>
            </div>
            {% endif %}
        </div>
        {% endfor %}
        
        <h2>ğŸ¯ é¡åˆ¥åˆ†æ</h2>
        {% for analysis in category_analyses %}
        <div class="insights">
            <h3>{{ analysis.category }}</h3>
            <p><strong>æœ€ä½³æä¾›å•†:</strong> {{ analysis.best_provider }}</p>
            <p><strong>è¡¨ç¾æœ€å·®:</strong> {{ analysis.worst_provider }}</p>
            <p><strong>åˆ†æ•¸ç¯„åœ:</strong> {{ "%.3f"|format(analysis.score_range) }}</p>
            {% if analysis.insights %}
            <ul>
                {% for insight in analysis.insights %}
                <li>{{ insight }}</li>
                {% endfor %}
            </ul>
            {% endif %}
        </div>
        {% endfor %}
        
        <h2>ğŸ“ ç¸½çµå»ºè­°</h2>
        <div class="insights">
            <h3>ğŸ† æ¨è–¦é¸æ“‡</h3>
            {% if metrics %}
            <p><strong>ç¶œåˆè¡¨ç¾æœ€ä½³:</strong> {{ metrics[0].provider }} (ç¸½åˆ†: {{ "%.3f"|format(metrics[0].overall_score) }})</p>
            <p><strong>æ€§åƒ¹æ¯”æœ€é«˜:</strong> {{ best_cost_efficiency.provider }} (æˆæœ¬æ•ˆç‡: {{ "%.3f"|format(best_cost_efficiency.cost_efficiency) }})</p>
            <p><strong>éŸ¿æ‡‰æœ€å¿«:</strong> {{ fastest_provider.provider }} (å¹³å‡éŸ¿æ‡‰æ™‚é–“: {{ "%.2fç§’"|format(fastest_provider.avg_response_time) }})</p>
            <p><strong>æœ€ç©©å®š:</strong> {{ most_consistent.provider }} (ä¸€è‡´æ€§åˆ†æ•¸: {{ "%.3f"|format(most_consistent.consistency_score) }})</p>
            {% endif %}
            
            <h3>ğŸ¯ ä½¿ç”¨å»ºè­°</h3>
            <ul>
                <li><strong>é«˜æ€§èƒ½éœ€æ±‚:</strong> é¸æ“‡éŸ¿æ‡‰æ™‚é–“æœ€å¿«ä¸”ååé‡æœ€é«˜çš„æä¾›å•†</li>
                <li><strong>ç©©å®šæ€§å„ªå…ˆ:</strong> é¸æ“‡ä¸€è‡´æ€§å’Œé­¯æ£’æ€§åˆ†æ•¸æœ€é«˜çš„æä¾›å•†</li>
                <li><strong>æˆæœ¬æ•æ„Ÿ:</strong> é¸æ“‡æˆæœ¬æ•ˆç‡æœ€é«˜çš„æä¾›å•†</li>
                <li><strong>æº–ç¢ºæ€§è¦æ±‚:</strong> é¸æ“‡æº–ç¢ºæ€§åˆ†æ•¸æœ€é«˜çš„æä¾›å•†</li>
            </ul>
        </div>
        
        <footer style="margin-top: 50px; text-align: center; color: #666;">
            <p>å ±å‘Šç”± LLMæ¨¡å‹å°æ¯”åˆ†æå·¥å…· è‡ªå‹•ç”Ÿæˆ</p>
            <p>ç”Ÿæˆæ™‚é–“: {{ timestamp }}</p>
        </footer>
    </div>
</body>
</html>
        """
        
        # æº–å‚™æ¨¡æ¿æ•¸æ“š
        template_data = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'provider_count': len(self.metrics),
            'total_tests': sum(m.total_tests for m in self.metrics),
            'metrics': self.metrics,
            'charts': chart_files,
            'chart_titles': {
                'radar': 'ç¶œåˆæ€§èƒ½é›·é”åœ–',
                'response_time': 'éŸ¿æ‡‰æ™‚é–“åˆ†æ',
                'success_rate': 'æˆåŠŸç‡å°æ¯”',
                'throughput': 'ååé‡å°æ¯”',
                'heatmap': 'æ€§èƒ½ç†±åŠ›åœ–',
                'cost_efficiency': 'æˆæœ¬æ•ˆç‡åˆ†æ'
            },
            'category_analyses': self.category_analyses,
            'best_cost_efficiency': max(self.metrics, key=lambda x: x.cost_efficiency) if self.metrics else None,
            'fastest_provider': min(self.metrics, key=lambda x: x.avg_response_time) if self.metrics else None,
            'most_consistent': max(self.metrics, key=lambda x: x.consistency_score) if self.metrics else None
        }
        
        # æ¸²æŸ“HTML
        template = Template(html_template)
        html_content = template.render(**template_data)
        
        # ä¿å­˜HTMLæ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        html_filename = self.output_dir / "html" / f"llm_comparison_report_{timestamp}.html"
        
        with open(html_filename, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"   ğŸ“„ HTMLå ±å‘Šå·²ç”Ÿæˆ: {html_filename}")
        return str(html_filename)
    
    def save_comparison_data(self) -> str:
        """ä¿å­˜å°æ¯”æ•¸æ“š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = self.output_dir / "data" / f"comparison_analysis_{timestamp}.json"
        
        # æº–å‚™ä¿å­˜çš„æ•¸æ“š
        save_data = {
            'analysis_summary': {
                'timestamp': datetime.now().isoformat(),
                'provider_count': len(self.metrics),
                'total_tests': sum(m.total_tests for m in self.metrics)
            },
            'metrics': [asdict(metric) for metric in self.metrics],
            'category_analyses': [asdict(analysis) for analysis in self.category_analyses],
            'trend_analyses': [asdict(trend) for trend in self.trend_analyses]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ’¾ å°æ¯”åˆ†ææ•¸æ“šå·²ä¿å­˜: {filename}")
        return str(filename)
    
    def print_summary(self) -> None:
        """æ‰“å°æ‘˜è¦"""
        if not self.metrics:
            print("âŒ æ²’æœ‰åˆ†æçµæœå¯é¡¯ç¤º")
            return
        
        print("\nğŸ“Š LLMæ¨¡å‹å°æ¯”åˆ†ææ‘˜è¦")
        print("=" * 60)
        
        print(f"\nğŸ† ç¶œåˆæ’å (å…± {len(self.metrics)} å€‹æä¾›å•†):")
        for i, metric in enumerate(self.metrics, 1):
            print(f"   {i}. {metric.provider:<15} ç¸½åˆ†: {metric.overall_score:.3f}")
        
        if self.metrics:
            best = self.metrics[0]
            print(f"\nğŸ¥‡ æœ€ä½³æä¾›å•†: {best.provider}")
            print(f"   ç¸½åˆ†: {best.overall_score:.3f}")
            print(f"   æˆåŠŸç‡: {best.success_rate:.1%}")
            print(f"   éŸ¿æ‡‰æ™‚é–“: {best.avg_response_time:.2f}ç§’")
            print(f"   ååé‡: {best.throughput:.2f} è«‹æ±‚/ç§’")
        
        print(f"\nğŸ“ˆ é¡åˆ¥åˆ†æ (å…± {len(self.category_analyses)} å€‹é¡åˆ¥):")
        for analysis in self.category_analyses:
            print(f"   {analysis.category:<10} æœ€ä½³: {analysis.best_provider}")


def main():
    """ä¸»å‡½æ•¸ - å ±å‘Šç”Ÿæˆå™¨å…¥å£é»"""
    print("ğŸ“Š LLMæ¨¡å‹å°æ¯”åˆ†æå ±å‘Šç”Ÿæˆå™¨")
    print("=" * 60)
    
    # å‰µå»ºå ±å‘Šç”Ÿæˆå™¨
    generator = LLMComparisonReportGenerator()
    
    # æŸ¥æ‰¾æ¸¬è©¦çµæœæ–‡ä»¶
    result_files = []
    current_dir = Path(".")
    
    # æœå°‹æ¸¬è©¦çµæœæ–‡ä»¶
    patterns = [
        "*test_results*.json",
        "*llm_test*.json",
        "*comparison*.json",
        "*benchmark*.json"
    ]
    
    for pattern in patterns:
        result_files.extend(current_dir.glob(pattern))
    
    if not result_files:
        print("âŒ æœªæ‰¾åˆ°æ¸¬è©¦çµæœæ–‡ä»¶")
        print("è«‹ç¢ºä¿ç•¶å‰ç›®éŒ„ä¸‹æœ‰ä»¥ä¸‹æ ¼å¼çš„æ–‡ä»¶:")
        for pattern in patterns:
            print(f"   - {pattern}")
        return
    
    print(f"\nğŸ“‚ æ‰¾åˆ° {len(result_files)} å€‹æ¸¬è©¦çµæœæ–‡ä»¶:")
    for file in result_files:
        print(f"   - {file}")
    
    try:
        # è¼‰å…¥æ¸¬è©¦çµæœ
        generator.load_test_results([str(f) for f in result_files])
        
        # åˆ†æå°æ¯”æŒ‡æ¨™
        generator.analyze_comparison_metrics()
        
        # åˆ†æå„é¡åˆ¥è¡¨ç¾
        generator.analyze_categories()
        
        # ç”Ÿæˆåœ–è¡¨
        chart_files = generator.generate_charts()
        
        # ç”ŸæˆHTMLå ±å‘Š
        html_report = generator.generate_html_report(chart_files)
        
        # ä¿å­˜åˆ†ææ•¸æ“š
        data_file = generator.save_comparison_data()
        
        # é¡¯ç¤ºæ‘˜è¦
        generator.print_summary()
        
        print(f"\nâœ… å ±å‘Šç”Ÿæˆå®Œæˆï¼")
        print(f"   ğŸ“„ HTMLå ±å‘Š: {html_report}")
        print(f"   ğŸ’¾ åˆ†ææ•¸æ“š: {data_file}")
        print(f"   ğŸ“Š åœ–è¡¨ç›®éŒ„: {generator.output_dir / 'charts'}")
        
    except KeyboardInterrupt:
        print("\nâŒ å ±å‘Šç”Ÿæˆè¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ å ±å‘Šç”Ÿæˆéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()