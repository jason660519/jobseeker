#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•¸æ“šæŸ¥è©¢å·¥å…·
æä¾›å‘½ä»¤è¡Œç•Œé¢ä¾†æŸ¥è©¢å’Œåˆ†æçˆ¬èŸ²æ•¸æ“š
"""

import json
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
import argparse

class DataQuery:
    """æ•¸æ“šæŸ¥è©¢å™¨"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self.index_file = self.data_dir / "index" / "data_index.json"
        self.load_index()
    
    def load_index(self):
        """åŠ è¼‰æ•¸æ“šç´¢å¼•"""
        if self.index_file.exists():
            with open(self.index_file, 'r', encoding='utf-8') as f:
                self.index = json.load(f)
        else:
            self.index = {"files": {}, "by_site": {}, "by_date": {}, "by_search_term": {}}
    
    def list_sites(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰ç¶²ç«™"""
        return list(self.index.get("by_site", {}).keys())
    
    def list_dates(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰æ—¥æœŸ"""
        return sorted(self.index.get("by_date", {}).keys())
    
    def list_search_terms(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰æœå°‹è©"""
        return list(self.index.get("by_search_term", {}).keys())
    
    def get_data_by_site(self, site: str) -> List[Dict]:
        """æ ¹æ“šç¶²ç«™ç²å–æ•¸æ“š"""
        return self.index.get("by_site", {}).get(site, [])
    
    def get_data_by_date(self, date: str) -> List[Dict]:
        """æ ¹æ“šæ—¥æœŸç²å–æ•¸æ“š"""
        return self.index.get("by_date", {}).get(date, [])
    
    def get_data_by_search_term(self, search_term: str) -> List[Dict]:
        """æ ¹æ“šæœå°‹è©ç²å–æ•¸æ“š"""
        return self.index.get("by_search_term", {}).get(search_term, [])
    
    def get_data_summary(self) -> Dict:
        """ç²å–æ•¸æ“šæ‘˜è¦"""
        total_files = len(self.index.get("files", {}))
        total_records = sum(
            file_info.get("total_records", 0) 
            for file_info in self.index.get("files", {}).values()
        )
        
        sites = self.list_sites()
        dates = self.list_dates()
        search_terms = self.list_search_terms()
        
        return {
            "total_files": total_files,
            "total_records": total_records,
            "sites": sites,
            "date_range": {
                "earliest": dates[0] if dates else None,
                "latest": dates[-1] if dates else None
            },
            "unique_search_terms": len(search_terms),
            "unique_sites": len(sites)
        }
    
    def search_data(self, site: Optional[str] = None, date: Optional[str] = None, 
                   search_term: Optional[str] = None) -> List[Dict]:
        """æœå°‹æ•¸æ“š"""
        results = []
        
        if site:
            results.extend(self.get_data_by_site(site))
        
        if date:
            results.extend(self.get_data_by_date(date))
        
        if search_term:
            results.extend(self.get_data_by_search_term(search_term))
        
        # å»é‡
        seen = set()
        unique_results = []
        for result in results:
            key = result.get("filepath", "")
            if key not in seen:
                seen.add(key)
                unique_results.append(result)
        
        return unique_results
    
    def load_job_data(self, filepath: str) -> List[Dict]:
        """åŠ è¼‰è·ä½æ•¸æ“š"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if "jobs" in data:
                return data["jobs"]
            elif "data" in data:
                return data["data"]
            else:
                return data
        except Exception as e:
            print(f"âŒ åŠ è¼‰æ•¸æ“šå¤±æ•— {filepath}: {e}")
            return []
    
    def analyze_jobs(self, jobs: List[Dict]) -> Dict:
        """åˆ†æè·ä½æ•¸æ“š"""
        if not jobs:
            return {}
        
        df = pd.DataFrame(jobs)
        
        analysis = {
            "total_jobs": len(jobs),
            "unique_companies": df['company'].nunique() if 'company' in df.columns else 0,
            "unique_locations": df['location'].nunique() if 'location' in df.columns else 0,
            "salary_info": self.analyze_salaries(df),
            "top_companies": self.get_top_companies(df),
            "top_locations": self.get_top_locations(df),
            "job_types": self.get_job_types(df)
        }
        
        return analysis
    
    def analyze_salaries(self, df: pd.DataFrame) -> Dict:
        """åˆ†æè–ªè³‡ä¿¡æ¯"""
        if 'salary' not in df.columns:
            return {"available": False}
        
        salary_data = df[df['salary'].notna() & (df['salary'] != '')]
        
        if len(salary_data) == 0:
            return {"available": False, "count": 0}
        
        return {
            "available": True,
            "count": len(salary_data),
            "percentage": round(len(salary_data) / len(df) * 100, 2)
        }
    
    def get_top_companies(self, df: pd.DataFrame, top_n: int = 10) -> List[Dict]:
        """ç²å–ç†±é–€å…¬å¸"""
        if 'company' not in df.columns:
            return []
        
        company_counts = df['company'].value_counts().head(top_n)
        
        return [
            {"company": company, "job_count": count}
            for company, count in company_counts.items()
        ]
    
    def get_top_locations(self, df: pd.DataFrame, top_n: int = 10) -> List[Dict]:
        """ç²å–ç†±é–€åœ°é»"""
        if 'location' not in df.columns:
            return []
        
        location_counts = df['location'].value_counts().head(top_n)
        
        return [
            {"location": location, "job_count": count}
            for location, count in location_counts.items()
        ]
    
    def get_job_types(self, df: pd.DataFrame) -> Dict:
        """ç²å–å·¥ä½œé¡å‹åˆ†å¸ƒ"""
        if 'job_type' not in df.columns:
            return {}
        
        job_type_counts = df['job_type'].value_counts()
        
        return {
            job_type: count
            for job_type, count in job_type_counts.items()
        }
    
    def export_analysis(self, analysis: Dict, output_file: str):
        """å°å‡ºåˆ†æçµæœ"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… åˆ†æçµæœå·²å°å‡º: {output_file}")

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="JobSpy æ•¸æ“šæŸ¥è©¢å·¥å…·")
    parser.add_argument("--data-dir", default="data", help="æ•¸æ“šç›®éŒ„è·¯å¾‘")
    parser.add_argument("--site", help="æŒ‰ç¶²ç«™ç¯©é¸")
    parser.add_argument("--date", help="æŒ‰æ—¥æœŸç¯©é¸ (YYYYMMDD)")
    parser.add_argument("--search-term", help="æŒ‰æœå°‹è©ç¯©é¸")
    parser.add_argument("--summary", action="store_true", help="é¡¯ç¤ºæ•¸æ“šæ‘˜è¦")
    parser.add_argument("--list-sites", action="store_true", help="åˆ—å‡ºæ‰€æœ‰ç¶²ç«™")
    parser.add_argument("--list-dates", action="store_true", help="åˆ—å‡ºæ‰€æœ‰æ—¥æœŸ")
    parser.add_argument("--analyze", action="store_true", help="åˆ†æè·ä½æ•¸æ“š")
    parser.add_argument("--export", help="å°å‡ºåˆ†æçµæœåˆ°æ–‡ä»¶")
    
    args = parser.parse_args()
    
    query = DataQuery(args.data_dir)
    
    if args.list_sites:
        sites = query.list_sites()
        print("ğŸ“Š å¯ç”¨ç¶²ç«™:")
        for site in sites:
            print(f"  - {site}")
    
    elif args.list_dates:
        dates = query.list_dates()
        print("ğŸ“… å¯ç”¨æ—¥æœŸ:")
        for date in dates:
            print(f"  - {date}")
    
    elif args.summary:
        summary = query.get_data_summary()
        print("ğŸ“Š æ•¸æ“šæ‘˜è¦:")
        print(f"  ç¸½æ–‡ä»¶æ•¸: {summary['total_files']}")
        print(f"  ç¸½è·ä½æ•¸: {summary['total_records']}")
        print(f"  ç¶²ç«™æ•¸: {summary['unique_sites']}")
        print(f"  æœå°‹è©æ•¸: {summary['unique_search_terms']}")
        print(f"  æ—¥æœŸç¯„åœ: {summary['date_range']['earliest']} ~ {summary['date_range']['latest']}")
    
    else:
        # æœå°‹æ•¸æ“š
        results = query.search_data(
            site=args.site,
            date=args.date,
            search_term=args.search_term
        )
        
        print(f"ğŸ” æ‰¾åˆ° {len(results)} å€‹åŒ¹é…çš„æ•¸æ“šæ–‡ä»¶")
        
        if args.analyze and results:
            # åˆ†æç¬¬ä¸€å€‹æ–‡ä»¶çš„æ•¸æ“š
            first_result = results[0]
            jobs = query.load_job_data(first_result['filepath'])
            analysis = query.analyze_jobs(jobs)
            
            print("ğŸ“Š è·ä½åˆ†æ:")
            print(f"  ç¸½è·ä½æ•¸: {analysis.get('total_jobs', 0)}")
            print(f"  å…¬å¸æ•¸: {analysis.get('unique_companies', 0)}")
            print(f"  åœ°é»æ•¸: {analysis.get('unique_locations', 0)}")
            
            if analysis.get('salary_info', {}).get('available'):
                salary_info = analysis['salary_info']
                print(f"  æœ‰è–ªè³‡è³‡è¨Š: {salary_info['count']} ({salary_info['percentage']}%)")
            
            if analysis.get('top_companies'):
                print("  ç†±é–€å…¬å¸:")
                for company in analysis['top_companies'][:5]:
                    print(f"    - {company['company']}: {company['job_count']} å€‹è·ä½")
            
            if args.export:
                query.export_analysis(analysis, args.export)

if __name__ == "__main__":
    main()
