#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•¸æ“šæ¸…ç†è…³æœ¬
æ¸…ç†èˆŠæ•¸æ“šã€å£“ç¸®æ­¸æª”ã€ç”Ÿæˆæ¸…ç†å ±å‘Š
"""

import json
import gzip
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List
import argparse

class DataCleaner:
    """æ•¸æ“šæ¸…ç†å™¨"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self.cleanup_log = []
    
    def cleanup_old_data(self, retention_days: int = 30, dry_run: bool = False):
        """æ¸…ç†èˆŠæ•¸æ“š"""
        print(f"ğŸ§¹ é–‹å§‹æ¸…ç† {retention_days} å¤©å‰çš„æ•¸æ“š...")
        
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        cutoff_str = cutoff_date.strftime("%Y%m%d")
        
        # æ¸…ç†æŒ‰æ—¥æœŸåˆ†é¡çš„åŸå§‹æ•¸æ“š
        raw_date_dir = self.data_dir / "raw" / "by_date"
        archived_count = 0
        
        if raw_date_dir.exists():
            for date_dir in raw_date_dir.iterdir():
                if date_dir.is_dir() and date_dir.name < cutoff_str:
                    if dry_run:
                        print(f"  [DRY RUN] å°‡æ­¸æª”ç›®éŒ„: {date_dir.name}")
                    else:
                        self.archive_directory(date_dir)
                    archived_count += 1
        
        # æ¸…ç†èˆŠçš„å°å‡ºæ–‡ä»¶
        exports_dir = self.data_dir / "exports"
        cleaned_exports = 0
        
        if exports_dir.exists():
            for format_dir in exports_dir.iterdir():
                if format_dir.is_dir():
                    cleaned_exports += self.cleanup_old_files(format_dir, cutoff_date, dry_run)
        
        # æ¸…ç†èˆŠçš„å ±å‘Šæ–‡ä»¶
        reports_dir = self.data_dir / "reports"
        cleaned_reports = 0
        
        if reports_dir.exists():
            for report_type_dir in reports_dir.iterdir():
                if report_type_dir.is_dir():
                    cleaned_reports += self.cleanup_old_files(report_type_dir, cutoff_date, dry_run)
        
        print(f"âœ… æ¸…ç†å®Œæˆ:")
        print(f"  - æ­¸æª”äº† {archived_count} å€‹æ—¥æœŸç›®éŒ„")
        print(f"  - æ¸…ç†äº† {cleaned_exports} å€‹å°å‡ºæ–‡ä»¶")
        print(f"  - æ¸…ç†äº† {cleaned_reports} å€‹å ±å‘Šæ–‡ä»¶")
        
        return {
            "archived_directories": archived_count,
            "cleaned_exports": cleaned_exports,
            "cleaned_reports": cleaned_reports
        }
    
    def archive_directory(self, source_dir: Path):
        """æ­¸æª”ç›®éŒ„"""
        archive_dir = self.data_dir / "archive" / source_dir.name
        
        if archive_dir.exists():
            shutil.rmtree(archive_dir)
        
        shutil.move(str(source_dir), str(archive_dir))
        
        self.cleanup_log.append({
            "action": "archive_directory",
            "source": str(source_dir),
            "target": str(archive_dir),
            "timestamp": datetime.now().isoformat()
        })
    
    def cleanup_old_files(self, directory: Path, cutoff_date: datetime, dry_run: bool = False) -> int:
        """æ¸…ç†ç›®éŒ„ä¸­çš„èˆŠæ–‡ä»¶"""
        cleaned_count = 0
        
        for file_path in directory.iterdir():
            if file_path.is_file():
                file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                if file_time < cutoff_date:
                    if dry_run:
                        print(f"  [DRY RUN] å°‡åˆªé™¤æ–‡ä»¶: {file_path.name}")
                    else:
                        file_path.unlink()
                        self.cleanup_log.append({
                            "action": "delete_file",
                            "file": str(file_path),
                            "timestamp": datetime.now().isoformat()
                        })
                    cleaned_count += 1
        
        return cleaned_count
    
    def compress_archive(self, compress_days: int = 7):
        """å£“ç¸®æ­¸æª”æ•¸æ“š"""
        print(f"ğŸ—œï¸  é–‹å§‹å£“ç¸® {compress_days} å¤©å‰çš„æ­¸æª”æ•¸æ“š...")
        
        archive_dir = self.data_dir / "archive"
        if not archive_dir.exists():
            print("âš ï¸  æ­¸æª”ç›®éŒ„ä¸å­˜åœ¨ï¼Œè·³éå£“ç¸®")
            return 0
        
        compressed_count = 0
        cutoff_date = datetime.now() - timedelta(days=compress_days)
        
        for item in archive_dir.iterdir():
            if item.is_dir():
                item_time = datetime.fromtimestamp(item.stat().st_mtime)
                if item_time < cutoff_date:
                    # å£“ç¸®ç›®éŒ„
                    compressed_file = item.with_suffix('.tar.gz')
                    if not compressed_file.exists():
                        self.compress_directory(item, compressed_file)
                        compressed_count += 1
        
        print(f"âœ… å·²å£“ç¸® {compressed_count} å€‹æ­¸æª”ç›®éŒ„")
        return compressed_count
    
    def compress_directory(self, source_dir: Path, target_file: Path):
        """å£“ç¸®ç›®éŒ„"""
        import tarfile
        
        with tarfile.open(target_file, "w:gz") as tar:
            tar.add(source_dir, arcname=source_dir.name)
        
        # åˆªé™¤åŸå§‹ç›®éŒ„
        shutil.rmtree(source_dir)
        
        self.cleanup_log.append({
            "action": "compress_directory",
            "source": str(source_dir),
            "target": str(target_file),
            "timestamp": datetime.now().isoformat()
        })
    
    def optimize_storage(self):
        """å„ªåŒ–å­˜å„²ç©ºé–“"""
        print("ğŸ”§ é–‹å§‹å„ªåŒ–å­˜å„²ç©ºé–“...")
        
        # 1. æ¸…ç†é‡è¤‡æ–‡ä»¶
        duplicates_removed = self.remove_duplicate_files()
        
        # 2. å£“ç¸®å¤§æ–‡ä»¶
        compressed_files = self.compress_large_files()
        
        # 3. æ¸…ç†ç©ºç›®éŒ„
        empty_dirs_removed = self.remove_empty_directories()
        
        print(f"âœ… å­˜å„²å„ªåŒ–å®Œæˆ:")
        print(f"  - ç§»é™¤äº† {duplicates_removed} å€‹é‡è¤‡æ–‡ä»¶")
        print(f"  - å£“ç¸®äº† {compressed_files} å€‹å¤§æ–‡ä»¶")
        print(f"  - ç§»é™¤äº† {empty_dirs_removed} å€‹ç©ºç›®éŒ„")
        
        return {
            "duplicates_removed": duplicates_removed,
            "compressed_files": compressed_files,
            "empty_dirs_removed": empty_dirs_removed
        }
    
    def remove_duplicate_files(self) -> int:
        """ç§»é™¤é‡è¤‡æ–‡ä»¶"""
        # é€™è£¡å¯ä»¥å¯¦ç¾é‡è¤‡æ–‡ä»¶æª¢æ¸¬é‚è¼¯
        # æš«æ™‚è¿”å› 0
        return 0
    
    def compress_large_files(self, size_mb: int = 10) -> int:
        """å£“ç¸®å¤§æ–‡ä»¶"""
        compressed_count = 0
        size_bytes = size_mb * 1024 * 1024
        
        for file_path in self.data_dir.rglob("*"):
            if file_path.is_file() and file_path.stat().st_size > size_bytes:
                if file_path.suffix not in ['.gz', '.zip', '.tar']:
                    compressed_file = file_path.with_suffix(file_path.suffix + '.gz')
                    if not compressed_file.exists():
                        self.compress_file(file_path, compressed_file)
                        compressed_count += 1
        
        return compressed_count
    
    def compress_file(self, source_file: Path, target_file: Path):
        """å£“ç¸®å–®å€‹æ–‡ä»¶"""
        with open(source_file, 'rb') as f_in:
            with gzip.open(target_file, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        
        # åˆªé™¤åŸå§‹æ–‡ä»¶
        source_file.unlink()
        
        self.cleanup_log.append({
            "action": "compress_file",
            "source": str(source_file),
            "target": str(target_file),
            "timestamp": datetime.now().isoformat()
        })
    
    def remove_empty_directories(self) -> int:
        """ç§»é™¤ç©ºç›®éŒ„"""
        removed_count = 0
        
        # å¾æœ€æ·±å±¤é–‹å§‹æª¢æŸ¥
        for dir_path in sorted(self.data_dir.rglob("*"), key=lambda p: len(p.parts), reverse=True):
            if dir_path.is_dir() and dir_path != self.data_dir:
                try:
                    if not any(dir_path.iterdir()):
                        dir_path.rmdir()
                        removed_count += 1
                except OSError:
                    pass
        
        return removed_count
    
    def generate_cleanup_report(self):
        """ç”Ÿæˆæ¸…ç†å ±å‘Š"""
        report_data = {
            "cleanup_info": {
                "cleaned_at": datetime.now().isoformat(),
                "total_actions": len(self.cleanup_log),
                "data_directory": str(self.data_dir)
            },
            "cleanup_log": self.cleanup_log,
            "storage_info": self.get_storage_info()
        }
        
        report_path = self.data_dir / "cleanup_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š æ¸…ç†å ±å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    def get_storage_info(self) -> Dict:
        """ç²å–å­˜å„²ä¿¡æ¯"""
        total_size = 0
        file_count = 0
        
        for file_path in self.data_dir.rglob("*"):
            if file_path.is_file():
                total_size += file_path.stat().st_size
                file_count += 1
        
        return {
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "file_count": file_count,
            "directory_structure": self.get_directory_structure()
        }
    
    def get_directory_structure(self) -> Dict:
        """ç²å–ç›®éŒ„çµæ§‹"""
        structure = {}
        
        for item in self.data_dir.iterdir():
            if item.is_dir():
                structure[item.name] = self.count_directory_contents(item)
        
        return structure
    
    def count_directory_contents(self, directory: Path) -> Dict:
        """çµ±è¨ˆç›®éŒ„å…§å®¹"""
        files = 0
        dirs = 0
        size = 0
        
        for item in directory.rglob("*"):
            if item.is_file():
                files += 1
                size += item.stat().st_size
            elif item.is_dir():
                dirs += 1
        
        return {
            "files": files,
            "directories": dirs,
            "size_mb": round(size / (1024 * 1024), 2)
        }

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="JobSpy æ•¸æ“šæ¸…ç†å·¥å…·")
    parser.add_argument("--data-dir", default="data", help="æ•¸æ“šç›®éŒ„è·¯å¾‘")
    parser.add_argument("--retention-days", type=int, default=30, help="æ•¸æ“šä¿ç•™å¤©æ•¸")
    parser.add_argument("--compress-days", type=int, default=7, help="å£“ç¸®æ­¸æª”å¤©æ•¸")
    parser.add_argument("--dry-run", action="store_true", help="è©¦é‹è¡Œæ¨¡å¼")
    parser.add_argument("--optimize", action="store_true", help="å„ªåŒ–å­˜å„²ç©ºé–“")
    
    args = parser.parse_args()
    
    print("ğŸ§¹ é–‹å§‹ JobSpy æ•¸æ“šæ¸…ç†...")
    
    cleaner = DataCleaner(args.data_dir)
    
    # æ¸…ç†èˆŠæ•¸æ“š
    cleanup_result = cleaner.cleanup_old_data(args.retention_days, args.dry_run)
    
    # å£“ç¸®æ­¸æª”
    compress_result = cleaner.compress_archive(args.compress_days)
    
    # å„ªåŒ–å­˜å„²
    if args.optimize:
        optimize_result = cleaner.optimize_storage()
    
    # ç”Ÿæˆå ±å‘Š
    cleaner.generate_cleanup_report()
    
    print("âœ… æ•¸æ“šæ¸…ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()
