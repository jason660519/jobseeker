#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•¸æ“šé·ç§»è…³æœ¬
å°‡ç¾æœ‰çš„æ··äº‚æ•¸æ“šé‡æ–°çµ„ç¹”åˆ°æ–°çš„ç›®éŒ„çµæ§‹ä¸­
"""

import json
import shutil
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import pandas as pd

class DataMigrator:
    """æ•¸æ“šé·ç§»å™¨"""
    
    def __init__(self, source_dir: str = ".", target_dir: str = "data"):
        self.source_dir = Path(source_dir)
        self.target_dir = Path(target_dir)
        self.migration_log = []
    
    def migrate_all_data(self):
        """é·ç§»æ‰€æœ‰ç¾æœ‰æ•¸æ“š"""
        print("ğŸš€ é–‹å§‹æ•¸æ“šé·ç§»...")
        
        # 1. é·ç§»æ¸¬è©¦çµæœæ•¸æ“š
        self.migrate_test_results()
        
        # 2. é·ç§» Web App ä¸‹è¼‰æ•¸æ“š
        self.migrate_web_app_downloads()
        
        # 3. é·ç§»å…¶ä»–æ•£è½çš„æ•¸æ“šæ–‡ä»¶
        self.migrate_scattered_files()
        
        # 4. ç”Ÿæˆé·ç§»å ±å‘Š
        self.generate_migration_report()
        
        print("âœ… æ•¸æ“šé·ç§»å®Œæˆï¼")
    
    def migrate_test_results(self):
        """é·ç§»æ¸¬è©¦çµæœæ•¸æ“š"""
        print("ğŸ“ é·ç§»æ¸¬è©¦çµæœæ•¸æ“š...")
        
        test_results_dir = self.source_dir / "tests" / "results"
        if not test_results_dir.exists():
            print("âš ï¸  æ¸¬è©¦çµæœç›®éŒ„ä¸å­˜åœ¨ï¼Œè·³é")
            return
        
        migrated_count = 0
        
        for item in test_results_dir.iterdir():
            if item.is_dir():
                # è™•ç†æŒ‰æ™‚é–“æˆ³å‘½åçš„ç›®éŒ„
                if self.is_timestamp_directory(item.name):
                    self.migrate_timestamp_directory(item)
                    migrated_count += 1
                else:
                    # è™•ç†å…¶ä»–ç›®éŒ„
                    self.migrate_other_directory(item)
                    migrated_count += 1
            elif item.is_file() and item.suffix == '.json':
                # è™•ç†å–®å€‹ JSON æ–‡ä»¶
                self.migrate_raw_json_file(item, "unknown", datetime.now().strftime("%Y%m%d_%H%M%S"))
                migrated_count += 1
        
        print(f"âœ… å·²é·ç§» {migrated_count} å€‹æ¸¬è©¦çµæœé …ç›®")
    
    def is_timestamp_directory(self, dirname: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºæ™‚é–“æˆ³ç›®éŒ„"""
        # åŒ¹é…æ ¼å¼: search_term_location_YYYYMMDD_HHMMSS
        pattern = r'^[a-zA-Z0-9_]+_\d{8}_\d{6}$'
        return bool(re.match(pattern, dirname))
    
    def migrate_timestamp_directory(self, source_dir: Path):
        """é·ç§»æ™‚é–“æˆ³ç›®éŒ„"""
        print(f"  ğŸ“‚ é·ç§»ç›®éŒ„: {source_dir.name}")
        
        # è§£æç›®éŒ„å
        parts = source_dir.name.split('_')
        if len(parts) >= 3:
            search_term = '_'.join(parts[:-2])
            timestamp = '_'.join(parts[-2:])
        else:
            search_term = source_dir.name
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # è™•ç† raw_data ç›®éŒ„
        raw_data_dir = source_dir / "raw_data"
        if raw_data_dir.exists():
            for json_file in raw_data_dir.glob("*.json"):
                self.migrate_raw_json_file(json_file, search_term, timestamp)
        
        # è™•ç† csv_data ç›®éŒ„
        csv_data_dir = source_dir / "csv_data"
        if csv_data_dir.exists():
            for csv_file in csv_data_dir.glob("*.csv"):
                self.migrate_csv_file(csv_file, search_term, timestamp)
        
        # è™•ç† summary ç›®éŒ„
        summary_dir = source_dir / "summary"
        if summary_dir.exists():
            for json_file in summary_dir.glob("*.json"):
                self.migrate_raw_json_file(json_file, search_term, timestamp)
    
    def migrate_other_directory(self, source_dir: Path):
        """é·ç§»å…¶ä»–ç›®éŒ„"""
        print(f"  ğŸ“‚ é·ç§»å…¶ä»–ç›®éŒ„: {source_dir.name}")
        
        # ç°¡å–®åœ°è¤‡è£½æ•´å€‹ç›®éŒ„åˆ°æ­¸æª”ä½ç½®
        archive_dir = self.target_dir / "archive" / source_dir.name
        archive_dir.mkdir(parents=True, exist_ok=True)
        
        for item in source_dir.iterdir():
            if item.is_file():
                target_file = archive_dir / item.name
                shutil.copy2(item, target_file)
                
                self.migration_log.append({
                    "action": "migrate_other_file",
                    "source": str(item),
                    "target": str(target_file),
                    "timestamp": datetime.now().isoformat()
                })
    
    def migrate_raw_json_file(self, source_file: Path, search_term: str, timestamp: str):
        """é·ç§»åŸå§‹ JSON æ–‡ä»¶"""
        try:
            # è®€å–åŸå§‹æ•¸æ“š
            with open(source_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æå–ç¶²ç«™åç¨±
            site = self.extract_site_from_filename(source_file.name)
            
            # å‰µå»ºç›®æ¨™ç›®éŒ„
            target_date_dir = self.target_dir / "raw" / "by_date" / timestamp[:8] / site
            target_date_dir.mkdir(parents=True, exist_ok=True)
            
            target_site_dir = self.target_dir / "raw" / "by_site" / site
            target_site_dir.mkdir(parents=True, exist_ok=True)
            
            # æ¨™æº–åŒ–æ•¸æ“šæ ¼å¼
            standardized_data = self.standardize_json_data(data, site, search_term, timestamp)
            
            # ä¿å­˜åˆ°æ–°ä½ç½®
            filename = f"{site}_{search_term}_unknown_{timestamp}.json"
            
            # ä¿å­˜åˆ°æŒ‰æ—¥æœŸåˆ†é¡çš„ç›®éŒ„
            target_date_file = target_date_dir / filename
            with open(target_date_file, 'w', encoding='utf-8') as f:
                json.dump(standardized_data, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜åˆ°æŒ‰ç¶²ç«™åˆ†é¡çš„ç›®éŒ„
            target_site_file = target_site_dir / filename
            with open(target_site_file, 'w', encoding='utf-8') as f:
                json.dump(standardized_data, f, ensure_ascii=False, indent=2)
            
            self.migration_log.append({
                "action": "migrate_raw_json",
                "source": str(source_file),
                "target": str(target_date_file),
                "site": site,
                "search_term": search_term,
                "timestamp": timestamp
            })
            
        except Exception as e:
            print(f"âŒ é·ç§»æ–‡ä»¶å¤±æ•— {source_file}: {e}")
    
    def migrate_csv_file(self, source_file: Path, search_term: str, timestamp: str):
        """é·ç§» CSV æ–‡ä»¶"""
        try:
            # è®€å– CSV æ•¸æ“š
            df = pd.read_csv(source_file, encoding='utf-8')
            
            # æå–ç¶²ç«™åç¨±
            site = self.extract_site_from_filename(source_file.name)
            
            # å‰µå»ºç›®æ¨™ç›®éŒ„
            target_dir = self.target_dir / "processed" / "csv"
            target_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜åˆ°æ–°ä½ç½®
            filename = f"{site}_{search_term}_unknown_{timestamp}.csv"
            target_file = target_dir / filename
            
            df.to_csv(target_file, index=False, encoding='utf-8-sig')
            
            self.migration_log.append({
                "action": "migrate_csv",
                "source": str(source_file),
                "target": str(target_file),
                "site": site,
                "search_term": search_term,
                "timestamp": timestamp
            })
            
        except Exception as e:
            print(f"âŒ é·ç§» CSV æ–‡ä»¶å¤±æ•— {source_file}: {e}")
    
    def migrate_web_app_downloads(self):
        """é·ç§» Web App ä¸‹è¼‰æ•¸æ“š"""
        print("ğŸ“ é·ç§» Web App ä¸‹è¼‰æ•¸æ“š...")
        
        downloads_dir = self.source_dir / "web_app" / "downloads"
        if not downloads_dir.exists():
            print("âš ï¸  Web App ä¸‹è¼‰ç›®éŒ„ä¸å­˜åœ¨ï¼Œè·³é")
            return
        
        migrated_count = 0
        
        for file_path in downloads_dir.iterdir():
            if file_path.is_file():
                self.migrate_download_file(file_path)
                migrated_count += 1
        
        print(f"âœ… å·²é·ç§» {migrated_count} å€‹ä¸‹è¼‰æ–‡ä»¶")
    
    def migrate_download_file(self, source_file: Path):
        """é·ç§»ä¸‹è¼‰æ–‡ä»¶"""
        try:
            # è§£ææ–‡ä»¶å
            filename_parts = source_file.stem.split('_')
            if len(filename_parts) >= 3:
                search_term = filename_parts[1]
                timestamp = filename_parts[2]
            else:
                search_term = "unknown"
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # å‰µå»ºç›®æ¨™ç›®éŒ„
            format_type = source_file.suffix[1:]  # ç§»é™¤é»è™Ÿ
            target_dir = self.target_dir / "exports" / format_type
            target_dir.mkdir(parents=True, exist_ok=True)
            
            # è¤‡è£½æ–‡ä»¶
            target_file = target_dir / source_file.name
            shutil.copy2(source_file, target_file)
            
            self.migration_log.append({
                "action": "migrate_download",
                "source": str(source_file),
                "target": str(target_file),
                "search_term": search_term,
                "timestamp": timestamp,
                "format": format_type
            })
            
        except Exception as e:
            print(f"âŒ é·ç§»ä¸‹è¼‰æ–‡ä»¶å¤±æ•— {source_file}: {e}")
    
    def migrate_scattered_files(self):
        """é·ç§»æ•£è½çš„æ•¸æ“šæ–‡ä»¶"""
        print("ğŸ“ é·ç§»æ•£è½çš„æ•¸æ“šæ–‡ä»¶...")
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ•¸æ“šæ–‡ä»¶
        data_patterns = [
            "**/*_jobs_*.json",
            "**/*_results_*.json", 
            "**/*_data_*.json",
            "**/*_jobs_*.csv",
            "**/*_results_*.csv",
            "**/*_data_*.csv"
        ]
        
        migrated_count = 0
        
        for pattern in data_patterns:
            for file_path in self.source_dir.glob(pattern):
                # è·³éå·²ç¶“åœ¨ç›®æ¨™ç›®éŒ„ä¸­çš„æ–‡ä»¶
                if str(file_path).startswith(str(self.target_dir)):
                    continue
                
                # è·³éå·²ç¶“è™•ç†éçš„ç›®éŒ„
                if file_path.parent.name in ["raw", "processed", "exports", "reports"]:
                    continue
                
                self.migrate_scattered_file(file_path)
                migrated_count += 1
        
        print(f"âœ… å·²é·ç§» {migrated_count} å€‹æ•£è½æ–‡ä»¶")
    
    def migrate_scattered_file(self, source_file: Path):
        """é·ç§»æ•£è½æ–‡ä»¶"""
        try:
            # æ ¹æ“šæ–‡ä»¶é¡å‹æ±ºå®šç›®æ¨™ç›®éŒ„
            if source_file.suffix == '.json':
                target_dir = self.target_dir / "raw" / "by_site" / "unknown"
            elif source_file.suffix == '.csv':
                target_dir = self.target_dir / "processed" / "csv"
            else:
                return
            
            target_dir.mkdir(parents=True, exist_ok=True)
            
            # è¤‡è£½æ–‡ä»¶
            target_file = target_dir / source_file.name
            shutil.copy2(source_file, target_file)
            
            self.migration_log.append({
                "action": "migrate_scattered",
                "source": str(source_file),
                "target": str(target_file),
                "file_type": source_file.suffix
            })
            
        except Exception as e:
            print(f"âŒ é·ç§»æ•£è½æ–‡ä»¶å¤±æ•— {source_file}: {e}")
    
    def extract_site_from_filename(self, filename: str) -> str:
        """å¾æ–‡ä»¶åæå–ç¶²ç«™åç¨±"""
        # å¸¸è¦‹çš„ç¶²ç«™åç¨±æ˜ å°„
        site_mapping = {
            "linkedin": "linkedin",
            "indeed": "indeed", 
            "glassdoor": "glassdoor",
            "tw104": "tw104",
            "tw1111": "tw1111",
            "seek": "seek",
            "ziprecruiter": "ziprecruiter",
            "google": "google"
        }
        
        filename_lower = filename.lower()
        for site_key, site_name in site_mapping.items():
            if site_key in filename_lower:
                return site_name
        
        return "unknown"
    
    def standardize_json_data(self, data: Dict, site: str, search_term: str, timestamp: str) -> Dict:
        """æ¨™æº–åŒ– JSON æ•¸æ“šæ ¼å¼"""
        # æª¢æŸ¥æ˜¯å¦å·²ç¶“æ˜¯æ¨™æº–æ ¼å¼
        if "metadata" in data and "jobs" in data:
            return data
        
        # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
        standardized = {
            "metadata": {
                "scrape_id": f"migrated_{timestamp}",
                "site": site,
                "search_term": search_term,
                "location": "unknown",
                "timestamp": timestamp,
                "total_records": len(data.get("data", [])),
                "scraper_version": "1.0.0",
                "created_at": datetime.now().isoformat(),
                "migrated": True
            },
            "jobs": data.get("data", [])
        }
        
        return standardized
    
    def generate_migration_report(self):
        """ç”Ÿæˆé·ç§»å ±å‘Š"""
        report_data = {
            "migration_info": {
                "migrated_at": datetime.now().isoformat(),
                "total_migrations": len(self.migration_log),
                "source_directory": str(self.source_dir),
                "target_directory": str(self.target_dir)
            },
            "migration_log": self.migration_log,
            "summary": self.get_migration_summary()
        }
        
        report_path = self.target_dir / "migration_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š é·ç§»å ±å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    def get_migration_summary(self) -> Dict:
        """ç²å–é·ç§»æ‘˜è¦"""
        summary = {
            "by_action": {},
            "by_site": {},
            "by_format": {}
        }
        
        for log_entry in self.migration_log:
            action = log_entry.get("action", "unknown")
            site = log_entry.get("site", "unknown")
            format_type = log_entry.get("format", "unknown")
            
            summary["by_action"][action] = summary["by_action"].get(action, 0) + 1
            summary["by_site"][site] = summary["by_site"].get(site, 0) + 1
            summary["by_format"][format_type] = summary["by_format"].get(format_type, 0) + 1
        
        return summary

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹ JobSpy æ•¸æ“šé·ç§»...")
    
    migrator = DataMigrator()
    migrator.migrate_all_data()
    
    print("âœ… æ•¸æ“šé·ç§»å®Œæˆï¼")
    print("ğŸ“ æ–°çš„æ•¸æ“šç›®éŒ„çµæ§‹å·²å‰µå»ºåœ¨ 'data/' ç›®éŒ„ä¸­")
    print("ğŸ“Š è©³ç´°çš„é·ç§»å ±å‘Šè«‹æŸ¥çœ‹ 'data/migration_report.json'")

if __name__ == "__main__":
    main()
