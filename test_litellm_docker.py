#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LiteLLM Docker ä»£ç†æœå‹™å™¨æ¸¬è©¦è…³æœ¬
ç”¨æ–¼æ¸¬è©¦é€šéDockeréƒ¨ç½²çš„LiteLLMä»£ç†æœå‹™å™¨åŠŸèƒ½
"""

import requests
import json
import time
from typing import Dict, Any, Optional

class LiteLLMDockerClient:
    """
    LiteLLM Docker ä»£ç†å®¢æˆ¶ç«¯
    ç”¨æ–¼èˆ‡Dockeréƒ¨ç½²çš„LiteLLMä»£ç†æœå‹™å™¨é€²è¡Œé€šä¿¡
    """
    
    def __init__(self, base_url: str = "http://localhost:4000", master_key: Optional[str] = None):
        """
        åˆå§‹åŒ–å®¢æˆ¶ç«¯
        
        Args:
            base_url: LiteLLMä»£ç†æœå‹™å™¨çš„åŸºç¤URL
            master_key: ä¸»å¯†é‘°ï¼ˆå¦‚æœè¨­ç½®äº†çš„è©±ï¼‰
        """
        self.base_url = base_url.rstrip('/')
        self.headers = {
            "Content-Type": "application/json"
        }
        if master_key:
            self.headers["Authorization"] = f"Bearer {master_key}"
    
    def health_check(self) -> Dict[str, Any]:
        """
        æª¢æŸ¥ä»£ç†æœå‹™å™¨å¥åº·ç‹€æ…‹
        
        Returns:
            Dict[str, Any]: å¥åº·æª¢æŸ¥çµæœ
        """
        try:
            response = requests.get(f"{self.base_url}/health", headers=self.headers, timeout=10)
            return {
                "status": "healthy" if response.status_code == 200 else "unhealthy",
                "status_code": response.status_code,
                "response": response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    def list_models(self) -> Dict[str, Any]:
        """
        ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        
        Returns:
            Dict[str, Any]: æ¨¡å‹åˆ—è¡¨
        """
        try:
            response = requests.get(f"{self.base_url}/v1/models", headers=self.headers, timeout=10)
            return {
                "status": "success" if response.status_code == 200 else "error",
                "status_code": response.status_code,
                "models": response.json() if response.status_code == 200 else None,
                "error": response.text if response.status_code != 200 else None
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    def chat_completion(self, model: str, messages: list, **kwargs) -> Dict[str, Any]:
        """
        ç™¼é€èŠå¤©å®Œæˆè«‹æ±‚
        
        Args:
            model: æ¨¡å‹åç¨±
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–åƒæ•¸
        
        Returns:
            Dict[str, Any]: èŠå¤©å®Œæˆçµæœ
        """
        payload = {
            "model": model,
            "messages": messages,
            **kwargs
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/v1/chat/completions",
                headers=self.headers,
                json=payload,
                timeout=30
            )
            
            return {
                "status": "success" if response.status_code == 200 else "error",
                "status_code": response.status_code,
                "response": response.json() if response.status_code == 200 else None,
                "error": response.text if response.status_code != 200 else None
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }

def test_litellm_docker_proxy():
    """
    æ¸¬è©¦LiteLLM Dockerä»£ç†æœå‹™å™¨åŠŸèƒ½
    """
    print("=" * 60)
    print("ğŸ³ LiteLLM Docker ä»£ç†æœå‹™å™¨æ¸¬è©¦")
    print("=" * 60)
    
    # åˆå§‹åŒ–å®¢æˆ¶ç«¯
    client = LiteLLMDockerClient(
        base_url="http://localhost:4000",
        master_key="sk-litellm-master-key"
    )
    
    # 1. å¥åº·æª¢æŸ¥
    print("\nğŸ¥ 1. å¥åº·æª¢æŸ¥")
    print("-" * 40)
    health = client.health_check()
    if health["status"] == "healthy":
        print("âœ… ä»£ç†æœå‹™å™¨é‹è¡Œæ­£å¸¸")
        print(f"ğŸ“Š éŸ¿æ‡‰: {health.get('response', 'N/A')}")
    else:
        print("âŒ ä»£ç†æœå‹™å™¨ä¸å¯ç”¨")
        print(f"ğŸ” éŒ¯èª¤: {health.get('error', 'Unknown error')}")
        print("\nğŸ’¡ è«‹ç¢ºä¿Dockerå®¹å™¨æ­£åœ¨é‹è¡Œ:")
        print("   docker-compose -f docker-compose.litellm.yml up -d")
        return
    
    # 2. ç²å–æ¨¡å‹åˆ—è¡¨
    print("\nğŸ“‹ 2. å¯ç”¨æ¨¡å‹åˆ—è¡¨")
    print("-" * 40)
    models_result = client.list_models()
    if models_result["status"] == "success":
        models = models_result["models"].get("data", [])
        print(f"âœ… æ‰¾åˆ° {len(models)} å€‹å¯ç”¨æ¨¡å‹:")
        for model in models[:5]:  # åªé¡¯ç¤ºå‰5å€‹
            print(f"   ğŸ“ {model.get('id', 'Unknown')}")
        if len(models) > 5:
            print(f"   ... é‚„æœ‰ {len(models) - 5} å€‹æ¨¡å‹")
    else:
        print("âŒ ç„¡æ³•ç²å–æ¨¡å‹åˆ—è¡¨")
        print(f"ğŸ” éŒ¯èª¤: {models_result.get('error', 'Unknown error')}")
    
    # 3. æ¸¬è©¦èŠå¤©å®Œæˆ
    print("\nğŸ’¬ 3. èŠå¤©å®Œæˆæ¸¬è©¦")
    print("-" * 40)
    
    test_models = ["claude-3-haiku", "deepseek-chat", "gpt-3.5-turbo"]
    test_message = "ä½ å¥½ï¼Œè«‹ç”¨ä¸€å¥è©±ä»‹ç´¹ä½ è‡ªå·±ã€‚"
    
    for model in test_models:
        print(f"\nğŸ¤– æ¸¬è©¦æ¨¡å‹: {model}")
        result = client.chat_completion(
            model=model,
            messages=[{"role": "user", "content": test_message}],
            max_tokens=100,
            temperature=0.7
        )
        
        if result["status"] == "success":
            response_content = result["response"]["choices"][0]["message"]["content"]
            print(f"âœ… å›æ‡‰: {response_content[:100]}{'...' if len(response_content) > 100 else ''}")
        else:
            print(f"âŒ å¤±æ•—: {result.get('error', 'Unknown error')[:100]}")
    
    # 4. æ€§èƒ½æ¸¬è©¦
    print("\nâš¡ 4. æ€§èƒ½æ¸¬è©¦")
    print("-" * 40)
    
    start_time = time.time()
    concurrent_requests = 3
    successful_requests = 0
    
    for i in range(concurrent_requests):
        result = client.chat_completion(
            model="claude-3-haiku",
            messages=[{"role": "user", "content": f"æ¸¬è©¦è«‹æ±‚ #{i+1}"}],
            max_tokens=50
        )
        if result["status"] == "success":
            successful_requests += 1
    
    end_time = time.time()
    duration = end_time - start_time
    
    print(f"ğŸ“Š å®Œæˆ {concurrent_requests} å€‹è«‹æ±‚")
    print(f"âœ… æˆåŠŸ: {successful_requests}/{concurrent_requests}")
    print(f"â±ï¸  ç¸½æ™‚é–“: {duration:.2f} ç§’")
    print(f"ğŸš€ å¹³å‡éŸ¿æ‡‰æ™‚é–“: {duration/concurrent_requests:.2f} ç§’/è«‹æ±‚")
    
    # 5. ç¸½çµ
    print("\nğŸ“Š 5. æ¸¬è©¦ç¸½çµ")
    print("=" * 60)
    print("âœ… LiteLLM Dockerä»£ç†æœå‹™å™¨æ¸¬è©¦å®Œæˆ")
    print("ğŸ¯ ä¸»è¦åŠŸèƒ½:")
    print("   â€¢ çµ±ä¸€APIæ¥å£èª¿ç”¨å¤šå€‹LLMæä¾›å•†")
    print("   â€¢ æ”¯æ´è² è¼‰å‡è¡¡å’Œæ•…éšœè½‰ç§»")
    print("   â€¢ æä¾›æ¨™æº–OpenAIæ ¼å¼çš„éŸ¿æ‡‰")
    print("   â€¢ æ”¯æ´é€Ÿç‡é™åˆ¶å’Œä½¿ç”¨ç›£æ§")
    print("\nğŸ”— ä»£ç†æœå‹™å™¨åœ°å€: http://localhost:4000")
    print("ğŸ“š APIæ–‡æª”: http://localhost:4000/docs")
    print("\n" + "=" * 60)

def main():
    """
    ä¸»å‡½æ•¸
    """
    try:
        test_litellm_docker_proxy()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\n\nâŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()