# LLMæŒ‡ä»¤éµå¾ªèˆ‡çµæ§‹åŒ–è¼¸å‡ºæ¨™æº–åº«

[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Build Status](https://img.shields.io/badge/build-passing-brightgreen.svg)]()
[![Coverage](https://img.shields.io/badge/coverage-95%25-brightgreen.svg)]()

ä¸€å€‹çµ±ä¸€çš„Pythonåº«ï¼Œç”¨æ–¼å¯¦ç¾è·¨å¤šå€‹å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›å•†çš„æŒ‡ä»¤éµå¾ªå’Œçµæ§‹åŒ–è¼¸å‡ºæ¨™æº–åŒ–ã€‚

## ğŸŒŸ ç‰¹æ€§

- **ğŸ”„ çµ±ä¸€æ¥å£**: ç‚ºOpenAIã€Anthropicã€Googleã€DeepSeekç­‰æä¾›å•†æä¾›ä¸€è‡´çš„API
- **ğŸ“‹ æ¨™æº–åŒ–æŒ‡ä»¤**: å®šç¾©æ¸…æ™°çš„æŒ‡ä»¤æ ¼å¼å’Œè¼¸å‡ºçµæ§‹
- **âœ… è‡ªå‹•é©—è­‰**: å…§ç½®JSON Schemaé©—è­‰å’ŒéŸ¿æ‡‰æ ¼å¼æª¢æŸ¥
- **ğŸ”„ æ•…éšœè½‰ç§»**: è‡ªå‹•è² è¼‰å‡è¡¡å’Œæ•…éšœè½‰ç§»æ©Ÿåˆ¶
- **ğŸ“Š æ€§èƒ½ç›£æ§**: è©³ç´°çš„æ€§èƒ½æŒ‡æ¨™å’Œä½¿ç”¨çµ±è¨ˆ
- **ğŸ›¡ï¸ éŒ¯èª¤è™•ç†**: å®Œå–„çš„éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶
- **âš™ï¸ éˆæ´»é…ç½®**: æ”¯æŒå¤šç¨®é…ç½®æ–¹å¼å’Œè‡ªå®šç¾©é¸é …
- **ğŸš€ ç•°æ­¥æ”¯æŒ**: æ”¯æŒç•°æ­¥æ“ä½œå’Œæ‰¹é‡è™•ç†
- **ğŸ“ è±å¯Œæ—¥èªŒ**: çµæ§‹åŒ–æ—¥èªŒå’Œèª¿è©¦ä¿¡æ¯
- **ğŸ”§ å‘½ä»¤è¡Œå·¥å…·**: ä¾¿æ·çš„CLIå·¥å…·ç”¨æ–¼æ¸¬è©¦å’Œç®¡ç†

## ğŸ“¦ å®‰è£

### åŸºç¤å®‰è£

```bash
pip install llm-instruction-standard
```

### å®Œæ•´å®‰è£ï¼ˆåŒ…å«æ‰€æœ‰å¯é¸ä¾è³´ï¼‰

```bash
pip install llm-instruction-standard[all]
```

### é–‹ç™¼ç’°å¢ƒå®‰è£

```bash
git clone https://github.com/jobspy/llm-instruction-standard.git
cd llm-instruction-standard
pip install -e .[dev]
```

## ğŸš€ å¿«é€Ÿé–‹å§‹

### åŸºæœ¬ä½¿ç”¨

```python
from llm_standard import StandardLLMClient

# å‰µå»ºå®¢æˆ¶ç«¯
client = StandardLLMClient(
    provider="openai",
    api_key="your-api-key",
    model="gpt-4"
)

# å‰µå»ºæŒ‡ä»¤
instruction = client.create_instruction(
    instruction_type="structured_output",
    description="åˆ†æç”¨æˆ¶è©•è«–çš„æƒ…æ„Ÿ",
    output_schema={
        "type": "object",
        "properties": {
            "sentiment": {"type": "string", "enum": ["positive", "negative", "neutral"]},
            "confidence": {"type": "number", "minimum": 0, "maximum": 1},
            "keywords": {"type": "array", "items": {"type": "string"}}
        },
        "required": ["sentiment", "confidence"]
    }
)

# åŸ·è¡ŒæŒ‡ä»¤
response = client.execute(instruction, "é€™å€‹ç”¢å“çœŸçš„å¾ˆæ£’ï¼Œæˆ‘éå¸¸æ»¿æ„ï¼")

if response['status'] == 'success':
    data = response['data']
    print(f"æƒ…æ„Ÿ: {data['sentiment']}")
    print(f"ç½®ä¿¡åº¦: {data['confidence']}")
    print(f"é—œéµè©: {data.get('keywords', [])}")
else:
    print(f"éŒ¯èª¤: {response['errors']}")
```

### å®¢æˆ¶ç«¯æ± å’Œè² è¼‰å‡è¡¡

```python
from llm_standard import StandardLLMClient
from llm_standard.client import LLMClientPool

# å‰µå»ºå¤šå€‹å®¢æˆ¶ç«¯
clients = [
    StandardLLMClient(provider="openai", api_key="key1", model="gpt-4"),
    StandardLLMClient(provider="anthropic", api_key="key2", model="claude-3-haiku"),
    StandardLLMClient(provider="deepseek", api_key="key3", model="deepseek-chat")
]

# å‰µå»ºå®¢æˆ¶ç«¯æ± 
pool = LLMClientPool(clients)

# ä½¿ç”¨æ± åŸ·è¡ŒæŒ‡ä»¤ï¼ˆè‡ªå‹•è² è¼‰å‡è¡¡å’Œæ•…éšœè½‰ç§»ï¼‰
response = pool.execute(instruction, "åˆ†æé€™æ®µæ–‡æœ¬")
```

### æ‰¹é‡è™•ç†

```python
# æ‰¹é‡è™•ç†å¤šå€‹è¼¸å…¥
inputs = [
    "é€™å€‹ç”¢å“å¾ˆå¥½ç”¨",
    "æœå‹™æ…‹åº¦ä¸€èˆ¬",
    "åƒ¹æ ¼å¤ªè²´äº†",
    "è³ªé‡ä¸éŒ¯ï¼Œæ¨è–¦è³¼è²·"
]

responses = client.batch_execute(instruction, inputs)

for i, response in enumerate(responses):
    if response['status'] == 'success':
        print(f"è¼¸å…¥ {i+1}: {response['data']['sentiment']}")
```

## ğŸ“‹ æŒ‡ä»¤æ ¼å¼æ¨™æº–

### æ¨™æº–æŒ‡ä»¤çµæ§‹

```json
{
  "instruction_type": "structured_output",
  "version": "1.0",
  "task": {
    "description": "ä»»å‹™æè¿°",
    "context": "ä»»å‹™ä¸Šä¸‹æ–‡ï¼ˆå¯é¸ï¼‰",
    "constraints": ["ç´„æŸæ¢ä»¶1", "ç´„æŸæ¢ä»¶2"]
  },
  "output_schema": {
    "type": "object",
    "properties": {
      "field1": {"type": "string"},
      "field2": {"type": "integer"}
    },
    "required": ["field1"]
  },
  "examples": [
    {
      "input": "ç¤ºä¾‹è¼¸å…¥",
      "output": {"field1": "ç¤ºä¾‹è¼¸å‡º"}
    }
  ],
  "metadata": {
    "priority": "high",
    "timeout": 30,
    "retry_count": 3
  }
}
```

### æ”¯æŒçš„æŒ‡ä»¤é¡å‹

- `structured_output`: çµæ§‹åŒ–è¼¸å‡º
- `text_analysis`: æ–‡æœ¬åˆ†æ
- `extraction`: ä¿¡æ¯æå–
- `classification`: æ–‡æœ¬åˆ†é¡
- `summarization`: æ–‡æœ¬æ‘˜è¦
- `generation`: æ–‡æœ¬ç”Ÿæˆ
- `translation`: æ–‡æœ¬ç¿»è­¯
- `question_answering`: å•ç­”

## ğŸ”§ é…ç½®ç®¡ç†

### ç’°å¢ƒè®Šé‡é…ç½®

```bash
# OpenAIé…ç½®
export OPENAI_API_KEY="your-openai-key"
export OPENAI_MODEL="gpt-4"

# Anthropicé…ç½®
export ANTHROPIC_API_KEY="your-anthropic-key"
export ANTHROPIC_MODEL="claude-3-haiku-20240307"

# Googleé…ç½®
export GOOGLE_API_KEY="your-google-key"
export GOOGLE_MODEL="gemini-pro"

# DeepSeeké…ç½®
export DEEPSEEK_API_KEY="your-deepseek-key"
export DEEPSEEK_MODEL="deepseek-chat"

# å…¶ä»–é…ç½®
export LLM_LOG_LEVEL="INFO"
export LLM_CACHE_ENABLED="true"
export LLM_VALIDATE_STRICT="true"
```

### é…ç½®æ–‡ä»¶

```python
from llm_standard.config import StandardConfig

# å¾æ–‡ä»¶åŠ è¼‰é…ç½®
config = StandardConfig.from_file("config.json")

# å¾ç’°å¢ƒè®Šé‡åŠ è¼‰é…ç½®
config = StandardConfig.from_env()

# è¨­ç½®å…¨å±€é…ç½®
from llm_standard.config import set_config
set_config(config)
```

### é…ç½®æ–‡ä»¶ç¤ºä¾‹ (config.json)

```json
{
  "models": {
    "openai": {
      "provider": "openai",
      "model": "gpt-4",
      "api_key": "your-api-key",
      "max_tokens": 4000,
      "temperature": 0.7,
      "timeout": 30
    },
    "anthropic": {
      "provider": "anthropic",
      "model": "claude-3-haiku-20240307",
      "api_key": "your-api-key",
      "max_tokens": 4000,
      "temperature": 0.7
    }
  },
  "validation": {
    "validate_instructions": true,
    "validate_responses": true,
    "strict_schema": true
  },
  "logging": {
    "level": "INFO",
    "log_requests": false,
    "log_responses": false
  },
  "performance": {
    "enable_caching": true,
    "cache_ttl": 3600,
    "enable_metrics": true
  }
}
```

## ğŸ–¥ï¸ å‘½ä»¤è¡Œå·¥å…·

### å®‰è£å¾Œå¯ç”¨çš„CLIå‘½ä»¤

```bash
# æ¸¬è©¦é€£æ¥
llm-standard test-connection --provider openai --api-key your-key

# åŸ·è¡ŒæŒ‡ä»¤
llm-standard execute --provider openai --instruction-type structured_output \
  --description "åˆ†ææƒ…æ„Ÿ" --schema-file schema.json "é€™æ˜¯ä¸€æ®µæ¸¬è©¦æ–‡æœ¬"

# æ€§èƒ½åŸºæº–æ¸¬è©¦
llm-standard benchmark --providers openai anthropic --iterations 10

# åˆå§‹åŒ–é…ç½®æ–‡ä»¶
llm-standard init-config --output my-config.json

# é©—è­‰JSON Schema
llm-standard validate-schema --schema-file schema.json --data-file data.json
```

### CLIä½¿ç”¨ç¤ºä¾‹

```bash
# æ¸¬è©¦OpenAIé€£æ¥
llm-standard test-connection -p openai -k sk-your-api-key -m gpt-4

# åŸ·è¡Œæ–‡æœ¬åˆ†æ
llm-standard execute -p openai -t text_analysis \
  -d "åˆ†æé€™æ®µæ–‡æœ¬çš„æƒ…æ„Ÿ" \
  "æˆ‘å°é€™å€‹ç”¢å“éå¸¸æ»¿æ„ï¼Œè³ªé‡å¾ˆå¥½ï¼"

# æ‰¹é‡æ€§èƒ½æ¸¬è©¦
llm-standard benchmark -p openai anthropic deepseek -n 20 --concurrent 3
```

## ğŸ“Š æ€§èƒ½ç›£æ§

### å…§ç½®æŒ‡æ¨™

```python
# ç²å–å®¢æˆ¶ç«¯çµ±è¨ˆä¿¡æ¯
stats = client.get_stats()
print(f"ç¸½è«‹æ±‚æ•¸: {stats['total_requests']}")
print(f"æˆåŠŸç‡: {stats['success_rate']:.2%}")
print(f"å¹³å‡éŸ¿æ‡‰æ™‚é–“: {stats['avg_response_time']:.2f}ç§’")
print(f"ç¸½Tokenä½¿ç”¨: {stats['total_tokens']}")

# å¥åº·æª¢æŸ¥
health = client.health_check()
if health['status'] == 'healthy':
    print("å®¢æˆ¶ç«¯ç‹€æ…‹æ­£å¸¸")
else:
    print(f"å®¢æˆ¶ç«¯ç•°å¸¸: {health['error']}")
```

### æ€§èƒ½åˆ†æ

```python
# å•Ÿç”¨æ€§èƒ½åˆ†æ
client = StandardLLMClient(
    provider="openai",
    api_key="your-key",
    model="gpt-4",
    enable_profiling=True
)

# åŸ·è¡Œå¾ŒæŸ¥çœ‹æ€§èƒ½å ±å‘Š
response = client.execute(instruction, "æ¸¬è©¦æ–‡æœ¬")
profile = response['metadata']['performance_profile']
print(f"APIèª¿ç”¨æ™‚é–“: {profile['api_call_time']:.2f}ç§’")
print(f"é©—è­‰æ™‚é–“: {profile['validation_time']:.2f}ç§’")
print(f"ç¸½è™•ç†æ™‚é–“: {profile['total_time']:.2f}ç§’")
```

## ğŸ›¡ï¸ éŒ¯èª¤è™•ç†

### ç•°å¸¸é¡å‹

```python
from llm_standard.exceptions import (
    InvalidInputError,
    SchemaValidationError,
    TokenLimitExceededError,
    ProviderError,
    TimeoutError,
    RateLimitError
)

try:
    response = client.execute(instruction, "æ¸¬è©¦è¼¸å…¥")
except InvalidInputError as e:
    print(f"è¼¸å…¥ç„¡æ•ˆ: {e.message}")
    print(f"éŒ¯èª¤å­—æ®µ: {e.details['field']}")
except SchemaValidationError as e:
    print(f"Schemaé©—è­‰å¤±æ•—: {e.message}")
    for error in e.details['validation_errors']:
        print(f"  - {error['path']}: {error['message']}")
except TokenLimitExceededError as e:
    print(f"Tokené™åˆ¶è¶…å‡º: {e.details['current_tokens']}/{e.details['max_tokens']}")
except ProviderError as e:
    print(f"æä¾›å•†éŒ¯èª¤: {e.message}")
    print(f"ç‹€æ…‹ç¢¼: {e.details['status_code']}")
except TimeoutError as e:
    print(f"è«‹æ±‚è¶…æ™‚: {e.message}")
except RateLimitError as e:
    print(f"é€Ÿç‡é™åˆ¶: {e.message}")
    print(f"é‡è©¦æ™‚é–“: {e.details['retry_after']}ç§’")
```

### é‡è©¦æ©Ÿåˆ¶

```python
# é…ç½®é‡è©¦ç­–ç•¥
client = StandardLLMClient(
    provider="openai",
    api_key="your-key",
    model="gpt-4",
    retry_count=3,
    retry_delay=1.0,
    retry_on_timeout=True,
    retry_on_rate_limit=True
)
```

## ğŸ§ª æ¸¬è©¦

### é‹è¡Œæ¸¬è©¦

```bash
# é‹è¡Œæ‰€æœ‰æ¸¬è©¦
pytest

# é‹è¡Œç‰¹å®šæ¸¬è©¦
pytest tests/test_client.py

# é‹è¡Œæ¸¬è©¦ä¸¦ç”Ÿæˆè¦†è“‹ç‡å ±å‘Š
pytest --cov=llm_standard --cov-report=html

# é‹è¡Œæ€§èƒ½æ¸¬è©¦
pytest tests/test_performance.py -v
```

### æ¸¬è©¦é…ç½®

```python
# tests/conftest.py
import pytest
from llm_standard import StandardLLMClient

@pytest.fixture
def mock_client():
    return StandardLLMClient(
        provider="openai",
        api_key="test-key",
        model="gpt-4"
    )
```

## ğŸ“š é«˜ç´šç”¨æ³•

### è‡ªå®šç¾©é©é…å™¨

```python
from llm_standard.adapters import BaseLLMAdapter

class CustomAdapter(BaseLLMAdapter):
    def convert_instruction(self, instruction, user_input):
        # è‡ªå®šç¾©æŒ‡ä»¤è½‰æ›é‚è¼¯
        pass
    
    def call_api(self, converted_instruction):
        # è‡ªå®šç¾©APIèª¿ç”¨é‚è¼¯
        pass
    
    def parse_response(self, api_response):
        # è‡ªå®šç¾©éŸ¿æ‡‰è§£æé‚è¼¯
        pass

# è¨»å†Šè‡ªå®šç¾©é©é…å™¨
client = StandardLLMClient(
    provider="custom",
    adapter=CustomAdapter(),
    api_key="your-key"
)
```

### è‡ªå®šç¾©é©—è­‰å™¨

```python
from llm_standard.validators import SchemaValidator

class CustomValidator(SchemaValidator):
    def validate_custom_format(self, value, format_name):
        if format_name == "custom-id":
            return value.startswith("ID_") and len(value) == 10
        return super().validate_custom_format(value, format_name)

# ä½¿ç”¨è‡ªå®šç¾©é©—è­‰å™¨
validator = CustomValidator()
client = StandardLLMClient(
    provider="openai",
    api_key="your-key",
    validator=validator
)
```

### ç·©å­˜é…ç½®

```python
from llm_standard.cache import RedisCache

# ä½¿ç”¨Redisç·©å­˜
cache = RedisCache(
    host="localhost",
    port=6379,
    db=0,
    ttl=3600
)

client = StandardLLMClient(
    provider="openai",
    api_key="your-key",
    cache=cache
)
```

## ğŸ¤ è²¢ç»

æˆ‘å€‘æ­¡è¿ç¤¾å€è²¢ç»ï¼è«‹æŸ¥çœ‹ [CONTRIBUTING.md](CONTRIBUTING.md) äº†è§£å¦‚ä½•åƒèˆ‡ã€‚

### é–‹ç™¼ç’°å¢ƒè¨­ç½®

```bash
# å…‹éš†å€‰åº«
git clone https://github.com/jobspy/llm-instruction-standard.git
cd llm-instruction-standard

# å‰µå»ºè™›æ“¬ç’°å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# å®‰è£é–‹ç™¼ä¾è³´
pip install -e .[dev]

# å®‰è£pre-commité‰¤å­
pre-commit install

# é‹è¡Œæ¸¬è©¦
pytest
```

### ä»£ç¢¼è¦ç¯„

```bash
# ä»£ç¢¼æ ¼å¼åŒ–
black llm_standard/
isort llm_standard/

# ä»£ç¢¼æª¢æŸ¥
flake8 llm_standard/
mypy llm_standard/

# é‹è¡Œæ‰€æœ‰æª¢æŸ¥
pre-commit run --all-files
```

## ğŸ“„ è¨±å¯è­‰

æœ¬é …ç›®æ¡ç”¨ MIT è¨±å¯è­‰ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è©³æƒ…ã€‚

## ğŸ†˜ æ”¯æŒ

- ğŸ“– [æ–‡æª”](https://llm-instruction-standard.readthedocs.io/)
- ğŸ› [å•é¡Œè¿½è¹¤](https://github.com/jobspy/llm-instruction-standard/issues)
- ğŸ’¬ [è¨è«–å€](https://github.com/jobspy/llm-instruction-standard/discussions)
- ğŸ“§ [éƒµä»¶æ”¯æŒ](mailto:support@jobspy.com)

## ğŸ—ºï¸ è·¯ç·šåœ–

- [ ] æ”¯æŒæ›´å¤šLLMæä¾›å•†ï¼ˆCohereã€Hugging Faceç­‰ï¼‰
- [ ] å¯¦ç¾æµå¼éŸ¿æ‡‰æ”¯æŒ
- [ ] æ·»åŠ åœ–åƒå’Œå¤šæ¨¡æ…‹è¼¸å…¥æ”¯æŒ
- [ ] é–‹ç™¼Webç•Œé¢å’Œç®¡ç†æ§åˆ¶å°
- [ ] å¯¦ç¾åˆ†å¸ƒå¼éƒ¨ç½²æ”¯æŒ
- [ ] æ·»åŠ A/Bæ¸¬è©¦åŠŸèƒ½
- [ ] é›†æˆæ›´å¤šç›£æ§å’Œåˆ†æå·¥å…·

## ğŸ“Š çµ±è¨ˆ

![GitHub stars](https://img.shields.io/github/stars/jobspy/llm-instruction-standard?style=social)
![GitHub forks](https://img.shields.io/github/forks/jobspy/llm-instruction-standard?style=social)
![GitHub issues](https://img.shields.io/github/issues/jobspy/llm-instruction-standard)
![GitHub pull requests](https://img.shields.io/github/issues-pr/jobspy/llm-instruction-standard)

---

**ç”± [JobSpy Team](https://github.com/jobspy) é–‹ç™¼å’Œç¶­è­·**