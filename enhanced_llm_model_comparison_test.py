#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼·ç‰ˆLLMæ¨¡å‹æ¯”è¼ƒæ¸¬è©¦
æ“´å……æ›´å¤šæ¸¬è©¦æ¡ˆä¾‹ï¼Œæ¯”è¼ƒä¸åŒLLMæ¨¡å‹çš„è¼¸å‡ºè¡¨ç¾å·®ç•°

åŠŸèƒ½ç‰¹è‰²ï¼š
1. æ”¯æ´å¤šå€‹LLMæä¾›å•†çš„ä¸¦è¡Œæ¸¬è©¦
2. è©³ç´°çš„æ€§èƒ½æŒ‡æ¨™åˆ†æ
3. æ¨¡å‹ä¸€è‡´æ€§è©•ä¼°
4. é‚Šç•Œæ¡ˆä¾‹å’Œè¤‡é›œå ´æ™¯æ¸¬è©¦
5. ç”Ÿæˆè©³ç´°çš„æ¯”è¼ƒå ±å‘Š

Author: JobSpy Team
Date: 2025-01-27
"""

import sys
import os
import json
import time
import asyncio
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import statistics

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from jobseeker.llm_intent_analyzer import LLMIntentAnalyzer, LLMProvider
from jobseeker.llm_config import LLMConfigManager


@dataclass
class TestCase:
    """æ¸¬è©¦æ¡ˆä¾‹æ•¸æ“šçµæ§‹"""
    id: str
    category: str
    subcategory: str
    description: str
    query: str
    expected_job_related: bool
    difficulty_level: str  # easy, medium, hard, extreme
    language: str  # zh, en, mixed
    context_type: str  # explicit, implicit, ambiguous
    expected_confidence_range: Tuple[float, float]  # (min, max)
    tags: List[str]


@dataclass
class ModelTestResult:
    """å–®å€‹æ¨¡å‹æ¸¬è©¦çµæœ"""
    provider: str
    model_name: str
    test_id: str
    query: str
    expected: bool
    actual: bool
    confidence: float
    response_time: float
    passed: bool
    intent_type: str
    reasoning: str
    structured_intent: Dict[str, Any]
    error: Optional[str] = None


@dataclass
class ModelPerformanceMetrics:
    """æ¨¡å‹æ€§èƒ½æŒ‡æ¨™"""
    provider: str
    model_name: str
    total_tests: int
    passed_tests: int
    failed_tests: int
    accuracy: float
    avg_confidence: float
    avg_response_time: float
    consistency_score: float
    category_performance: Dict[str, float]
    difficulty_performance: Dict[str, float]
    language_performance: Dict[str, float]
    error_rate: float
    edge_case_handling: float


class EnhancedLLMModelComparisonTester:
    """å¢å¼·ç‰ˆLLMæ¨¡å‹æ¯”è¼ƒæ¸¬è©¦å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¸¬è©¦å™¨"""
        self.config_manager = LLMConfigManager()
        self.test_cases = self._create_comprehensive_test_cases()
        self.results: Dict[str, List[ModelTestResult]] = {}
        self.performance_metrics: Dict[str, ModelPerformanceMetrics] = {}
        
    def _create_comprehensive_test_cases(self) -> List[TestCase]:
        """å‰µå»ºå…¨é¢çš„æ¸¬è©¦æ¡ˆä¾‹"""
        test_cases = []
        
        # 1. åŸºç¤æ±‚è·æŸ¥è©¢ (Easy)
        basic_job_queries = [
            TestCase(
                id="basic_001",
                category="åŸºç¤æ±‚è·æŸ¥è©¢",
                subcategory="è·ä½+åœ°é»",
                description="æ¨™æº–è·ä½æœå°‹",
                query="Pythonå·¥ç¨‹å¸« å°åŒ—",
                expected_job_related=True,
                difficulty_level="easy",
                language="zh",
                context_type="explicit",
                expected_confidence_range=(0.8, 1.0),
                tags=["è·ä½", "åœ°é»", "ç¨‹å¼èªè¨€"]
            ),
            TestCase(
                id="basic_002",
                category="åŸºç¤æ±‚è·æŸ¥è©¢",
                subcategory="è·ä½+åœ°é»",
                description="è‹±æ–‡è·ä½æœå°‹",
                query="Software Engineer in Taipei",
                expected_job_related=True,
                difficulty_level="easy",
                language="en",
                context_type="explicit",
                expected_confidence_range=(0.8, 1.0),
                tags=["è·ä½", "åœ°é»", "è‹±æ–‡"]
            ),
            TestCase(
                id="basic_003",
                category="åŸºç¤æ±‚è·æŸ¥è©¢",
                subcategory="è·ä½+è–ªè³‡",
                description="åŒ…å«è–ªè³‡è¦æ±‚",
                query="å‰ç«¯å·¥ç¨‹å¸« æœˆè–ª60kä»¥ä¸Š",
                expected_job_related=True,
                difficulty_level="easy",
                language="zh",
                context_type="explicit",
                expected_confidence_range=(0.7, 1.0),
                tags=["è·ä½", "è–ªè³‡", "å‰ç«¯"]
            ),
            TestCase(
                id="basic_004",
                category="åŸºç¤æ±‚è·æŸ¥è©¢",
                subcategory="æŠ€èƒ½å°å‘",
                description="æŠ€èƒ½é—œéµå­—æœå°‹",
                query="Reacté–‹ç™¼è€… é ç¨‹å·¥ä½œ",
                expected_job_related=True,
                difficulty_level="easy",
                language="zh",
                context_type="explicit",
                expected_confidence_range=(0.7, 1.0),
                tags=["æŠ€èƒ½", "é ç¨‹", "æ¡†æ¶"]
            ),
            TestCase(
                id="basic_005",
                category="åŸºç¤æ±‚è·æŸ¥è©¢",
                subcategory="å¤šæŠ€èƒ½çµ„åˆ",
                description="å¤šå€‹æŠ€èƒ½çµ„åˆ",
                query="Python Django PostgreSQL å¾Œç«¯é–‹ç™¼",
                expected_job_related=True,
                difficulty_level="medium",
                language="zh",
                context_type="explicit",
                expected_confidence_range=(0.6, 0.9),
                tags=["å¤šæŠ€èƒ½", "å¾Œç«¯", "è³‡æ–™åº«"]
            )
        ]
        
        # 2. é€²éšæ±‚è·æŸ¥è©¢ (Medium)
        advanced_job_queries = [
            TestCase(
                id="advanced_001",
                category="é€²éšæ±‚è·æŸ¥è©¢",
                subcategory="è¤‡åˆæ¢ä»¶",
                description="å¤šé‡æ¢ä»¶çµ„åˆ",
                query="å°‹æ‰¾å°åŒ—çš„å…¨ç«¯å·¥ç¨‹å¸«è·ä½ï¼Œè¦æ±‚Reactå’ŒNode.jsç¶“é©—ï¼Œè–ªè³‡50kä»¥ä¸Šï¼Œå¯é ç¨‹å·¥ä½œ",
                expected_job_related=True,
                difficulty_level="medium",
                language="zh",
                context_type="explicit",
                expected_confidence_range=(0.7, 0.9),
                tags=["è¤‡åˆæ¢ä»¶", "å…¨ç«¯", "é ç¨‹", "è–ªè³‡"]
            ),
            TestCase(
                id="advanced_002",
                category="é€²éšæ±‚è·æŸ¥è©¢",
                subcategory="è·æ¶¯ç™¼å±•",
                description="è·æ¶¯è½‰æ›è«®è©¢",
                query="å¾å‚³çµ±è£½é€ æ¥­è½‰å…¥ç§‘æŠ€æ¥­çš„è»Ÿé«”å·¥ç¨‹å¸«è·ä½å»ºè­°",
                expected_job_related=True,
                difficulty_level="medium",
                language="zh",
                context_type="implicit",
                expected_confidence_range=(0.6, 0.8),
                tags=["è·æ¶¯è½‰æ›", "è«®è©¢", "è£½é€ æ¥­", "ç§‘æŠ€æ¥­"]
            ),
            TestCase(
                id="advanced_003",
                category="é€²éšæ±‚è·æŸ¥è©¢",
                subcategory="æ–°èˆˆæŠ€è¡“",
                description="AI/MLç›¸é—œè·ä½",
                query="æ©Ÿå™¨å­¸ç¿’å·¥ç¨‹å¸« TensorFlow PyTorch æ·±åº¦å­¸ç¿’ æ–°ç«¹ç§‘å­¸åœ’å€",
                expected_job_related=True,
                difficulty_level="medium",
                language="zh",
                context_type="explicit",
                expected_confidence_range=(0.7, 0.9),
                tags=["AI", "ML", "æ·±åº¦å­¸ç¿’", "ç§‘å­¸åœ’å€"]
            ),
            TestCase(
                id="advanced_004",
                category="é€²éšæ±‚è·æŸ¥è©¢",
                subcategory="åœ‹éš›åŒ–",
                description="è·¨åœ‹å…¬å¸è·ä½",
                query="Looking for DevOps engineer position in multinational company, AWS experience required, Taipei or remote",
                expected_job_related=True,
                difficulty_level="medium",
                language="en",
                context_type="explicit",
                expected_confidence_range=(0.7, 0.9),
                tags=["DevOps", "è·¨åœ‹", "AWS", "é›²ç«¯"]
            ),
            TestCase(
                id="advanced_005",
                category="é€²éšæ±‚è·æŸ¥è©¢",
                subcategory="å‰µæ¥­ç›¸é—œ",
                description="æ–°å‰µå…¬å¸è·ä½",
                query="æƒ³åŠ å…¥æ–°å‰µå…¬å¸æ“”ä»»ç”¢å“ç¶“ç†ï¼Œæœ‰é›»å•†èƒŒæ™¯ï¼Œå¸Œæœ›åœ¨å°åŒ—",
                expected_job_related=True,
                difficulty_level="medium",
                language="zh",
                context_type="implicit",
                expected_confidence_range=(0.6, 0.8),
                tags=["æ–°å‰µ", "ç”¢å“ç¶“ç†", "é›»å•†"]
            )
        ]
        
        # 3. å›°é›£é‚Šç•Œæ¡ˆä¾‹ (Hard)
        edge_cases = [
            TestCase(
                id="edge_001",
                category="é‚Šç•Œæ¡ˆä¾‹",
                subcategory="æ¨¡ç³Šæ„åœ–",
                description="åƒ…åœ°é»åç¨±",
                query="å°åŒ—",
                expected_job_related=False,
                difficulty_level="hard",
                language="zh",
                context_type="ambiguous",
                expected_confidence_range=(0.0, 0.3),
                tags=["åœ°é»", "æ¨¡ç³Š"]
            ),
            TestCase(
                id="edge_002",
                category="é‚Šç•Œæ¡ˆä¾‹",
                subcategory="å­¸ç¿’æ„åœ–",
                description="æŠ€èƒ½å­¸ç¿’æŸ¥è©¢",
                query="å¦‚ä½•å­¸ç¿’Pythonç¨‹å¼è¨­è¨ˆçš„æœ€ä½³æ–¹æ³•å’Œè³‡æºæ¨è–¦",
                expected_job_related=False,
                difficulty_level="hard",
                language="zh",
                context_type="ambiguous",
                expected_confidence_range=(0.0, 0.4),
                tags=["å­¸ç¿’", "æ•™è‚²", "è³‡æº"]
            ),
            TestCase(
                id="edge_003",
                category="é‚Šç•Œæ¡ˆä¾‹",
                subcategory="è–ªè³‡è¨è«–",
                description="ç´”è–ªè³‡è¨è«–",
                query="è»Ÿé«”å·¥ç¨‹å¸«çš„è–ªæ°´æ°´æº–å¦‚ä½•",
                expected_job_related=False,
                difficulty_level="hard",
                language="zh",
                context_type="ambiguous",
                expected_confidence_range=(0.2, 0.6),
                tags=["è–ªè³‡", "è¨è«–", "å¸‚å ´"]
            ),
            TestCase(
                id="edge_004",
                category="é‚Šç•Œæ¡ˆä¾‹",
                subcategory="æŠ€è¡“è¨è«–",
                description="ç´”æŠ€è¡“è¨è«–",
                query="Reactå’ŒVue.jsçš„å„ªç¼ºé»æ¯”è¼ƒ",
                expected_job_related=False,
                difficulty_level="hard",
                language="zh",
                context_type="ambiguous",
                expected_confidence_range=(0.0, 0.4),
                tags=["æŠ€è¡“", "æ¯”è¼ƒ", "æ¡†æ¶"]
            ),
            TestCase(
                id="edge_005",
                category="é‚Šç•Œæ¡ˆä¾‹",
                subcategory="æ··åˆèªè¨€",
                description="ä¸­è‹±æ··åˆæŸ¥è©¢",
                query="æƒ³æ‰¾Software Engineerçš„å·¥ä½œåœ¨å°åŒ—ï¼Œéœ€è¦æœƒPythonå’ŒJavaScript",
                expected_job_related=True,
                difficulty_level="hard",
                language="mixed",
                context_type="explicit",
                expected_confidence_range=(0.6, 0.8),
                tags=["æ··åˆèªè¨€", "è·ä½", "æŠ€èƒ½"]
            )
        ]
        
        # 4. æ¥µç«¯æ¸¬è©¦æ¡ˆä¾‹ (Extreme)
        extreme_cases = [
            TestCase(
                id="extreme_001",
                category="æ¥µç«¯æ¡ˆä¾‹",
                subcategory="è¶…é•·æŸ¥è©¢",
                description="éå¸¸è©³ç´°çš„æ±‚è·éœ€æ±‚",
                query="æˆ‘æ˜¯ä¸€å€‹æœ‰5å¹´ç¶“é©—çš„å…¨ç«¯å·¥ç¨‹å¸«ï¼Œç†Ÿæ‚‰Reactã€Vue.jsã€Node.jsã€Pythonã€Djangoã€PostgreSQLã€MongoDBã€AWSã€Dockerã€Kubernetesï¼Œç›®å‰åœ¨ä¸€å®¶å‚³çµ±é‡‘èå…¬å¸å·¥ä½œï¼Œæƒ³è½‰åˆ°æ–°å‰µç§‘æŠ€å…¬å¸ï¼Œå¸Œæœ›èƒ½æ‰¾åˆ°å°åŒ—æˆ–æ–°ç«¹çš„é ç¨‹å·¥ä½œæ©Ÿæœƒï¼Œè–ªè³‡æœŸæœ›åœ¨80k-120kä¹‹é–“ï¼Œå…¬å¸æ–‡åŒ–è¦é–‹æ”¾å‰µæ–°ï¼Œæœ‰å­¸ç¿’æˆé•·æ©Ÿæœƒï¼Œæœ€å¥½æ˜¯B2Cç”¢å“å°å‘çš„å…¬å¸",
                expected_job_related=True,
                difficulty_level="extreme",
                language="zh",
                context_type="explicit",
                expected_confidence_range=(0.8, 1.0),
                tags=["è¶…é•·", "è©³ç´°", "å¤šæŠ€èƒ½", "è½‰è·"]
            ),
            TestCase(
                id="extreme_002",
                category="æ¥µç«¯æ¡ˆä¾‹",
                subcategory="éš±å«æ„åœ–",
                description="é«˜åº¦éš±å«çš„æ±‚è·æ„åœ–",
                query="æœ€è¿‘å·¥ä½œå£“åŠ›å¾ˆå¤§ï¼Œæƒ³æ›å€‹ç’°å¢ƒï¼Œè½èªªç§‘æŠ€æ¥­å¾…é‡ä¸éŒ¯",
                expected_job_related=True,
                difficulty_level="extreme",
                language="zh",
                context_type="implicit",
                expected_confidence_range=(0.3, 0.7),
                tags=["éš±å«", "è½‰è·", "å£“åŠ›"]
            ),
            TestCase(
                id="extreme_003",
                category="æ¥µç«¯æ¡ˆä¾‹",
                subcategory="åå‘æŸ¥è©¢",
                description="ä¸æƒ³è¦çš„è·ä½é¡å‹",
                query="ä¸æƒ³åšå‰ç«¯é–‹ç™¼ï¼Œä¹Ÿä¸æƒ³åŠ ç­å¤ªå¤šï¼Œæœ‰ä»€éº¼å…¶ä»–çš„å·¥ç¨‹å¸«è·ä½æ¨è–¦",
                expected_job_related=True,
                difficulty_level="extreme",
                language="zh",
                context_type="implicit",
                expected_confidence_range=(0.4, 0.7),
                tags=["åå‘", "æ’é™¤", "æ¨è–¦"]
            ),
            TestCase(
                id="extreme_004",
                category="æ¥µç«¯æ¡ˆä¾‹",
                subcategory="æƒ…å¢ƒåŒ–æŸ¥è©¢",
                description="ç‰¹å®šæƒ…å¢ƒä¸‹çš„æ±‚è·",
                query="å‰›ç•¢æ¥­çš„è³‡å·¥ç³»å­¸ç”Ÿï¼Œæ²’æœ‰å¯¦ç¿’ç¶“é©—ï¼Œä½†æœ‰åšéä¸€äº›side projectï¼Œæƒ³æ‰¾entry levelçš„å·¥ä½œ",
                expected_job_related=True,
                difficulty_level="extreme",
                language="zh",
                context_type="implicit",
                expected_confidence_range=(0.6, 0.8),
                tags=["æ–°é®®äºº", "ç„¡ç¶“é©—", "side project"]
            ),
            TestCase(
                id="extreme_005",
                category="æ¥µç«¯æ¡ˆä¾‹",
                subcategory="å¤šé‡å¦å®š",
                description="è¤‡é›œçš„å¦å®šè¡¨é”",
                query="ä¸æ˜¯ä¸æƒ³æ‰¾å·¥ä½œï¼Œåªæ˜¯ä¸ç¢ºå®šç¾åœ¨çš„å¸‚å ´ç’°å¢ƒé©ä¸é©åˆè½‰è·",
                expected_job_related=True,
                difficulty_level="extreme",
                language="zh",
                context_type="ambiguous",
                expected_confidence_range=(0.2, 0.6),
                tags=["å¤šé‡å¦å®š", "ä¸ç¢ºå®š", "å¸‚å ´"]
            )
        ]
        
        # 5. éæ±‚è·ç›¸é—œæŸ¥è©¢
        non_job_queries = [
            TestCase(
                id="non_job_001",
                category="éæ±‚è·ç›¸é—œ",
                subcategory="å¤©æ°£æŸ¥è©¢",
                description="å¤©æ°£è³‡è¨Š",
                query="ä»Šå¤©å°åŒ—çš„å¤©æ°£å¦‚ä½•",
                expected_job_related=False,
                difficulty_level="easy",
                language="zh",
                context_type="explicit",
                expected_confidence_range=(0.0, 0.2),
                tags=["å¤©æ°£", "è³‡è¨Š"]
            ),
            TestCase(
                id="non_job_002",
                category="éæ±‚è·ç›¸é—œ",
                subcategory="å¨›æ¨‚æ¨è–¦",
                description="é›»å½±æ¨è–¦",
                query="æ¨è–¦ä¸€äº›å¥½çœ‹çš„ç§‘å¹»é›»å½±",
                expected_job_related=False,
                difficulty_level="easy",
                language="zh",
                context_type="explicit",
                expected_confidence_range=(0.0, 0.2),
                tags=["å¨›æ¨‚", "é›»å½±"]
            ),
            TestCase(
                id="non_job_003",
                category="éæ±‚è·ç›¸é—œ",
                subcategory="è³¼ç‰©æŸ¥è©¢",
                description="å•†å“è³¼è²·",
                query="å“ªè£¡å¯ä»¥è²·åˆ°ä¾¿å®œçš„MacBook Pro",
                expected_job_related=False,
                difficulty_level="easy",
                language="zh",
                context_type="explicit",
                expected_confidence_range=(0.0, 0.2),
                tags=["è³¼ç‰©", "é›»è…¦"]
            ),
            TestCase(
                id="non_job_004",
                category="éæ±‚è·ç›¸é—œ",
                subcategory="æ—…éŠè¦åŠƒ",
                description="æ—…éŠæ™¯é»",
                query="å°åŒ—æœ‰ä»€éº¼å¥½ç©çš„æ™¯é»å’Œç¾é£Ÿæ¨è–¦",
                expected_job_related=False,
                difficulty_level="medium",
                language="zh",
                context_type="explicit",
                expected_confidence_range=(0.0, 0.3),
                tags=["æ—…éŠ", "æ™¯é»", "ç¾é£Ÿ"]
            ),
            TestCase(
                id="non_job_005",
                category="éæ±‚è·ç›¸é—œ",
                subcategory="å¥åº·è«®è©¢",
                description="å¥åº·å•é¡Œ",
                query="æœ€è¿‘å¸¸å¸¸é ­ç—›ï¼Œå¯èƒ½æ˜¯ä»€éº¼åŸå› ",
                expected_job_related=False,
                difficulty_level="easy",
                language="zh",
                context_type="explicit",
                expected_confidence_range=(0.0, 0.2),
                tags=["å¥åº·", "é†«ç™‚"]
            )
        ]
        
        # 6. ç‰¹æ®Šèªè¨€å’Œæ ¼å¼æ¸¬è©¦
        special_format_tests = [
            TestCase(
                id="special_001",
                category="ç‰¹æ®Šæ ¼å¼",
                subcategory="è¡¨æƒ…ç¬¦è™Ÿ",
                description="åŒ…å«è¡¨æƒ…ç¬¦è™Ÿ",
                query="æ‰¾å·¥ä½œå¥½é›£ğŸ˜­ æœ‰æ²’æœ‰é©åˆæ–°æ‰‹çš„ç¨‹å¼è¨­è¨ˆå¸«è·ä½ğŸ¤”",
                expected_job_related=True,
                difficulty_level="medium",
                language="zh",
                context_type="explicit",
                expected_confidence_range=(0.6, 0.8),
                tags=["è¡¨æƒ…ç¬¦è™Ÿ", "æ–°æ‰‹"]
            ),
            TestCase(
                id="special_002",
                category="ç‰¹æ®Šæ ¼å¼",
                subcategory="ç¸®å¯«è¡“èª",
                description="æŠ€è¡“ç¸®å¯«",
                query="æ‰¾ML/AIç›¸é—œçš„SWEè·ä½ï¼Œprefer FAANGæˆ–unicorn startup",
                expected_job_related=True,
                difficulty_level="hard",
                language="mixed",
                context_type="explicit",
                expected_confidence_range=(0.6, 0.8),
                tags=["ç¸®å¯«", "è¡“èª", "FAANG"]
            ),
            TestCase(
                id="special_003",
                category="ç‰¹æ®Šæ ¼å¼",
                subcategory="æ•¸å­—ç¬¦è™Ÿ",
                description="åŒ…å«ç‰¹æ®Šç¬¦è™Ÿ",
                query="C++ / C# é–‹ç™¼è€… @ å°åŒ—ï¼Œè–ªè³‡ > 70K",
                expected_job_related=True,
                difficulty_level="medium",
                language="mixed",
                context_type="explicit",
                expected_confidence_range=(0.7, 0.9),
                tags=["ç¬¦è™Ÿ", "ç¨‹å¼èªè¨€"]
            )
        ]
        
        # åˆä½µæ‰€æœ‰æ¸¬è©¦æ¡ˆä¾‹
        all_test_cases = (
            basic_job_queries + 
            advanced_job_queries + 
            edge_cases + 
            extreme_cases + 
            non_job_queries + 
            special_format_tests
        )
        
        return all_test_cases
    
    def run_model_comparison_test(self, providers: Optional[List[LLMProvider]] = None) -> Dict[str, Any]:
        """é‹è¡Œæ¨¡å‹æ¯”è¼ƒæ¸¬è©¦"""
        print("ğŸš€ é–‹å§‹å¢å¼·ç‰ˆLLMæ¨¡å‹æ¯”è¼ƒæ¸¬è©¦")
        print("=" * 60)
        
        # ç¢ºå®šè¦æ¸¬è©¦çš„æä¾›å•†
        if providers is None:
            providers = self.config_manager.get_available_providers()
        
        if not providers:
            print("âŒ æ²’æœ‰å¯ç”¨çš„LLMæä¾›å•†")
            return {}
        
        print(f"ğŸ“‹ æ¸¬è©¦æä¾›å•†: {[p.value for p in providers]}")
        print(f"ğŸ“Š æ¸¬è©¦æ¡ˆä¾‹æ•¸é‡: {len(self.test_cases)}")
        print()
        
        # ç‚ºæ¯å€‹æä¾›å•†é‹è¡Œæ¸¬è©¦
        for provider in providers:
            print(f"\nğŸ”„ æ¸¬è©¦æä¾›å•†: {provider.value}")
            print("-" * 40)
            
            try:
                analyzer = LLMIntentAnalyzer(provider=provider)
                model_name = self.config_manager.get_config(provider).model_name or "default"
                
                provider_results = []
                
                for i, test_case in enumerate(self.test_cases, 1):
                    print(f"\ré€²åº¦: {i}/{len(self.test_cases)} ({i/len(self.test_cases)*100:.1f}%)", end="", flush=True)
                    
                    # åŸ·è¡Œå–®å€‹æ¸¬è©¦
                    result = self._run_single_test(analyzer, test_case, provider.value, model_name)
                    provider_results.append(result)
                
                print()  # æ›è¡Œ
                self.results[provider.value] = provider_results
                
                # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
                metrics = self._calculate_performance_metrics(provider.value, model_name, provider_results)
                self.performance_metrics[provider.value] = metrics
                
                print(f"âœ… {provider.value} æ¸¬è©¦å®Œæˆ")
                print(f"   æº–ç¢ºç‡: {metrics.accuracy:.1f}%")
                print(f"   å¹³å‡éŸ¿æ‡‰æ™‚é–“: {metrics.avg_response_time:.2f}s")
                print(f"   éŒ¯èª¤ç‡: {metrics.error_rate:.1f}%")
                
            except Exception as e:
                print(f"âŒ {provider.value} æ¸¬è©¦å¤±æ•—: {str(e)}")
                continue
        
        # ç”Ÿæˆæ¯”è¼ƒå ±å‘Š
        comparison_report = self._generate_comparison_report()
        
        # ä¿å­˜çµæœ
        self._save_results(comparison_report)
        
        return comparison_report
    
    def _run_single_test(self, analyzer: LLMIntentAnalyzer, test_case: TestCase, 
                        provider: str, model_name: str) -> ModelTestResult:
        """é‹è¡Œå–®å€‹æ¸¬è©¦æ¡ˆä¾‹"""
        start_time = time.time()
        
        try:
            # åŸ·è¡Œæ„åœ–åˆ†æ
            result = analyzer.analyze_intent(test_case.query)
            
            response_time = time.time() - start_time
            
            # æª¢æŸ¥çµæœ
            actual_job_related = result.is_job_related
            passed = actual_job_related == test_case.expected_job_related
            
            # æå–çµæ§‹åŒ–æ„åœ–
            structured_intent = {}
            if result.structured_intent and result.is_job_related:
                intent = result.structured_intent
                structured_intent = {
                    "job_titles": getattr(intent, 'job_titles', []),
                    "skills": getattr(intent, 'skills', []),
                    "locations": getattr(intent, 'locations', []),
                    "salary_range": getattr(intent, 'salary_range', None),
                    "work_type": getattr(intent, 'work_type', None)
                }
            
            return ModelTestResult(
                provider=provider,
                model_name=model_name,
                test_id=test_case.id,
                query=test_case.query,
                expected=test_case.expected_job_related,
                actual=actual_job_related,
                confidence=result.confidence,
                response_time=response_time,
                passed=passed,
                intent_type=result.intent_type.value if hasattr(result.intent_type, 'value') else str(result.intent_type),
                reasoning=getattr(result, 'llm_reasoning', ''),
                structured_intent=structured_intent
            )
            
        except Exception as e:
            response_time = time.time() - start_time
            return ModelTestResult(
                provider=provider,
                model_name=model_name,
                test_id=test_case.id,
                query=test_case.query,
                expected=test_case.expected_job_related,
                actual=False,
                confidence=0.0,
                response_time=response_time,
                passed=False,
                intent_type="error",
                reasoning="",
                structured_intent={},
                error=str(e)
            )
    
    def _calculate_performance_metrics(self, provider: str, model_name: str, 
                                     results: List[ModelTestResult]) -> ModelPerformanceMetrics:
        """è¨ˆç®—æ€§èƒ½æŒ‡æ¨™"""
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r.passed)
        failed_tests = total_tests - passed_tests
        accuracy = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        
        # è¨ˆç®—å¹³å‡ç½®ä¿¡åº¦å’ŒéŸ¿æ‡‰æ™‚é–“
        valid_results = [r for r in results if r.error is None]
        avg_confidence = statistics.mean([r.confidence for r in valid_results]) if valid_results else 0
        avg_response_time = statistics.mean([r.response_time for r in results])
        
        # éŒ¯èª¤ç‡
        error_count = sum(1 for r in results if r.error is not None)
        error_rate = (error_count / total_tests) * 100 if total_tests > 0 else 0
        
        # æŒ‰é¡åˆ¥è¨ˆç®—æ€§èƒ½
        category_performance = self._calculate_category_performance(results)
        
        # æŒ‰é›£åº¦è¨ˆç®—æ€§èƒ½
        difficulty_performance = self._calculate_difficulty_performance(results)
        
        # æŒ‰èªè¨€è¨ˆç®—æ€§èƒ½
        language_performance = self._calculate_language_performance(results)
        
        # è¨ˆç®—ä¸€è‡´æ€§åˆ†æ•¸ï¼ˆç½®ä¿¡åº¦èˆ‡å¯¦éš›çµæœçš„ä¸€è‡´æ€§ï¼‰
        consistency_score = self._calculate_consistency_score(results)
        
        # é‚Šç•Œæ¡ˆä¾‹è™•ç†èƒ½åŠ›
        edge_case_handling = self._calculate_edge_case_handling(results)
        
        return ModelPerformanceMetrics(
            provider=provider,
            model_name=model_name,
            total_tests=total_tests,
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            accuracy=accuracy,
            avg_confidence=avg_confidence,
            avg_response_time=avg_response_time,
            consistency_score=consistency_score,
            category_performance=category_performance,
            difficulty_performance=difficulty_performance,
            language_performance=language_performance,
            error_rate=error_rate,
            edge_case_handling=edge_case_handling
        )
    
    def _calculate_category_performance(self, results: List[ModelTestResult]) -> Dict[str, float]:
        """è¨ˆç®—å„é¡åˆ¥æ€§èƒ½"""
        category_stats = {}
        
        for result in results:
            test_case = next((tc for tc in self.test_cases if tc.id == result.test_id), None)
            if test_case:
                category = test_case.category
                if category not in category_stats:
                    category_stats[category] = {'total': 0, 'passed': 0}
                
                category_stats[category]['total'] += 1
                if result.passed:
                    category_stats[category]['passed'] += 1
        
        return {
            category: (stats['passed'] / stats['total']) * 100 if stats['total'] > 0 else 0
            for category, stats in category_stats.items()
        }
    
    def _calculate_difficulty_performance(self, results: List[ModelTestResult]) -> Dict[str, float]:
        """è¨ˆç®—å„é›£åº¦ç´šåˆ¥æ€§èƒ½"""
        difficulty_stats = {}
        
        for result in results:
            test_case = next((tc for tc in self.test_cases if tc.id == result.test_id), None)
            if test_case:
                difficulty = test_case.difficulty_level
                if difficulty not in difficulty_stats:
                    difficulty_stats[difficulty] = {'total': 0, 'passed': 0}
                
                difficulty_stats[difficulty]['total'] += 1
                if result.passed:
                    difficulty_stats[difficulty]['passed'] += 1
        
        return {
            difficulty: (stats['passed'] / stats['total']) * 100 if stats['total'] > 0 else 0
            for difficulty, stats in difficulty_stats.items()
        }
    
    def _calculate_language_performance(self, results: List[ModelTestResult]) -> Dict[str, float]:
        """è¨ˆç®—å„èªè¨€æ€§èƒ½"""
        language_stats = {}
        
        for result in results:
            test_case = next((tc for tc in self.test_cases if tc.id == result.test_id), None)
            if test_case:
                language = test_case.language
                if language not in language_stats:
                    language_stats[language] = {'total': 0, 'passed': 0}
                
                language_stats[language]['total'] += 1
                if result.passed:
                    language_stats[language]['passed'] += 1
        
        return {
            language: (stats['passed'] / stats['total']) * 100 if stats['total'] > 0 else 0
            for language, stats in language_stats.items()
        }
    
    def _calculate_consistency_score(self, results: List[ModelTestResult]) -> float:
        """è¨ˆç®—ä¸€è‡´æ€§åˆ†æ•¸ï¼ˆç½®ä¿¡åº¦èˆ‡çµæœçš„ä¸€è‡´æ€§ï¼‰"""
        valid_results = [r for r in results if r.error is None]
        if not valid_results:
            return 0.0
        
        consistency_scores = []
        for result in valid_results:
            # å¦‚æœé æ¸¬æ­£ç¢ºä¸”ç½®ä¿¡åº¦é«˜ï¼Œæˆ–é æ¸¬éŒ¯èª¤ä¸”ç½®ä¿¡åº¦ä½ï¼Œå‰‡ä¸€è‡´æ€§é«˜
            if result.passed:
                consistency = result.confidence
            else:
                consistency = 1.0 - result.confidence
            
            consistency_scores.append(consistency)
        
        return statistics.mean(consistency_scores) * 100
    
    def _calculate_edge_case_handling(self, results: List[ModelTestResult]) -> float:
        """è¨ˆç®—é‚Šç•Œæ¡ˆä¾‹è™•ç†èƒ½åŠ›"""
        edge_case_results = []
        
        for result in results:
            test_case = next((tc for tc in self.test_cases if tc.id == result.test_id), None)
            if test_case and test_case.category == "é‚Šç•Œæ¡ˆä¾‹":
                edge_case_results.append(result)
        
        if not edge_case_results:
            return 0.0
        
        passed_edge_cases = sum(1 for r in edge_case_results if r.passed)
        return (passed_edge_cases / len(edge_case_results)) * 100
    
    def _generate_comparison_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ¯”è¼ƒå ±å‘Š"""
        if not self.performance_metrics:
            return {}
        
        # æ•´é«”çµ±è¨ˆ
        providers = list(self.performance_metrics.keys())
        
        # æ‰¾å‡ºæœ€ä½³è¡¨ç¾çš„æ¨¡å‹
        best_accuracy = max(self.performance_metrics.values(), key=lambda x: x.accuracy)
        best_speed = min(self.performance_metrics.values(), key=lambda x: x.avg_response_time)
        best_consistency = max(self.performance_metrics.values(), key=lambda x: x.consistency_score)
        best_edge_case = max(self.performance_metrics.values(), key=lambda x: x.edge_case_handling)
        
        # è¨ˆç®—å¹³å‡æŒ‡æ¨™
        avg_accuracy = statistics.mean([m.accuracy for m in self.performance_metrics.values()])
        avg_response_time = statistics.mean([m.avg_response_time for m in self.performance_metrics.values()])
        avg_consistency = statistics.mean([m.consistency_score for m in self.performance_metrics.values()])
        
        # ç”Ÿæˆè©³ç´°æ¯”è¼ƒ
        detailed_comparison = self._generate_detailed_comparison()
        
        return {
            "test_summary": {
                "total_providers": len(providers),
                "total_test_cases": len(self.test_cases),
                "test_categories": list(set(tc.category for tc in self.test_cases)),
                "difficulty_levels": list(set(tc.difficulty_level for tc in self.test_cases)),
                "languages": list(set(tc.language for tc in self.test_cases)),
                "timestamp": datetime.now().isoformat()
            },
            "performance_summary": {
                "average_accuracy": avg_accuracy,
                "average_response_time": avg_response_time,
                "average_consistency": avg_consistency,
                "best_accuracy_model": {
                    "provider": best_accuracy.provider,
                    "accuracy": best_accuracy.accuracy
                },
                "fastest_model": {
                    "provider": best_speed.provider,
                    "response_time": best_speed.avg_response_time
                },
                "most_consistent_model": {
                    "provider": best_consistency.provider,
                    "consistency_score": best_consistency.consistency_score
                },
                "best_edge_case_model": {
                    "provider": best_edge_case.provider,
                    "edge_case_score": best_edge_case.edge_case_handling
                }
            },
            "detailed_metrics": {
                provider: asdict(metrics) for provider, metrics in self.performance_metrics.items()
            },
            "detailed_comparison": detailed_comparison,
            "recommendations": self._generate_recommendations()
        }
    
    def _generate_detailed_comparison(self) -> Dict[str, Any]:
        """ç”Ÿæˆè©³ç´°æ¯”è¼ƒåˆ†æ"""
        comparison = {
            "accuracy_ranking": [],
            "speed_ranking": [],
            "consistency_ranking": [],
            "category_comparison": {},
            "difficulty_comparison": {},
            "language_comparison": {}
        }
        
        # æº–ç¢ºç‡æ’å
        accuracy_sorted = sorted(self.performance_metrics.items(), 
                                key=lambda x: x[1].accuracy, reverse=True)
        comparison["accuracy_ranking"] = [
            {"provider": provider, "accuracy": metrics.accuracy}
            for provider, metrics in accuracy_sorted
        ]
        
        # é€Ÿåº¦æ’å
        speed_sorted = sorted(self.performance_metrics.items(), 
                             key=lambda x: x[1].avg_response_time)
        comparison["speed_ranking"] = [
            {"provider": provider, "response_time": metrics.avg_response_time}
            for provider, metrics in speed_sorted
        ]
        
        # ä¸€è‡´æ€§æ’å
        consistency_sorted = sorted(self.performance_metrics.items(), 
                                   key=lambda x: x[1].consistency_score, reverse=True)
        comparison["consistency_ranking"] = [
            {"provider": provider, "consistency_score": metrics.consistency_score}
            for provider, metrics in consistency_sorted
        ]
        
        # é¡åˆ¥æ¯”è¼ƒ
        all_categories = set()
        for metrics in self.performance_metrics.values():
            all_categories.update(metrics.category_performance.keys())
        
        for category in all_categories:
            comparison["category_comparison"][category] = {
                provider: metrics.category_performance.get(category, 0)
                for provider, metrics in self.performance_metrics.items()
            }
        
        # é›£åº¦æ¯”è¼ƒ
        all_difficulties = set()
        for metrics in self.performance_metrics.values():
            all_difficulties.update(metrics.difficulty_performance.keys())
        
        for difficulty in all_difficulties:
            comparison["difficulty_comparison"][difficulty] = {
                provider: metrics.difficulty_performance.get(difficulty, 0)
                for provider, metrics in self.performance_metrics.items()
            }
        
        # èªè¨€æ¯”è¼ƒ
        all_languages = set()
        for metrics in self.performance_metrics.values():
            all_languages.update(metrics.language_performance.keys())
        
        for language in all_languages:
            comparison["language_comparison"][language] = {
                provider: metrics.language_performance.get(language, 0)
                for provider, metrics in self.performance_metrics.items()
            }
        
        return comparison
    
    def _generate_recommendations(self) -> Dict[str, Any]:
        """ç”Ÿæˆä½¿ç”¨å»ºè­°"""
        if not self.performance_metrics:
            return {}
        
        recommendations = {
            "best_overall": "",
            "best_for_accuracy": "",
            "best_for_speed": "",
            "best_for_consistency": "",
            "best_for_edge_cases": "",
            "usage_scenarios": {},
            "improvement_suggestions": []
        }
        
        # æ‰¾å‡ºå„é …æœ€ä½³æ¨¡å‹
        best_accuracy = max(self.performance_metrics.items(), key=lambda x: x[1].accuracy)
        best_speed = min(self.performance_metrics.items(), key=lambda x: x[1].avg_response_time)
        best_consistency = max(self.performance_metrics.items(), key=lambda x: x[1].consistency_score)
        best_edge_case = max(self.performance_metrics.items(), key=lambda x: x[1].edge_case_handling)
        
        recommendations["best_for_accuracy"] = f"{best_accuracy[0]} (æº–ç¢ºç‡: {best_accuracy[1].accuracy:.1f}%)"
        recommendations["best_for_speed"] = f"{best_speed[0]} (éŸ¿æ‡‰æ™‚é–“: {best_speed[1].avg_response_time:.2f}s)"
        recommendations["best_for_consistency"] = f"{best_consistency[0]} (ä¸€è‡´æ€§: {best_consistency[1].consistency_score:.1f}%)"
        recommendations["best_for_edge_cases"] = f"{best_edge_case[0]} (é‚Šç•Œæ¡ˆä¾‹: {best_edge_case[1].edge_case_handling:.1f}%)"
        
        # è¨ˆç®—ç¶œåˆåˆ†æ•¸
        overall_scores = {}
        for provider, metrics in self.performance_metrics.items():
            # ç¶œåˆåˆ†æ•¸ = æº–ç¢ºç‡ * 0.4 + ä¸€è‡´æ€§ * 0.3 + (100-éŒ¯èª¤ç‡) * 0.2 + é‚Šç•Œæ¡ˆä¾‹è™•ç† * 0.1
            overall_score = (
                metrics.accuracy * 0.4 +
                metrics.consistency_score * 0.3 +
                (100 - metrics.error_rate) * 0.2 +
                metrics.edge_case_handling * 0.1
            )
            overall_scores[provider] = overall_score
        
        best_overall = max(overall_scores.items(), key=lambda x: x[1])
        recommendations["best_overall"] = f"{best_overall[0]} (ç¶œåˆåˆ†æ•¸: {best_overall[1]:.1f})"
        
        # ä½¿ç”¨å ´æ™¯å»ºè­°
        recommendations["usage_scenarios"] = {
            "ç”Ÿç”¢ç’°å¢ƒ": f"æ¨è–¦ {best_accuracy[0]}ï¼Œæº–ç¢ºç‡æœ€é«˜",
            "å³æ™‚æ‡‰ç”¨": f"æ¨è–¦ {best_speed[0]}ï¼ŒéŸ¿æ‡‰é€Ÿåº¦æœ€å¿«",
            "è¤‡é›œæŸ¥è©¢": f"æ¨è–¦ {best_edge_case[0]}ï¼Œé‚Šç•Œæ¡ˆä¾‹è™•ç†æœ€ä½³",
            "ç©©å®šæ€§è¦æ±‚": f"æ¨è–¦ {best_consistency[0]}ï¼Œä¸€è‡´æ€§æœ€é«˜"
        }
        
        # æ”¹é€²å»ºè­°
        for provider, metrics in self.performance_metrics.items():
            if metrics.accuracy < 80:
                recommendations["improvement_suggestions"].append(
                    f"{provider}: æº–ç¢ºç‡åä½({metrics.accuracy:.1f}%)ï¼Œå»ºè­°å„ªåŒ–æç¤ºè©æˆ–èª¿æ•´åƒæ•¸"
                )
            if metrics.error_rate > 10:
                recommendations["improvement_suggestions"].append(
                    f"{provider}: éŒ¯èª¤ç‡è¼ƒé«˜({metrics.error_rate:.1f}%)ï¼Œå»ºè­°æª¢æŸ¥APIé…ç½®å’Œç¶²è·¯é€£æ¥"
                )
            if metrics.avg_response_time > 5:
                recommendations["improvement_suggestions"].append(
                    f"{provider}: éŸ¿æ‡‰æ™‚é–“è¼ƒæ…¢({metrics.avg_response_time:.2f}s)ï¼Œå»ºè­°å„ªåŒ–ç¶²è·¯æˆ–ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹"
                )
        
        return recommendations
    
    def _save_results(self, comparison_report: Dict[str, Any]) -> None:
        """ä¿å­˜æ¸¬è©¦çµæœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è©³ç´°çµæœ
        detailed_filename = f"enhanced_llm_comparison_detailed_{timestamp}.json"
        detailed_data = {
            "comparison_report": comparison_report,
            "raw_results": {
                provider: [asdict(result) for result in results]
                for provider, results in self.results.items()
            },
            "test_cases": [asdict(tc) for tc in self.test_cases]
        }
        
        with open(detailed_filename, 'w', encoding='utf-8') as f:
            json.dump(detailed_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ç°¡åŒ–å ±å‘Š
        summary_filename = f"enhanced_llm_comparison_summary_{timestamp}.json"
        with open(summary_filename, 'w', encoding='utf-8') as f:
            json.dump(comparison_report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ æ¸¬è©¦çµæœå·²ä¿å­˜:")
        print(f"   è©³ç´°çµæœ: {detailed_filename}")
        print(f"   æ‘˜è¦å ±å‘Š: {summary_filename}")
    
    def print_summary_report(self) -> None:
        """æ‰“å°æ‘˜è¦å ±å‘Š"""
        if not self.performance_metrics:
            print("âŒ æ²’æœ‰æ¸¬è©¦çµæœå¯é¡¯ç¤º")
            return
        
        print("\nğŸ“Š LLMæ¨¡å‹æ¯”è¼ƒæ¸¬è©¦æ‘˜è¦å ±å‘Š")
        print("=" * 60)
        
        # åŸºæœ¬çµ±è¨ˆ
        print(f"\nğŸ“‹ æ¸¬è©¦æ¦‚æ³:")
        print(f"   æ¸¬è©¦æä¾›å•†æ•¸é‡: {len(self.performance_metrics)}")
        print(f"   æ¸¬è©¦æ¡ˆä¾‹ç¸½æ•¸: {len(self.test_cases)}")
        print(f"   æ¸¬è©¦é¡åˆ¥: {len(set(tc.category for tc in self.test_cases))}å€‹")
        print(f"   é›£åº¦ç´šåˆ¥: {len(set(tc.difficulty_level for tc in self.test_cases))}å€‹")
        
        # æ€§èƒ½æ’å
        print(f"\nğŸ† æ€§èƒ½æ’å:")
        
        # æº–ç¢ºç‡æ’å
        accuracy_ranking = sorted(self.performance_metrics.items(), 
                                 key=lambda x: x[1].accuracy, reverse=True)
        print(f"\n   ğŸ“ˆ æº–ç¢ºç‡æ’å:")
        for i, (provider, metrics) in enumerate(accuracy_ranking, 1):
            print(f"      {i}. {provider}: {metrics.accuracy:.1f}%")
        
        # é€Ÿåº¦æ’å
        speed_ranking = sorted(self.performance_metrics.items(), 
                              key=lambda x: x[1].avg_response_time)
        print(f"\n   âš¡ é€Ÿåº¦æ’å:")
        for i, (provider, metrics) in enumerate(speed_ranking, 1):
            print(f"      {i}. {provider}: {metrics.avg_response_time:.2f}s")
        
        # ä¸€è‡´æ€§æ’å
        consistency_ranking = sorted(self.performance_metrics.items(), 
                                    key=lambda x: x[1].consistency_score, reverse=True)
        print(f"\n   ğŸ¯ ä¸€è‡´æ€§æ’å:")
        for i, (provider, metrics) in enumerate(consistency_ranking, 1):
            print(f"      {i}. {provider}: {metrics.consistency_score:.1f}%")
        
        # è©³ç´°æŒ‡æ¨™
        print(f"\nğŸ“Š è©³ç´°æŒ‡æ¨™:")
        for provider, metrics in self.performance_metrics.items():
            print(f"\n   ğŸ¤– {provider}:")
            print(f"      æº–ç¢ºç‡: {metrics.accuracy:.1f}%")
            print(f"      å¹³å‡ç½®ä¿¡åº¦: {metrics.avg_confidence:.2f}")
            print(f"      å¹³å‡éŸ¿æ‡‰æ™‚é–“: {metrics.avg_response_time:.2f}s")
            print(f"      ä¸€è‡´æ€§åˆ†æ•¸: {metrics.consistency_score:.1f}%")
            print(f"      éŒ¯èª¤ç‡: {metrics.error_rate:.1f}%")
            print(f"      é‚Šç•Œæ¡ˆä¾‹è™•ç†: {metrics.edge_case_handling:.1f}%")
        
        print("\n" + "=" * 60)


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ å¢å¼·ç‰ˆLLMæ¨¡å‹æ¯”è¼ƒæ¸¬è©¦å·¥å…·")
    print("=" * 50)
    
    # å‰µå»ºæ¸¬è©¦å™¨
    tester = EnhancedLLMModelComparisonTester()
    
    # ç²å–å¯ç”¨çš„æä¾›å•†
    available_providers = tester.config_manager.get_available_providers()
    
    if not available_providers:
        print("âŒ æ²’æœ‰å¯ç”¨çš„LLMæä¾›å•†ï¼Œè«‹æª¢æŸ¥é…ç½®")
        return
    
    print(f"ğŸ“‹ ç™¼ç¾ {len(available_providers)} å€‹å¯ç”¨æä¾›å•†: {[p.value for p in available_providers]}")
    print(f"ğŸ“Š æº–å‚™æ¸¬è©¦ {len(tester.test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹")
    
    # è©¢å•ç”¨æˆ¶æ˜¯å¦è¦æ¸¬è©¦æ‰€æœ‰æä¾›å•†
    response = input("\næ˜¯å¦æ¸¬è©¦æ‰€æœ‰å¯ç”¨æä¾›å•†ï¼Ÿ(y/n): ").lower().strip()
    
    if response == 'y':
        test_providers = available_providers
    else:
        print("\nå¯ç”¨æä¾›å•†:")
        for i, provider in enumerate(available_providers, 1):
            print(f"  {i}. {provider.value}")
        
        selected_indices = input("\nè«‹è¼¸å…¥è¦æ¸¬è©¦çš„æä¾›å•†ç·¨è™Ÿï¼ˆç”¨é€—è™Ÿåˆ†éš”ï¼‰: ").strip()
        try:
            indices = [int(x.strip()) - 1 for x in selected_indices.split(',')]
            test_providers = [available_providers[i] for i in indices if 0 <= i < len(available_providers)]
        except (ValueError, IndexError):
            print("âŒ è¼¸å…¥æ ¼å¼éŒ¯èª¤ï¼Œå°‡æ¸¬è©¦æ‰€æœ‰æä¾›å•†")
            test_providers = available_providers
    
    if not test_providers:
        print("âŒ æ²’æœ‰é¸æ“‡ä»»ä½•æä¾›å•†")
        return
    
    print(f"\nğŸ¯ å°‡æ¸¬è©¦ä»¥ä¸‹æä¾›å•†: {[p.value for p in test_providers]}")
    
    # é‹è¡Œæ¸¬è©¦
    try:
        comparison_report = tester.run_model_comparison_test(test_providers)
        
        if comparison_report:
            # é¡¯ç¤ºæ‘˜è¦å ±å‘Š
            tester.print_summary_report()
            
            print("\nâœ… æ¸¬è©¦å®Œæˆï¼")
            print("ğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜åˆ°JSONæ–‡ä»¶ä¸­")
        else:
            print("âŒ æ¸¬è©¦å¤±æ•—ï¼Œæ²’æœ‰ç”Ÿæˆå ±å‘Š")
            
    except KeyboardInterrupt:
        print("\nâš ï¸ æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()