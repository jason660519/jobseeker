#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMæ¸¬è©¦æ¡ˆä¾‹é©—è­‰å™¨
é©—è­‰ç”Ÿæˆçš„æ¸¬è©¦æ¡ˆä¾‹çš„è³ªé‡ã€æœ‰æ•ˆæ€§å’Œå®Œæ•´æ€§

Author: JobSpy Team
Date: 2025-01-27
"""

import sys
import os
import json
import re
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from collections import defaultdict, Counter
import statistics

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from jobseeker.llm_intent_analyzer import LLMProvider


class ValidationSeverity(Enum):
    """é©—è­‰å•é¡Œåš´é‡ç¨‹åº¦"""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


class ValidationCategory(Enum):
    """é©—è­‰é¡åˆ¥"""
    CONTENT_QUALITY = "content_quality"  # å…§å®¹è³ªé‡
    LANGUAGE_CONSISTENCY = "language_consistency"  # èªè¨€ä¸€è‡´æ€§
    INTENT_CLARITY = "intent_clarity"  # æ„åœ–æ¸…æ™°åº¦
    ENTITY_EXTRACTION = "entity_extraction"  # å¯¦é«”æå–
    EDGE_CASE_COVERAGE = "edge_case_coverage"  # é‚Šç•Œæ¡ˆä¾‹è¦†è“‹
    DIVERSITY = "diversity"  # å¤šæ¨£æ€§
    COMPLETENESS = "completeness"  # å®Œæ•´æ€§
    CONSISTENCY = "consistency"  # ä¸€è‡´æ€§
    BIAS_DETECTION = "bias_detection"  # åè¦‹æª¢æ¸¬
    SECURITY = "security"  # å®‰å…¨æ€§


@dataclass
class ValidationIssue:
    """é©—è­‰å•é¡Œ"""
    category: ValidationCategory
    severity: ValidationSeverity
    test_case_id: str
    message: str
    details: Dict[str, Any] = field(default_factory=dict)
    suggestions: List[str] = field(default_factory=list)
    auto_fixable: bool = False


@dataclass
class ValidationMetrics:
    """é©—è­‰æŒ‡æ¨™"""
    total_cases: int
    valid_cases: int
    invalid_cases: int
    warning_cases: int
    error_cases: int
    critical_cases: int
    coverage_score: float
    diversity_score: float
    quality_score: float
    overall_score: float
    category_scores: Dict[str, float] = field(default_factory=dict)


@dataclass
class ValidationReport:
    """é©—è­‰å ±å‘Š"""
    validation_time: datetime
    metrics: ValidationMetrics
    issues: List[ValidationIssue]
    recommendations: List[str]
    summary: Dict[str, Any]
    detailed_analysis: Dict[str, Any] = field(default_factory=dict)


class LLMTestCaseValidator:
    """LLMæ¸¬è©¦æ¡ˆä¾‹é©—è­‰å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–é©—è­‰å™¨"""
        self.validation_rules = self._load_validation_rules()
        self.language_patterns = self._load_language_patterns()
        self.intent_keywords = self._load_intent_keywords()
        self.security_patterns = self._load_security_patterns()
        
    def _load_validation_rules(self) -> Dict[str, Any]:
        """è¼‰å…¥é©—è­‰è¦å‰‡"""
        return {
            "min_query_length": 3,
            "max_query_length": 500,
            "min_confidence": 0.0,
            "max_confidence": 1.0,
            "required_fields": ["id", "query", "expected_intent", "category"],
            "valid_intents": [
                "job_search", "career_advice", "skill_query", "salary_query",
                "location_query", "company_query", "unknown", "inappropriate_query",
                "weather_query", "entertainment_query", "restaurant_query", "spam_query"
            ],
            "language_codes": ["zh-TW", "zh-CN", "en", "ja", "ko", "mixed"],
            "complexity_levels": ["simple", "medium", "complex", "extreme"],
            "min_diversity_threshold": 0.7,
            "min_coverage_threshold": 0.8
        }
    
    def _load_language_patterns(self) -> Dict[str, List[str]]:
        """è¼‰å…¥èªè¨€æ¨¡å¼"""
        return {
            "zh-TW": [r"[\u4e00-\u9fff]+", r"[ï¼Œã€‚ï¼Ÿï¼ï¼›ï¼š]"],
            "zh-CN": [r"[\u4e00-\u9fff]+", r"[ï¼Œã€‚ï¼Ÿï¼ï¼›ï¼š]"],
            "en": [r"[a-zA-Z]+", r"[,.?!;:]"],
            "ja": [r"[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9fff]+"],
            "ko": [r"[\uac00-\ud7af]+"],
            "mixed": [r"[\u4e00-\u9fff]+.*[a-zA-Z]+|[a-zA-Z]+.*[\u4e00-\u9fff]+"]
        }
    
    def _load_intent_keywords(self) -> Dict[str, List[str]]:
        """è¼‰å…¥æ„åœ–é—œéµè©"""
        return {
            "job_search": [
                "å·¥ä½œ", "è·ç¼º", "è·ä½", "æ±‚è·", "æ‰¾å·¥ä½œ", "æ‡‰å¾µ", "æ‹›è˜",
                "job", "position", "career", "employment", "hiring", "work",
                "ä»•äº‹", "å°±è·", "ì±„ìš©", "ì¼ìë¦¬"
            ],
            "career_advice": [
                "è½‰è·", "è·æ¶¯", "å»ºè­°", "ç™¼å±•", "è¦åŠƒ",
                "career change", "advice", "development", "planning",
                "è»¢è·", "ã‚­ãƒ£ãƒªã‚¢", "ì „ì§", "ê²½ë ¥"
            ],
            "salary_query": [
                "è–ªè³‡", "è–ªæ°´", "å¾…é‡", "æ”¶å…¥", "å¹´è–ª", "æœˆè–ª",
                "salary", "wage", "compensation", "income", "pay",
                "çµ¦æ–™", "å¹´å", "ê¸‰ì—¬", "ì—°ë´‰"
            ],
            "location_query": [
                "åœ°é»", "ä½ç½®", "åœ°å€", "åŸå¸‚", "å°åŒ—", "æ–°ç«¹",
                "location", "place", "city", "area", "remote",
                "å ´æ‰€", "åœ°åŸŸ", "ìœ„ì¹˜", "ì§€ì—­"
            ]
        }
    
    def _load_security_patterns(self) -> List[str]:
        """è¼‰å…¥å®‰å…¨æ¨¡å¼"""
        return [
            r"hack|hacking|ç ´è§£|å…¥ä¾µ",
            r"illegal|é•æ³•|éæ³•",
            r"steal|å·å–|ç«Šå–",
            r"fraud|è©é¨™|æ¬ºè©",
            r"password|å¯†ç¢¼|å¯†ç ",
            r"credit card|ä¿¡ç”¨å¡|ä¿¡ç”¨å¡",
            r"social security|èº«åˆ†è­‰|èº«ä»½è¯",
            r"personal information|å€‹äººè³‡æ–™|ä¸ªäººä¿¡æ¯"
        ]
    
    def validate_test_cases(self, test_cases: List[Dict[str, Any]]) -> ValidationReport:
        """é©—è­‰æ¸¬è©¦æ¡ˆä¾‹"""
        print(f"ğŸ” é–‹å§‹é©—è­‰ {len(test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹...")
        
        start_time = time.time()
        issues = []
        
        # åŸ·è¡Œå„ç¨®é©—è­‰
        issues.extend(self._validate_content_quality(test_cases))
        issues.extend(self._validate_language_consistency(test_cases))
        issues.extend(self._validate_intent_clarity(test_cases))
        issues.extend(self._validate_entity_extraction(test_cases))
        issues.extend(self._validate_completeness(test_cases))
        issues.extend(self._validate_consistency(test_cases))
        issues.extend(self._validate_diversity(test_cases))
        issues.extend(self._validate_edge_case_coverage(test_cases))
        issues.extend(self._validate_bias_detection(test_cases))
        issues.extend(self._validate_security(test_cases))
        
        # è¨ˆç®—æŒ‡æ¨™
        metrics = self._calculate_metrics(test_cases, issues)
        
        # ç”Ÿæˆå»ºè­°
        recommendations = self._generate_recommendations(issues, metrics)
        
        # å‰µå»ºè©³ç´°åˆ†æ
        detailed_analysis = self._create_detailed_analysis(test_cases, issues)
        
        # å‰µå»ºæ‘˜è¦
        summary = self._create_summary(metrics, issues)
        
        validation_time = time.time() - start_time
        
        report = ValidationReport(
            validation_time=datetime.now(),
            metrics=metrics,
            issues=issues,
            recommendations=recommendations,
            summary=summary,
            detailed_analysis=detailed_analysis
        )
        
        print(f"   âœ… é©—è­‰å®Œæˆï¼Œè€—æ™‚ {validation_time:.2f} ç§’")
        print(f"   ğŸ“Š ç™¼ç¾ {len(issues)} å€‹å•é¡Œ")
        
        return report
    
    def _validate_content_quality(self, test_cases: List[Dict[str, Any]]) -> List[ValidationIssue]:
        """é©—è­‰å…§å®¹è³ªé‡"""
        issues = []
        
        for case in test_cases:
            case_id = case.get("id", "unknown")
            query = case.get("query", "")
            
            # æª¢æŸ¥æŸ¥è©¢é•·åº¦
            if len(query) < self.validation_rules["min_query_length"]:
                issues.append(ValidationIssue(
                    category=ValidationCategory.CONTENT_QUALITY,
                    severity=ValidationSeverity.ERROR,
                    test_case_id=case_id,
                    message="æŸ¥è©¢å…§å®¹éçŸ­",
                    details={"query_length": len(query), "min_required": self.validation_rules["min_query_length"]},
                    suggestions=["å¢åŠ æŸ¥è©¢å…§å®¹çš„é•·åº¦å’Œè©³ç´°ç¨‹åº¦"]
                ))
            
            if len(query) > self.validation_rules["max_query_length"]:
                issues.append(ValidationIssue(
                    category=ValidationCategory.CONTENT_QUALITY,
                    severity=ValidationSeverity.WARNING,
                    test_case_id=case_id,
                    message="æŸ¥è©¢å…§å®¹éé•·",
                    details={"query_length": len(query), "max_allowed": self.validation_rules["max_query_length"]},
                    suggestions=["ç°¡åŒ–æŸ¥è©¢å…§å®¹ï¼Œä¿æŒç°¡æ½”æ˜ç¢º"]
                ))
            
            # æª¢æŸ¥ç©ºç™½æˆ–ç„¡æ„ç¾©å…§å®¹
            if not query.strip():
                issues.append(ValidationIssue(
                    category=ValidationCategory.CONTENT_QUALITY,
                    severity=ValidationSeverity.CRITICAL,
                    test_case_id=case_id,
                    message="æŸ¥è©¢å…§å®¹ç‚ºç©º",
                    details={"query": query},
                    suggestions=["æä¾›æœ‰æ„ç¾©çš„æŸ¥è©¢å…§å®¹"]
                ))
            
            # æª¢æŸ¥é‡è¤‡å­—ç¬¦
            if re.search(r"(.)\1{5,}", query):
                issues.append(ValidationIssue(
                    category=ValidationCategory.CONTENT_QUALITY,
                    severity=ValidationSeverity.WARNING,
                    test_case_id=case_id,
                    message="æŸ¥è©¢åŒ…å«éå¤šé‡è¤‡å­—ç¬¦",
                    details={"query": query},
                    suggestions=["æ¸›å°‘é‡è¤‡å­—ç¬¦ï¼Œæé«˜å…§å®¹è³ªé‡"]
                ))
            
            # æª¢æŸ¥ç‰¹æ®Šå­—ç¬¦éå¤š
            special_char_ratio = len(re.findall(r"[^\w\s\u4e00-\u9fff]", query)) / max(len(query), 1)
            if special_char_ratio > 0.3:
                issues.append(ValidationIssue(
                    category=ValidationCategory.CONTENT_QUALITY,
                    severity=ValidationSeverity.WARNING,
                    test_case_id=case_id,
                    message="æŸ¥è©¢åŒ…å«éå¤šç‰¹æ®Šå­—ç¬¦",
                    details={"special_char_ratio": special_char_ratio, "query": query},
                    suggestions=["æ¸›å°‘ç‰¹æ®Šå­—ç¬¦çš„ä½¿ç”¨"]
                ))
        
        return issues
    
    def _validate_language_consistency(self, test_cases: List[Dict[str, Any]]) -> List[ValidationIssue]:
        """é©—è­‰èªè¨€ä¸€è‡´æ€§"""
        issues = []
        
        for case in test_cases:
            case_id = case.get("id", "unknown")
            query = case.get("query", "")
            declared_language = case.get("language", "")
            
            # æª¢æ¸¬å¯¦éš›èªè¨€
            detected_language = self._detect_language(query)
            
            if detected_language != declared_language:
                issues.append(ValidationIssue(
                    category=ValidationCategory.LANGUAGE_CONSISTENCY,
                    severity=ValidationSeverity.WARNING,
                    test_case_id=case_id,
                    message="è²æ˜èªè¨€èˆ‡æª¢æ¸¬èªè¨€ä¸ä¸€è‡´",
                    details={
                        "declared_language": declared_language,
                        "detected_language": detected_language,
                        "query": query
                    },
                    suggestions=[f"å°‡èªè¨€æ¨™è¨˜æ›´æ”¹ç‚º {detected_language} æˆ–èª¿æ•´æŸ¥è©¢å…§å®¹"]
                ))
        
        return issues
    
    def _validate_intent_clarity(self, test_cases: List[Dict[str, Any]]) -> List[ValidationIssue]:
        """é©—è­‰æ„åœ–æ¸…æ™°åº¦"""
        issues = []
        
        for case in test_cases:
            case_id = case.get("id", "unknown")
            query = case.get("query", "")
            expected_intent = case.get("expected_intent", "")
            
            # æª¢æŸ¥æ„åœ–æ˜¯å¦æœ‰æ•ˆ
            if expected_intent not in self.validation_rules["valid_intents"]:
                issues.append(ValidationIssue(
                    category=ValidationCategory.INTENT_CLARITY,
                    severity=ValidationSeverity.ERROR,
                    test_case_id=case_id,
                    message="ç„¡æ•ˆçš„æœŸæœ›æ„åœ–",
                    details={
                        "expected_intent": expected_intent,
                        "valid_intents": self.validation_rules["valid_intents"]
                    },
                    suggestions=["ä½¿ç”¨æœ‰æ•ˆçš„æ„åœ–æ¨™ç±¤"]
                ))
            
            # æª¢æŸ¥æŸ¥è©¢èˆ‡æ„åœ–çš„åŒ¹é…åº¦
            intent_keywords = self.intent_keywords.get(expected_intent, [])
            if intent_keywords:
                has_keyword = any(keyword.lower() in query.lower() for keyword in intent_keywords)
                if not has_keyword and expected_intent != "unknown":
                    issues.append(ValidationIssue(
                        category=ValidationCategory.INTENT_CLARITY,
                        severity=ValidationSeverity.WARNING,
                        test_case_id=case_id,
                        message="æŸ¥è©¢å…§å®¹èˆ‡æœŸæœ›æ„åœ–ä¸åŒ¹é…",
                        details={
                            "query": query,
                            "expected_intent": expected_intent,
                            "missing_keywords": intent_keywords
                        },
                        suggestions=["åœ¨æŸ¥è©¢ä¸­æ·»åŠ ç›¸é—œé—œéµè©æˆ–èª¿æ•´æœŸæœ›æ„åœ–"]
                    ))
        
        return issues
    
    def _validate_entity_extraction(self, test_cases: List[Dict[str, Any]]) -> List[ValidationIssue]:
        """é©—è­‰å¯¦é«”æå–"""
        issues = []
        
        for case in test_cases:
            case_id = case.get("id", "unknown")
            query = case.get("query", "")
            expected_entities = case.get("expected_entities", {})
            
            # æª¢æŸ¥å¯¦é«”æ˜¯å¦åœ¨æŸ¥è©¢ä¸­å­˜åœ¨
            for entity_type, entity_value in expected_entities.items():
                if isinstance(entity_value, str) and entity_value not in query:
                    issues.append(ValidationIssue(
                        category=ValidationCategory.ENTITY_EXTRACTION,
                        severity=ValidationSeverity.WARNING,
                        test_case_id=case_id,
                        message="æœŸæœ›å¯¦é«”åœ¨æŸ¥è©¢ä¸­æœªæ‰¾åˆ°",
                        details={
                            "entity_type": entity_type,
                            "entity_value": entity_value,
                            "query": query
                        },
                        suggestions=["ç¢ºä¿å¯¦é«”å€¼ç¢ºå¯¦å‡ºç¾åœ¨æŸ¥è©¢ä¸­"]
                    ))
        
        return issues
    
    def _validate_completeness(self, test_cases: List[Dict[str, Any]]) -> List[ValidationIssue]:
        """é©—è­‰å®Œæ•´æ€§"""
        issues = []
        
        for case in test_cases:
            case_id = case.get("id", "unknown")
            
            # æª¢æŸ¥å¿…éœ€å­—æ®µ
            for field in self.validation_rules["required_fields"]:
                if field not in case or not case[field]:
                    issues.append(ValidationIssue(
                        category=ValidationCategory.COMPLETENESS,
                        severity=ValidationSeverity.ERROR,
                        test_case_id=case_id,
                        message=f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}",
                        details={"missing_field": field},
                        suggestions=[f"æ·»åŠ  {field} å­—æ®µ"]
                    ))
            
            # æª¢æŸ¥ç½®ä¿¡åº¦ç¯„åœ
            confidence_range = case.get("expected_confidence_range", [])
            if isinstance(confidence_range, list) and len(confidence_range) == 2:
                min_conf, max_conf = confidence_range
                if not (self.validation_rules["min_confidence"] <= min_conf <= max_conf <= self.validation_rules["max_confidence"]):
                    issues.append(ValidationIssue(
                        category=ValidationCategory.COMPLETENESS,
                        severity=ValidationSeverity.ERROR,
                        test_case_id=case_id,
                        message="ç„¡æ•ˆçš„ç½®ä¿¡åº¦ç¯„åœ",
                        details={"confidence_range": confidence_range},
                        suggestions=["è¨­ç½®æœ‰æ•ˆçš„ç½®ä¿¡åº¦ç¯„åœ (0.0-1.0)"]
                    ))
        
        return issues
    
    def _validate_consistency(self, test_cases: List[Dict[str, Any]]) -> List[ValidationIssue]:
        """é©—è­‰ä¸€è‡´æ€§"""
        issues = []
        
        # æª¢æŸ¥IDå”¯ä¸€æ€§
        ids = [case.get("id", "") for case in test_cases]
        duplicate_ids = [id for id, count in Counter(ids).items() if count > 1]
        
        for dup_id in duplicate_ids:
            issues.append(ValidationIssue(
                category=ValidationCategory.CONSISTENCY,
                severity=ValidationSeverity.ERROR,
                test_case_id=dup_id,
                message="é‡è¤‡çš„æ¸¬è©¦æ¡ˆä¾‹ID",
                details={"duplicate_id": dup_id},
                suggestions=["ç¢ºä¿æ¯å€‹æ¸¬è©¦æ¡ˆä¾‹éƒ½æœ‰å”¯ä¸€çš„ID"]
            ))
        
        # æª¢æŸ¥æŸ¥è©¢é‡è¤‡
        queries = [case.get("query", "") for case in test_cases]
        duplicate_queries = [query for query, count in Counter(queries).items() if count > 1 and query.strip()]
        
        for dup_query in duplicate_queries:
            matching_cases = [case.get("id", "unknown") for case in test_cases if case.get("query") == dup_query]
            issues.append(ValidationIssue(
                category=ValidationCategory.CONSISTENCY,
                severity=ValidationSeverity.WARNING,
                test_case_id=",".join(matching_cases),
                message="é‡è¤‡çš„æŸ¥è©¢å…§å®¹",
                details={"duplicate_query": dup_query, "case_ids": matching_cases},
                suggestions=["å¢åŠ æŸ¥è©¢çš„å¤šæ¨£æ€§ï¼Œé¿å…é‡è¤‡å…§å®¹"]
            ))
        
        return issues
    
    def _validate_diversity(self, test_cases: List[Dict[str, Any]]) -> List[ValidationIssue]:
        """é©—è­‰å¤šæ¨£æ€§"""
        issues = []
        
        if not test_cases:
            return issues
        
        # è¨ˆç®—é¡åˆ¥å¤šæ¨£æ€§
        categories = [case.get("category", "") for case in test_cases]
        category_distribution = Counter(categories)
        total_cases = len(test_cases)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰é¡åˆ¥éåº¦é›†ä¸­
        for category, count in category_distribution.items():
            ratio = count / total_cases
            if ratio > 0.5:  # è¶…é50%é›†ä¸­åœ¨ä¸€å€‹é¡åˆ¥
                issues.append(ValidationIssue(
                    category=ValidationCategory.DIVERSITY,
                    severity=ValidationSeverity.WARNING,
                    test_case_id="global",
                    message=f"é¡åˆ¥ {category} éåº¦é›†ä¸­",
                    details={"category": category, "ratio": ratio, "count": count},
                    suggestions=["å¢åŠ å…¶ä»–é¡åˆ¥çš„æ¸¬è©¦æ¡ˆä¾‹ä»¥æé«˜å¤šæ¨£æ€§"]
                ))
        
        # è¨ˆç®—è¤‡é›œåº¦å¤šæ¨£æ€§
        complexities = [case.get("complexity", "") for case in test_cases]
        complexity_distribution = Counter(complexities)
        
        # æª¢æŸ¥æ˜¯å¦ç¼ºå°‘æŸäº›è¤‡é›œåº¦ç´šåˆ¥
        expected_complexities = set(self.validation_rules["complexity_levels"])
        actual_complexities = set(complexities)
        missing_complexities = expected_complexities - actual_complexities
        
        if missing_complexities:
            issues.append(ValidationIssue(
                category=ValidationCategory.DIVERSITY,
                severity=ValidationSeverity.INFO,
                test_case_id="global",
                message="ç¼ºå°‘æŸäº›è¤‡é›œåº¦ç´šåˆ¥",
                details={"missing_complexities": list(missing_complexities)},
                suggestions=[f"æ·»åŠ  {', '.join(missing_complexities)} è¤‡é›œåº¦çš„æ¸¬è©¦æ¡ˆä¾‹"]
            ))
        
        return issues
    
    def _validate_edge_case_coverage(self, test_cases: List[Dict[str, Any]]) -> List[ValidationIssue]:
        """é©—è­‰é‚Šç•Œæ¡ˆä¾‹è¦†è“‹"""
        issues = []
        
        edge_case_count = sum(1 for case in test_cases if case.get("category") == "edge_case")
        total_cases = len(test_cases)
        
        if total_cases > 0:
            edge_case_ratio = edge_case_count / total_cases
            if edge_case_ratio < 0.05:  # å°‘æ–¼5%çš„é‚Šç•Œæ¡ˆä¾‹
                issues.append(ValidationIssue(
                    category=ValidationCategory.EDGE_CASE_COVERAGE,
                    severity=ValidationSeverity.WARNING,
                    test_case_id="global",
                    message="é‚Šç•Œæ¡ˆä¾‹è¦†è“‹ä¸è¶³",
                    details={"edge_case_ratio": edge_case_ratio, "edge_case_count": edge_case_count},
                    suggestions=["å¢åŠ æ›´å¤šé‚Šç•Œæ¡ˆä¾‹ä»¥æé«˜æ¸¬è©¦è¦†è“‹ç‡"]
                ))
        
        return issues
    
    def _validate_bias_detection(self, test_cases: List[Dict[str, Any]]) -> List[ValidationIssue]:
        """é©—è­‰åè¦‹æª¢æ¸¬"""
        issues = []
        
        # æª¢æŸ¥æ€§åˆ¥åè¦‹
        gender_bias_patterns = [r"ç”·æ€§", r"å¥³æ€§", r"male", r"female", r"man", r"woman"]
        
        for case in test_cases:
            case_id = case.get("id", "unknown")
            query = case.get("query", "")
            
            for pattern in gender_bias_patterns:
                if re.search(pattern, query, re.IGNORECASE):
                    issues.append(ValidationIssue(
                        category=ValidationCategory.BIAS_DETECTION,
                        severity=ValidationSeverity.INFO,
                        test_case_id=case_id,
                        message="æª¢æ¸¬åˆ°å¯èƒ½çš„æ€§åˆ¥åè¦‹",
                        details={"pattern": pattern, "query": query},
                        suggestions=["æª¢æŸ¥æ˜¯å¦å­˜åœ¨æ€§åˆ¥åè¦‹ï¼Œç¢ºä¿æ¸¬è©¦æ¡ˆä¾‹çš„å…¬å¹³æ€§"]
                    ))
        
        return issues
    
    def _validate_security(self, test_cases: List[Dict[str, Any]]) -> List[ValidationIssue]:
        """é©—è­‰å®‰å…¨æ€§"""
        issues = []
        
        for case in test_cases:
            case_id = case.get("id", "unknown")
            query = case.get("query", "")
            
            for pattern in self.security_patterns:
                if re.search(pattern, query, re.IGNORECASE):
                    issues.append(ValidationIssue(
                        category=ValidationCategory.SECURITY,
                        severity=ValidationSeverity.WARNING,
                        test_case_id=case_id,
                        message="æª¢æ¸¬åˆ°æ½›åœ¨çš„å®‰å…¨é¢¨éšªå…§å®¹",
                        details={"pattern": pattern, "query": query},
                        suggestions=["æª¢æŸ¥æŸ¥è©¢å…§å®¹æ˜¯å¦åŒ…å«ä¸ç•¶æˆ–å±éšªä¿¡æ¯"]
                    ))
        
        return issues
    
    def _detect_language(self, text: str) -> str:
        """æª¢æ¸¬æ–‡æœ¬èªè¨€"""
        for lang, patterns in self.language_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text):
                    return lang
        return "unknown"
    
    def _calculate_metrics(self, test_cases: List[Dict[str, Any]], issues: List[ValidationIssue]) -> ValidationMetrics:
        """è¨ˆç®—é©—è­‰æŒ‡æ¨™"""
        total_cases = len(test_cases)
        
        # æŒ‰åš´é‡ç¨‹åº¦çµ±è¨ˆå•é¡Œ
        severity_counts = Counter(issue.severity for issue in issues)
        critical_cases = severity_counts[ValidationSeverity.CRITICAL]
        error_cases = severity_counts[ValidationSeverity.ERROR]
        warning_cases = severity_counts[ValidationSeverity.WARNING]
        
        # è¨ˆç®—æœ‰æ•ˆæ¡ˆä¾‹æ•¸
        invalid_case_ids = set()
        for issue in issues:
            if issue.severity in [ValidationSeverity.CRITICAL, ValidationSeverity.ERROR]:
                invalid_case_ids.add(issue.test_case_id)
        
        invalid_cases = len(invalid_case_ids)
        valid_cases = total_cases - invalid_cases
        
        # è¨ˆç®—è¦†è“‹ç‡åˆ†æ•¸
        categories = set(case.get("category", "") for case in test_cases)
        expected_categories = 10  # é æœŸçš„é¡åˆ¥æ•¸é‡
        coverage_score = min(len(categories) / expected_categories, 1.0)
        
        # è¨ˆç®—å¤šæ¨£æ€§åˆ†æ•¸
        if total_cases > 0:
            category_distribution = Counter(case.get("category", "") for case in test_cases)
            max_category_ratio = max(category_distribution.values()) / total_cases
            diversity_score = 1.0 - max_category_ratio
        else:
            diversity_score = 0.0
        
        # è¨ˆç®—è³ªé‡åˆ†æ•¸
        if total_cases > 0:
            quality_score = valid_cases / total_cases
        else:
            quality_score = 0.0
        
        # è¨ˆç®—ç¸½é«”åˆ†æ•¸
        overall_score = (coverage_score + diversity_score + quality_score) / 3
        
        # è¨ˆç®—é¡åˆ¥åˆ†æ•¸
        category_scores = {}
        for category in ValidationCategory:
            category_issues = [issue for issue in issues if issue.category == category]
            if category_issues:
                severity_weights = {
                    ValidationSeverity.INFO: 0.1,
                    ValidationSeverity.WARNING: 0.3,
                    ValidationSeverity.ERROR: 0.7,
                    ValidationSeverity.CRITICAL: 1.0
                }
                total_weight = sum(severity_weights[issue.severity] for issue in category_issues)
                category_scores[category.value] = max(0.0, 1.0 - (total_weight / total_cases))
            else:
                category_scores[category.value] = 1.0
        
        return ValidationMetrics(
            total_cases=total_cases,
            valid_cases=valid_cases,
            invalid_cases=invalid_cases,
            warning_cases=warning_cases,
            error_cases=error_cases,
            critical_cases=critical_cases,
            coverage_score=coverage_score,
            diversity_score=diversity_score,
            quality_score=quality_score,
            overall_score=overall_score,
            category_scores=category_scores
        )
    
    def _generate_recommendations(self, issues: List[ValidationIssue], metrics: ValidationMetrics) -> List[str]:
        """ç”Ÿæˆå»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼æŒ‡æ¨™çš„å»ºè­°
        if metrics.coverage_score < 0.8:
            recommendations.append("å¢åŠ æ¸¬è©¦æ¡ˆä¾‹çš„é¡åˆ¥è¦†è“‹ç‡ï¼Œç¢ºä¿æ¶µè“‹æ‰€æœ‰é‡è¦å ´æ™¯")
        
        if metrics.diversity_score < 0.7:
            recommendations.append("æé«˜æ¸¬è©¦æ¡ˆä¾‹çš„å¤šæ¨£æ€§ï¼Œé¿å…éåº¦é›†ä¸­åœ¨æŸäº›é¡åˆ¥")
        
        if metrics.quality_score < 0.9:
            recommendations.append("æ”¹å–„æ¸¬è©¦æ¡ˆä¾‹çš„è³ªé‡ï¼Œä¿®å¾©ç™¼ç¾çš„éŒ¯èª¤å’Œå•é¡Œ")
        
        # åŸºæ–¼å•é¡Œçš„å»ºè­°
        issue_categories = Counter(issue.category for issue in issues)
        
        if issue_categories[ValidationCategory.CONTENT_QUALITY] > 0:
            recommendations.append("æ”¹å–„å…§å®¹è³ªé‡ï¼Œç¢ºä¿æŸ¥è©¢é•·åº¦é©ä¸­ä¸”æœ‰æ„ç¾©")
        
        if issue_categories[ValidationCategory.LANGUAGE_CONSISTENCY] > 0:
            recommendations.append("æª¢æŸ¥èªè¨€æ¨™è¨˜çš„ä¸€è‡´æ€§ï¼Œç¢ºä¿è²æ˜èªè¨€èˆ‡å¯¦éš›å…§å®¹åŒ¹é…")
        
        if issue_categories[ValidationCategory.INTENT_CLARITY] > 0:
            recommendations.append("æé«˜æ„åœ–çš„æ¸…æ™°åº¦ï¼Œç¢ºä¿æŸ¥è©¢å…§å®¹èˆ‡æœŸæœ›æ„åœ–åŒ¹é…")
        
        if issue_categories[ValidationCategory.SECURITY] > 0:
            recommendations.append("æª¢æŸ¥ä¸¦ç§»é™¤å¯èƒ½çš„å®‰å…¨é¢¨éšªå…§å®¹")
        
        return recommendations
    
    def _create_detailed_analysis(self, test_cases: List[Dict[str, Any]], issues: List[ValidationIssue]) -> Dict[str, Any]:
        """å‰µå»ºè©³ç´°åˆ†æ"""
        analysis = {
            "category_distribution": Counter(case.get("category", "") for case in test_cases),
            "complexity_distribution": Counter(case.get("complexity", "") for case in test_cases),
            "language_distribution": Counter(case.get("language", "") for case in test_cases),
            "issue_distribution": Counter(issue.category.value for issue in issues),
            "severity_distribution": Counter(issue.severity.value for issue in issues),
            "query_length_stats": self._calculate_query_length_stats(test_cases),
            "confidence_range_stats": self._calculate_confidence_stats(test_cases)
        }
        
        return analysis
    
    def _calculate_query_length_stats(self, test_cases: List[Dict[str, Any]]) -> Dict[str, float]:
        """è¨ˆç®—æŸ¥è©¢é•·åº¦çµ±è¨ˆ"""
        lengths = [len(case.get("query", "")) for case in test_cases]
        
        if lengths:
            return {
                "mean": statistics.mean(lengths),
                "median": statistics.median(lengths),
                "min": min(lengths),
                "max": max(lengths),
                "std": statistics.stdev(lengths) if len(lengths) > 1 else 0.0
            }
        else:
            return {"mean": 0, "median": 0, "min": 0, "max": 0, "std": 0}
    
    def _calculate_confidence_stats(self, test_cases: List[Dict[str, Any]]) -> Dict[str, float]:
        """è¨ˆç®—ç½®ä¿¡åº¦çµ±è¨ˆ"""
        confidence_ranges = []
        
        for case in test_cases:
            conf_range = case.get("expected_confidence_range", [])
            if isinstance(conf_range, list) and len(conf_range) == 2:
                confidence_ranges.extend(conf_range)
        
        if confidence_ranges:
            return {
                "mean": statistics.mean(confidence_ranges),
                "median": statistics.median(confidence_ranges),
                "min": min(confidence_ranges),
                "max": max(confidence_ranges),
                "std": statistics.stdev(confidence_ranges) if len(confidence_ranges) > 1 else 0.0
            }
        else:
            return {"mean": 0, "median": 0, "min": 0, "max": 0, "std": 0}
    
    def _create_summary(self, metrics: ValidationMetrics, issues: List[ValidationIssue]) -> Dict[str, Any]:
        """å‰µå»ºæ‘˜è¦"""
        return {
            "total_cases": metrics.total_cases,
            "validation_status": "PASS" if metrics.overall_score >= 0.8 else "FAIL",
            "overall_score": metrics.overall_score,
            "key_metrics": {
                "coverage_score": metrics.coverage_score,
                "diversity_score": metrics.diversity_score,
                "quality_score": metrics.quality_score
            },
            "issue_summary": {
                "total_issues": len(issues),
                "critical": metrics.critical_cases,
                "error": metrics.error_cases,
                "warning": metrics.warning_cases
            }
        }
    
    def export_validation_report(self, report: ValidationReport, output_file: str = None) -> str:
        """å°å‡ºé©—è­‰å ±å‘Š"""
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"validation_report_{timestamp}.json"
        
        print(f"ğŸ’¾ å°å‡ºé©—è­‰å ±å‘Šåˆ° {output_file}...")
        
        report_data = {
            "validation_time": report.validation_time.isoformat(),
            "metrics": {
                "total_cases": report.metrics.total_cases,
                "valid_cases": report.metrics.valid_cases,
                "invalid_cases": report.metrics.invalid_cases,
                "warning_cases": report.metrics.warning_cases,
                "error_cases": report.metrics.error_cases,
                "critical_cases": report.metrics.critical_cases,
                "coverage_score": report.metrics.coverage_score,
                "diversity_score": report.metrics.diversity_score,
                "quality_score": report.metrics.quality_score,
                "overall_score": report.metrics.overall_score,
                "category_scores": report.metrics.category_scores
            },
            "issues": [
                {
                    "category": issue.category.value,
                    "severity": issue.severity.value,
                    "test_case_id": issue.test_case_id,
                    "message": issue.message,
                    "details": issue.details,
                    "suggestions": issue.suggestions,
                    "auto_fixable": issue.auto_fixable
                }
                for issue in report.issues
            ],
            "recommendations": report.recommendations,
            "summary": report.summary,
            "detailed_analysis": report.detailed_analysis
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"   âœ… é©—è­‰å ±å‘Šå·²å°å‡º")
        return output_file
    
    def print_validation_summary(self, report: ValidationReport) -> None:
        """æ‰“å°é©—è­‰æ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ“‹ LLMæ¸¬è©¦æ¡ˆä¾‹é©—è­‰å ±å‘Š")
        print("=" * 60)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"\nğŸ“Š åŸºæœ¬ä¿¡æ¯:")
        print(f"   é©—è­‰æ™‚é–“: {report.validation_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"   ç¸½æ¸¬è©¦æ¡ˆä¾‹: {report.metrics.total_cases}")
        print(f"   æœ‰æ•ˆæ¡ˆä¾‹: {report.metrics.valid_cases}")
        print(f"   ç„¡æ•ˆæ¡ˆä¾‹: {report.metrics.invalid_cases}")
        
        # é©—è­‰ç‹€æ…‹
        status = report.summary["validation_status"]
        status_emoji = "âœ…" if status == "PASS" else "âŒ"
        print(f"\n{status_emoji} é©—è­‰ç‹€æ…‹: {status}")
        print(f"   ç¸½é«”åˆ†æ•¸: {report.metrics.overall_score:.3f}")
        
        # é—œéµæŒ‡æ¨™
        print(f"\nğŸ“ˆ é—œéµæŒ‡æ¨™:")
        print(f"   è¦†è“‹ç‡åˆ†æ•¸: {report.metrics.coverage_score:.3f}")
        print(f"   å¤šæ¨£æ€§åˆ†æ•¸: {report.metrics.diversity_score:.3f}")
        print(f"   è³ªé‡åˆ†æ•¸: {report.metrics.quality_score:.3f}")
        
        # å•é¡Œçµ±è¨ˆ
        print(f"\nâš ï¸ å•é¡Œçµ±è¨ˆ:")
        print(f"   ç¸½å•é¡Œæ•¸: {len(report.issues)}")
        print(f"   åš´é‡å•é¡Œ: {report.metrics.critical_cases}")
        print(f"   éŒ¯èª¤å•é¡Œ: {report.metrics.error_cases}")
        print(f"   è­¦å‘Šå•é¡Œ: {report.metrics.warning_cases}")
        
        # é¡åˆ¥åˆ†æ•¸
        print(f"\nğŸ¯ é¡åˆ¥åˆ†æ•¸:")
        for category, score in sorted(report.metrics.category_scores.items()):
            score_emoji = "âœ…" if score >= 0.8 else "âš ï¸" if score >= 0.6 else "âŒ"
            print(f"   {score_emoji} {category}: {score:.3f}")
        
        # å»ºè­°
        if report.recommendations:
            print(f"\nğŸ’¡ æ”¹é€²å»ºè­°:")
            for i, recommendation in enumerate(report.recommendations, 1):
                print(f"   {i}. {recommendation}")
        
        # è©³ç´°åˆ†æ
        if report.detailed_analysis:
            print(f"\nğŸ“Š è©³ç´°åˆ†æ:")
            
            # é¡åˆ¥åˆ†ä½ˆ
            if "category_distribution" in report.detailed_analysis:
                print(f"   é¡åˆ¥åˆ†ä½ˆ:")
                for category, count in report.detailed_analysis["category_distribution"].most_common():
                    print(f"     {category}: {count}")
            
            # æŸ¥è©¢é•·åº¦çµ±è¨ˆ
            if "query_length_stats" in report.detailed_analysis:
                stats = report.detailed_analysis["query_length_stats"]
                print(f"   æŸ¥è©¢é•·åº¦çµ±è¨ˆ:")
                print(f"     å¹³å‡: {stats['mean']:.1f}, ä¸­ä½æ•¸: {stats['median']:.1f}")
                print(f"     æœ€å°: {stats['min']}, æœ€å¤§: {stats['max']}")
        
        print("\n" + "=" * 60)


def load_test_cases_from_file(file_path: str) -> List[Dict[str, Any]]:
    """å¾æ–‡ä»¶è¼‰å…¥æ¸¬è©¦æ¡ˆä¾‹"""
    print(f"ğŸ“‚ è¼‰å…¥æ¸¬è©¦æ¡ˆä¾‹å¾ {file_path}...")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if isinstance(data, dict) and "test_cases" in data:
            test_cases = data["test_cases"]
        elif isinstance(data, list):
            test_cases = data
        else:
            raise ValueError("ç„¡æ•ˆçš„æ–‡ä»¶æ ¼å¼")
        
        print(f"   âœ… æˆåŠŸè¼‰å…¥ {len(test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹")
        return test_cases
        
    except Exception as e:
        print(f"   âŒ è¼‰å…¥å¤±æ•—: {str(e)}")
        return []


def main():
    """ä¸»å‡½æ•¸ - æ¸¬è©¦æ¡ˆä¾‹é©—è­‰å™¨å…¥å£é»"""
    print("ğŸ” LLMæ¸¬è©¦æ¡ˆä¾‹é©—è­‰å™¨")
    print("=" * 60)
    
    validator = LLMTestCaseValidator()
    
    try:
        # ç²å–è¼¸å…¥æ–‡ä»¶
        input_file = input("è«‹è¼¸å…¥æ¸¬è©¦æ¡ˆä¾‹æ–‡ä»¶è·¯å¾‘ (æˆ–æŒ‰Enterä½¿ç”¨é»˜èª): ").strip()
        if not input_file:
            input_file = "generated_test_cases.json"
        
        # è¼‰å…¥æ¸¬è©¦æ¡ˆä¾‹
        test_cases = load_test_cases_from_file(input_file)
        
        if not test_cases:
            print("âŒ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ¸¬è©¦æ¡ˆä¾‹")
            return
        
        # åŸ·è¡Œé©—è­‰
        print(f"\nğŸ” é–‹å§‹é©—è­‰ {len(test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹...")
        report = validator.validate_test_cases(test_cases)
        
        # é¡¯ç¤ºæ‘˜è¦
        validator.print_validation_summary(report)
        
        # å°å‡ºå ±å‘Š
        export_report = input("\næ˜¯å¦å°å‡ºè©³ç´°é©—è­‰å ±å‘Šï¼Ÿ (y/n): ").strip().lower()
        if export_report == 'y':
            output_file = validator.export_validation_report(report)
            print(f"   ğŸ“„ å ±å‘Šæ–‡ä»¶: {output_file}")
        
        print("\nğŸ‰ æ¸¬è©¦æ¡ˆä¾‹é©—è­‰å®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâŒ é©—è­‰éç¨‹è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ é©—è­‰éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()