#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
JobSpy CSVæ ¼å¼é©—è­‰å·¥å…·

é€™å€‹è…³æœ¬ç”¨æ–¼é©—è­‰JobSpyå°ˆæ¡ˆä¸­çš„CSVæª”æ¡ˆæ˜¯å¦ç¬¦åˆçµ±ä¸€çš„æ ¼å¼æ¨™æº–ï¼Œ
ä¸¦å±•ç¤ºä¸åŒç¶²ç«™çˆ¬èŸ²è¼¸å‡ºçš„æ ¼å¼æ¨™æº–åŒ–éç¨‹ã€‚

ä½œè€…: JobSpy Team
æ—¥æœŸ: 2025
"""

import pandas as pd
import os
from typing import List, Dict, Any
from pathlib import Path

# JobSpyæ¨™æº–CSVæ¬„ä½é †åº
DESIRED_ORDER = [
    "id",
    "site",
    "job_url",
    "job_url_direct",
    "title",
    "company",
    "location",
    "date_posted",
    "job_type",
    "salary_source",
    "interval",
    "min_amount",
    "max_amount",
    "currency",
    "is_remote",
    "job_level",
    "job_function",
    "listing_type",
    "emails",
    "description",
    "company_industry",
    "company_url",
    "company_logo",
    "company_url_direct",
    "company_addresses",
    "company_num_employees",
    "company_revenue",
    "company_description",
    # naukri-specific fields
    "skills",
    "experience_range",
    "company_rating",
    "company_reviews_count",
    "vacancy_count",
    "work_from_home_type",
]

def validate_csv_format(csv_file_path: str) -> Dict[str, Any]:
    """
    é©—è­‰CSVæª”æ¡ˆæ ¼å¼æ˜¯å¦ç¬¦åˆJobSpyæ¨™æº–
    
    Args:
        csv_file_path: CSVæª”æ¡ˆè·¯å¾‘
        
    Returns:
        åŒ…å«é©—è­‰çµæœçš„å­—å…¸
    """
    try:
        # è®€å–CSVæª”æ¡ˆ
        df = pd.read_csv(csv_file_path)
        
        # åŸºæœ¬è³‡è¨Š
        result = {
            'file_path': csv_file_path,
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'columns': list(df.columns),
            'missing_standard_columns': [],
            'extra_columns': [],
            'column_order_correct': False,
            'format_compliant': False,
            'sites_found': [],
            'sample_data': {}
        }
        
        # æª¢æŸ¥ç¼ºå¤±çš„æ¨™æº–æ¬„ä½
        for col in DESIRED_ORDER:
            if col not in df.columns:
                result['missing_standard_columns'].append(col)
        
        # æª¢æŸ¥é¡å¤–çš„æ¬„ä½
        for col in df.columns:
            if col not in DESIRED_ORDER:
                result['extra_columns'].append(col)
        
        # æª¢æŸ¥æ¬„ä½é †åº
        existing_standard_cols = [col for col in DESIRED_ORDER if col in df.columns]
        actual_order = [col for col in df.columns if col in DESIRED_ORDER]
        result['column_order_correct'] = existing_standard_cols == actual_order
        
        # æª¢æŸ¥æ˜¯å¦å®Œå…¨ç¬¦åˆæ ¼å¼
        result['format_compliant'] = (
            len(result['missing_standard_columns']) == 0 and
            len(result['extra_columns']) == 0 and
            result['column_order_correct']
        )
        
        # ç²å–ç¶²ç«™è³‡è¨Š
        if 'site' in df.columns:
            result['sites_found'] = df['site'].unique().tolist()
        
        # ç²å–æ¨£æœ¬è³‡æ–™
        if len(df) > 0:
            sample_row = df.iloc[0].to_dict()
            # åªé¡¯ç¤ºå‰10å€‹æ¬„ä½çš„æ¨£æœ¬è³‡æ–™
            result['sample_data'] = {k: v for i, (k, v) in enumerate(sample_row.items()) if i < 10}
        
        return result
        
    except Exception as e:
        return {
            'file_path': csv_file_path,
            'error': str(e),
            'format_compliant': False
        }

def standardize_csv_format(input_csv: str, output_csv: str = None) -> bool:
    """
    å°‡CSVæª”æ¡ˆæ¨™æº–åŒ–ç‚ºJobSpyæ ¼å¼
    
    Args:
        input_csv: è¼¸å…¥CSVæª”æ¡ˆè·¯å¾‘
        output_csv: è¼¸å‡ºCSVæª”æ¡ˆè·¯å¾‘ï¼ˆå¦‚æœç‚ºNoneï¼Œå‰‡è¦†è“‹åŸæª”æ¡ˆï¼‰
        
    Returns:
        æ˜¯å¦æˆåŠŸæ¨™æº–åŒ–
    """
    try:
        # è®€å–åŸå§‹æª”æ¡ˆ
        df = pd.read_csv(input_csv)
        
        # ç¢ºä¿æ‰€æœ‰æ¨™æº–æ¬„ä½éƒ½å­˜åœ¨
        for col in DESIRED_ORDER:
            if col not in df.columns:
                df[col] = None
        
        # é‡æ–°æ’åºæ¬„ä½
        df = df[DESIRED_ORDER]
        
        # ä¿å­˜æ¨™æº–åŒ–å¾Œçš„æª”æ¡ˆ
        output_path = output_csv if output_csv else input_csv
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        print(f"âœ… å·²æ¨™æº–åŒ–: {output_path}")
        return True
        
    except Exception as e:
        print(f"âŒ æ¨™æº–åŒ–å¤±æ•— {input_csv}: {e}")
        return False

def generate_format_report(csv_files: List[str]) -> None:
    """
    ç”ŸæˆCSVæ ¼å¼é©—è­‰å ±å‘Š
    
    Args:
        csv_files: CSVæª”æ¡ˆè·¯å¾‘åˆ—è¡¨
    """
    print("\n" + "="*80)
    print("ğŸ“‹ JobSpy CSVæ ¼å¼é©—è­‰å ±å‘Š")
    print("="*80)
    
    compliant_files = []
    non_compliant_files = []
    
    for csv_file in csv_files:
        if not os.path.exists(csv_file):
            print(f"âš ï¸  æª”æ¡ˆä¸å­˜åœ¨: {csv_file}")
            continue
            
        result = validate_csv_format(csv_file)
        
        print(f"\nğŸ“„ æª”æ¡ˆ: {os.path.basename(csv_file)}")
        print(f"   è·¯å¾‘: {csv_file}")
        print(f"   è³‡æ–™ç­†æ•¸: {result.get('total_rows', 'N/A')}")
        print(f"   æ¬„ä½æ•¸é‡: {result.get('total_columns', 'N/A')}")
        
        if 'error' in result:
            print(f"   âŒ éŒ¯èª¤: {result['error']}")
            non_compliant_files.append(csv_file)
            continue
        
        if result['format_compliant']:
            print("   âœ… æ ¼å¼å®Œå…¨ç¬¦åˆæ¨™æº–")
            compliant_files.append(csv_file)
        else:
            print("   âš ï¸  æ ¼å¼ä¸å®Œå…¨ç¬¦åˆæ¨™æº–")
            non_compliant_files.append(csv_file)
            
            if result['missing_standard_columns']:
                print(f"   ç¼ºå¤±æ¬„ä½: {', '.join(result['missing_standard_columns'])}")
            
            if result['extra_columns']:
                print(f"   é¡å¤–æ¬„ä½: {', '.join(result['extra_columns'])}")
            
            if not result['column_order_correct']:
                print("   æ¬„ä½é †åºä¸æ­£ç¢º")
        
        if result['sites_found']:
            print(f"   åŒ…å«ç¶²ç«™: {', '.join(result['sites_found'])}")
    
    # ç¸½çµå ±å‘Š
    print("\n" + "="*80)
    print("ğŸ“Š é©—è­‰ç¸½çµ")
    print("="*80)
    print(f"âœ… ç¬¦åˆæ¨™æº–çš„æª”æ¡ˆ: {len(compliant_files)}")
    print(f"âš ï¸  ä¸ç¬¦åˆæ¨™æº–çš„æª”æ¡ˆ: {len(non_compliant_files)}")
    print(f"ğŸ“ ç¸½æª”æ¡ˆæ•¸: {len(csv_files)}")
    
    if non_compliant_files:
        print("\nğŸ”§ å»ºè­°æ¨™æº–åŒ–ä»¥ä¸‹æª”æ¡ˆ:")
        for file in non_compliant_files:
            print(f"   - {os.path.basename(file)}")

def demonstrate_format_standardization():
    """
    å±•ç¤ºæ ¼å¼æ¨™æº–åŒ–éç¨‹
    """
    print("\n" + "="*80)
    print("ğŸ”§ JobSpy CSVæ ¼å¼æ¨™æº–åŒ–å±•ç¤º")
    print("="*80)
    
    # å‰µå»ºç¯„ä¾‹åŸå§‹è³‡æ–™ï¼ˆæ¨¡æ“¬ä¸åŒç¶²ç«™çš„æ ¼å¼ï¼‰
    sample_data = {
        'indeed_raw.csv': {
            'SITE': ['indeed', 'indeed'],
            'TITLE': ['Software Engineer', 'Senior Software Engineer'],
            'COMPANY': ['AMERICAN SYSTEMS', 'TherapyNotes.com'],
            'CITY': ['Arlington', 'Philadelphia'],
            'STATE': ['VA', 'PA'],
            'JOB_TYPE': [None, 'fulltime'],
            'INTERVAL': ['yearly', 'yearly'],
            'MIN_AMOUNT': [150000, 110000],
            'MAX_AMOUNT': [200000, 135000],
            'JOB_URL': ['https://www.indeed.com/viewjob?jk=5e409e577046...', 
                       'https://www.indeed.com/viewjob?jk=da39574a40cb...'],
            'DESCRIPTION': ['THIS POSITION COMES WITH A 10K SIGNING BONUS!...', 
                           'About Us TherapyNotes is the national leader i...']
        },
        'linkedin_raw.csv': {
            'site': ['linkedin', 'linkedin'],
            'title': ['Software Engineer - Early Career', 'Full-Stack Software Engineer'],
            'company': ['Lockheed Martin', 'Rain'],
            'location': ['Sunnyvale, CA', 'New York, NY'],
            'job_type': ['fulltime', 'fulltime'],
            'interval': ['yearly', 'yearly'],
            'min_amount': [None, None],
            'max_amount': [None, None],
            'job_url': ['https://www.linkedin.com/jobs/view/3693012711',
                       'https://www.linkedin.com/jobs/view/3696158877'],
            'description': ['Description:By bringing together people that u...',
                           'Rain\'s mission is to create the fastest and ea...']
        }
    }
    
    # å‰µå»ºç¯„ä¾‹æª”æ¡ˆ
    for filename, data in sample_data.items():
        df = pd.DataFrame(data)
        df.to_csv(filename, index=False)
        print(f"ğŸ“ å·²å‰µå»ºç¯„ä¾‹æª”æ¡ˆ: {filename}")
    
    print("\nğŸ” åŸå§‹æ ¼å¼åˆ†æ:")
    for filename in sample_data.keys():
        result = validate_csv_format(filename)
        print(f"\nğŸ“„ {filename}:")
        print(f"   æ¬„ä½: {result['columns']}")
        print(f"   ç¬¦åˆæ¨™æº–: {'âœ…' if result['format_compliant'] else 'âŒ'}")
        
        if not result['format_compliant']:
            print(f"   ç¼ºå¤±æ¬„ä½: {result['missing_standard_columns'][:5]}..." 
                  if len(result['missing_standard_columns']) > 5 
                  else result['missing_standard_columns'])
    
    print("\nğŸ”§ åŸ·è¡Œæ¨™æº–åŒ–...")
    for filename in sample_data.keys():
        standardized_name = filename.replace('_raw.csv', '_standardized.csv')
        standardize_csv_format(filename, standardized_name)
    
    print("\nâœ… æ¨™æº–åŒ–å¾Œæ ¼å¼åˆ†æ:")
    for filename in sample_data.keys():
        standardized_name = filename.replace('_raw.csv', '_standardized.csv')
        result = validate_csv_format(standardized_name)
        print(f"\nğŸ“„ {standardized_name}:")
        print(f"   ç¬¦åˆæ¨™æº–: {'âœ…' if result['format_compliant'] else 'âŒ'}")
        print(f"   ç¸½æ¬„ä½æ•¸: {result['total_columns']}")
    
    # æ¸…ç†ç¯„ä¾‹æª”æ¡ˆ
    print("\nğŸ§¹ æ¸…ç†ç¯„ä¾‹æª”æ¡ˆ...")
    for filename in sample_data.keys():
        for suffix in ['', '_standardized']:
            file_to_remove = filename.replace('.csv', f'{suffix}.csv')
            if os.path.exists(file_to_remove):
                os.remove(file_to_remove)
                print(f"   å·²åˆªé™¤: {file_to_remove}")

def main():
    """
    ä¸»å‡½æ•¸
    """
    print("ğŸš€ JobSpy CSVæ ¼å¼é©—è­‰èˆ‡æ¨™æº–åŒ–å·¥å…·")
    
    # å°‹æ‰¾ç•¶å‰ç›®éŒ„ä¸‹çš„CSVæª”æ¡ˆ
    current_dir = Path('.')
    csv_files = list(current_dir.glob('*.csv'))
    csv_file_paths = [str(f) for f in csv_files]
    
    if csv_file_paths:
        print(f"\nğŸ“ æ‰¾åˆ° {len(csv_file_paths)} å€‹CSVæª”æ¡ˆ")
        generate_format_report(csv_file_paths)
    else:
        print("\nğŸ“ ç•¶å‰ç›®éŒ„ä¸‹æ²’æœ‰æ‰¾åˆ°CSVæª”æ¡ˆ")
    
    # å±•ç¤ºæ ¼å¼æ¨™æº–åŒ–éç¨‹
    demonstrate_format_standardization()
    
    print("\n" + "="*80)
    print("ğŸ“‹ JobSpyæ¨™æº–CSVæ ¼å¼èªªæ˜")
    print("="*80)
    print("JobSpyä½¿ç”¨çµ±ä¸€çš„CSVæ ¼å¼ä¾†ç¢ºä¿ä¸åŒç¶²ç«™çˆ¬èŸ²çš„è¼¸å‡ºä¸€è‡´æ€§:")
    print("\nğŸ”¹ æ ¸å¿ƒæ¬„ä½:")
    core_fields = DESIRED_ORDER[:15]
    for i, field in enumerate(core_fields, 1):
        print(f"   {i:2d}. {field}")
    
    print("\nğŸ”¹ æ“´å±•æ¬„ä½:")
    extended_fields = DESIRED_ORDER[15:]
    for i, field in enumerate(extended_fields, 16):
        print(f"   {i:2d}. {field}")
    
    print("\nğŸ’¡ é‡è¦ç‰¹é»:")
    print("   â€¢ æ‰€æœ‰ç¶²ç«™çˆ¬èŸ²è¼¸å‡ºéƒ½æœƒè½‰æ›ç‚ºæ­¤çµ±ä¸€æ ¼å¼")
    print("   â€¢ ç¼ºå¤±çš„æ¬„ä½æœƒå¡«å……ç‚ºNone/ç©ºå€¼")
    print("   â€¢ æ¬„ä½é †åºå›ºå®šï¼Œç¢ºä¿ä¸€è‡´æ€§")
    print("   â€¢ æ”¯æ´å¤šç¨®è–ªè³‡æ ¼å¼å’Œè²¨å¹£")
    print("   â€¢ åŒ…å«ç¶²ç«™ç‰¹å®šæ¬„ä½ï¼ˆå¦‚Naukriçš„æŠ€èƒ½è©•åˆ†ï¼‰")

if __name__ == "__main__":
    main()